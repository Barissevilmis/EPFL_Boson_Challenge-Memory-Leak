{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DATASET INTRINSICS AND SHAPE (TARGETS AND IDS INCLUDED)'''\n",
    "def DataSetInfo(y, tX_old, ids):\n",
    "    print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "    print(\"Targets: \", y)\n",
    "    print(\"Ids: \",ids)\n",
    "    print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)\n",
    "\n",
    "'''INITIALIZE WEIGHTS WITH RANDOM VALUES'''\n",
    "def InitWeights(feat):\n",
    "    ww = np.random.rand(feat)\n",
    "    #ww = np.zeros((feat))\n",
    "    init_w = np.array(ww, dtype=np.float64)\n",
    "    return init_w\n",
    "\n",
    "'''HYPER PARAMETERS'''\n",
    "def HyperParameters():\n",
    "    max_iter = 800\n",
    "    epochs = 5\n",
    "    gamma = 1e-2\n",
    "    lambda_ = 1e-1\n",
    "    decay = 1e-2\n",
    "    k_fold = 10\n",
    "    return max_iter, epochs, gamma, lambda_, decay, k_fold\n",
    "\n",
    "'''DECREASE LEARNING RATE AT EVERY EPOCH'''\n",
    "def GammaScheduler(gamma, decay, epoch):\n",
    "    gamma = gamma * (1/(1 + decay * epoch))\n",
    "    return gamma\n",
    "\n",
    "'''TAKE LOG TRANSFORMATION OF FEATURES'''\n",
    "def LogTransformData(tX, features):  \n",
    "    data = tX[:, features]\n",
    "    indices = np.where(data > -999)\n",
    "    data[indices] = np.log(1 + data[indices])\n",
    "    tX = ManipulateFeatures(tX, data, features)    \n",
    "    return tX\n",
    "\n",
    "'''DELETE GIVEN FEATURE VECTOR FROM FEATURE AND CONCETENATE WITH NEW DATA '''\n",
    "def ManipulateFeatures(tX, data,features):\n",
    "    tX = np.delete(tX, features, 1)\n",
    "    return np.hstack((tX, data))\n",
    "\n",
    "'''IMPUTE DATA WITH MEANS'''\n",
    "def ImputeData(tX):\n",
    "    tX = np.where(tX == -999, np.nan, tX)\n",
    "    #Remove all columns with NAN\n",
    "    tX = tX[:, ~np.all(np.isnan(tX), axis=0)]\n",
    "    #Remove highly correlated features\n",
    "    tX = tX[:, ~np.all(tX[1:] == tX[:-1], axis=0)]\n",
    "    #Find Mean excluding NAN values\n",
    "    tX_mean = np.nanmean(tX, axis=0)\n",
    "    # NAN = MEAN\n",
    "    tX[np.where(np.isnan(tX))] = np.take(tX_mean, np.where(np.isnan(tX))[1])\n",
    "    print(tX.shape)\n",
    "    return tX\n",
    "\n",
    "'''STANDARDIZE'''\n",
    "def Standardize(tX):\n",
    "    mean_x = np.mean(tX, axis=0)\n",
    "    tX = tX - mean_x\n",
    "    std_x = np.std(tX, axis=0)\n",
    "    tX[:, std_x > 0] = tX[:, std_x > 0] / std_x[std_x > 0]\n",
    "    return tX, mean_x, std_x\n",
    "\n",
    "'''PREPROCESS'''\n",
    "def PreProcess(tX, rr_ = True):\n",
    "    '''FEATURES PICKED BY HAND FOR LOG TRANSFORM'''\n",
    "    log_feature_vec = np.array(([0, 2, 5, 9, 13, 16, 19, 21, 23, 26, 29]))\n",
    "    tX = LogTransformData(tX, log_feature_vec)\n",
    "    print(tX.shape)\n",
    "    tX = ImputeData(tX)\n",
    "    if rr_:\n",
    "        tX = Standardize(tX)[0]\n",
    "        tX = AddFeatures(tX)\n",
    "    else:\n",
    "        tX = AddFeatures(tX)\n",
    "        tX = Standardize(tX)[0]\n",
    "    return tX\n",
    "\n",
    "'''DATASET SEPERATED IN TERMS OF CATEGORIES IN COLUMN 22'''\n",
    "def Categorize_Train(y, tX, ids):\n",
    "    '''CATEGORY 2 AND 3 CONCETENATED'''\n",
    "    ind = [[] for j in range(3)]\n",
    "    xx = [[] for j in range(3)]\n",
    "    yy = [[] for j in range(3)]\n",
    "    iids = [[] for j in range(3)]\n",
    "    \n",
    "    for i in range(3): \n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        if i == 2:\n",
    "            ind[i] = np.hstack((ind[i], np.nonzero(tX[:, 22] == (i+1))[0].T))       \n",
    "        xx[i] = tX[ind[i]]\n",
    "        yy[i] = y[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return np.array((yy)), np.array((xx)), np.array((iids)), np.array((ind))\n",
    "\n",
    "'''CATEGORIZE TEST'''\n",
    "def Categorize_Test(tX, ids):\n",
    "    '''CATEGORY 2 AND 3 CONCETENATED'''\n",
    "    ind = [[] for j in range(3)]\n",
    "    xx = [[] for j in range(3)]\n",
    "    iids = [[] for j in range(3)]\n",
    "    \n",
    "    for i in range(3): \n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        if i == 2:\n",
    "            ind[i] = np.hstack((ind[i], np.nonzero(tX[:, 22] == (i+1))[0].T))   \n",
    "        xx[i] = tX[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return np.array((xx)), np.array((iids)), np.array((ind))\n",
    "\n",
    "'''PREDICTIONS INTO COMPARABLE FORM'''\n",
    "def Decategorize(y_cat, ind):\n",
    "    size = y_cat[0].shape[0] + y_cat[1].shape[0] + y_cat[2].shape[0]\n",
    "    y = np.zeros((size,), dtype=np.float)\n",
    "    for i in range(len(y_cat)):\n",
    "        y[ind[i]] = y_cat[i]\n",
    "    return y\n",
    "\n",
    "'''CHECK VALIDATION SCORE'''\n",
    "def WeightedAverage(pred, target):\n",
    "    total_count = pred[0].shape[0] + pred[1].shape[0] + pred[2].shape[0]\n",
    "    true_count = 0\n",
    "    for i in range(3):\n",
    "        true_count +=  np.sum(pred[i] == target[i])\n",
    "    acc = true_count / total_count\n",
    "    return acc\n",
    "'''FEATURE CORRELATION MAP: ONLY FOR VISUALIZATION'''\n",
    "'''CORRELATED FEATURES: CORR > THRESHOLD : USE FOR SYNTHESIS'''\n",
    "def CorrMap(tX):\n",
    "    df = pd.DataFrame(tX)\n",
    "    corr = df.corr()\n",
    "    return corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "'''RANDOM DATA SPLIT'''\n",
    "def RandomizedDataSplit(tX, y, ids, inds, split_size = 0.1, my_seed=1):\n",
    "    '''SET SEED FOR REMOVING RANDOMNESS'''\n",
    "    #np.random.seed(my_seed)\n",
    "    '''RANDOM INDEXES'''\n",
    "    size = y.shape[0]\n",
    "    ind = np.random.permutation(size)\n",
    "    split = int(np.floor(split_size * size))\n",
    "    \n",
    "    ind_train = ind[split:]\n",
    "    ind_valid = ind[:split]\n",
    "    \n",
    "    \n",
    "    '''SPLIT DATA ACCORDING TO RANDOM INDICES'''\n",
    "    tX_train = tX[ind_train]\n",
    "    tX_valid = tX[ind_valid]\n",
    "    y_train = y[ind_train]\n",
    "    y_valid = y[ind_valid]\n",
    "    ids_train = ids[ind_train]\n",
    "    ids_valid = ids[ind_valid]\n",
    "    inds_train = inds[ind_train]\n",
    "    inds_valid = inds[ind_valid]\n",
    "    \n",
    "    print(\"Shapes of tX, y, Ids & Indices for Training: \", tX_train.shape, y_train.shape, ids_train.shape, inds_train.shape)\n",
    "    print(\"Shapes of tX, y, Ids & Indices for Validation: \", tX_valid.shape, y_valid.shape, ids_valid.shape, inds_valid.shape)\n",
    "    return (tX_train, y_train, ids_train, inds_train),(tX_valid, y_valid, ids_valid, inds_valid)\n",
    "\n",
    "'''BACKWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def BackwardSelection(y, tX, tX_valid, y_valid, model = \"RR\"):\n",
    "    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0\n",
    "    improved = True     \n",
    "    while improved:\n",
    "        improved = False\n",
    "        worst_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features:\n",
    "                \n",
    "                diff = set(list(range(tX.shape[1]))) - set(selected_features + [i])            \n",
    "                #calculate accuracy\n",
    "                #print(tX[:,list(diff)].shape,y.shape)\n",
    "                \n",
    "                cur_acc = CrossValidation(y, tX[:,list(diff)],10)\n",
    "                #print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    #print(\"best so far: \",cur_best_acc)\n",
    "                    improved = True\n",
    "                    cur_best_acc = cur_acc\n",
    "                    worst_ft = i                    \n",
    "        if improved:\n",
    "            selected_features.append(worst_ft)  \n",
    "        #print(\"burada\",improved)\n",
    "            \n",
    "    return list(set(list(range(tX.shape[1]))) - set(selected_features )), cur_best_acc\n",
    "\n",
    "'''FORWARD OF BACKWARD SELECTION'''\n",
    "def Selection(y, tX, tX_valid, y_valid, typ = \"BS\", model = \"RR\"):\n",
    "    if typ == \"BS\":\n",
    "        selected_features, cur_best_acc = BackwardSelection(y, tX, tX_valid, y_valid,model)\n",
    "        return selected_features, cur_best_acc\n",
    "    elif typ == \"FS\":\n",
    "        selected_features, cur_best_acc = ForwardSelection(y, tX, tX_valid, y_valid,model) \n",
    "        return selected_features, cur_best_acc\n",
    "    \n",
    "'''FORWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def ForwardSelection(y, tX, tX_valid, y_valid, model = \"RR\"):    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0  \n",
    "    improved = True\n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        best_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features: \n",
    "                #calculate accuracy             \n",
    "                cur_acc = CrossValidation(y, tX[:,selected_features+[i]],5)\n",
    "                #print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    improved = True                   \n",
    "                    cur_best_acc = cur_acc\n",
    "                    best_ft = i                 \n",
    "                    \n",
    "        if improved:\n",
    "            selected_features.append(best_ft)\n",
    "            #print(selected_features)\n",
    "         \n",
    "    return selected_features, cur_best_acc\n",
    "\n",
    "'''ADD NEW FUTURES'''\n",
    "def AddFeatures(tX):\n",
    "    prime_numbers = [2,3]\n",
    "    #ADD COS / SIN , SQRT \n",
    "    #CHECK FEATURE SYNTHESIS\n",
    "    pm = 0\n",
    "    loop_count = tX.shape[1]\n",
    "    for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.cos(tX[:,i]).reshape(-1,1)))\n",
    "    for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.sin(tX[:,i]).reshape(-1,1)))\n",
    "                \n",
    "    for pm in range(len(prime_numbers)):\n",
    "        for i in range(3*loop_count):\n",
    "            tX = np.hstack((tX, np.power(tX[:,i], prime_numbers[pm]).reshape(-1,1)))\n",
    "    \n",
    "    return tX\n",
    "\n",
    "'''CROSS VALIDATION HELPER FUNCTION'''\n",
    "def SelectIndices(y, k_fold, seed):\n",
    "    row_count = y.shape[0]\n",
    "    window_size = int((row_count / k_fold))\n",
    "    remainder = row_count % k_fold\n",
    "    '''SEED IN TERMS OF SHUFFLING ONLY ONCE'''\n",
    "    np.random.seed(seed)\n",
    "    rand_indices = np.random.permutation(row_count)\n",
    "    indices = [[] for i in range(k_fold)]\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        \n",
    "            indices[k] = np.array((rand_indices[k*window_size:(k+1)*window_size]))\n",
    "            \n",
    "    return np.array(indices)\n",
    "'''CROSS VALIDATION'''\n",
    "def CrossValidation(y, tX, k, cat_, lambda_):\n",
    "    seed = np.random.randint(10)\n",
    "    indices = SelectIndices(y,k,seed)\n",
    "    average_acc = 0\n",
    "    w_vec = list()\n",
    "    \n",
    "    for i in range (k): \n",
    "        tr_indices = list()\n",
    "        count = 1\n",
    "        for tr_ in range(len(indices)):\n",
    "            if i != tr_:\n",
    "                if count == 1:\n",
    "                    tr_indices = np.array((indices[tr_]))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    tr_indices = np.hstack((tr_indices, indices[tr_]))\n",
    "                \n",
    "        #print(tr_indices)\n",
    "        valid_indices = indices[i]   \n",
    "        xk_train = tX[tr_indices]\n",
    "        xk_valid = tX[valid_indices]\n",
    "        yk_train = y[tr_indices]\n",
    "        yk_valid = y[valid_indices]\n",
    "        #print(yk_train.shape,xk_train.shape)\n",
    "        #print(xk_valid.shape)\n",
    "        init_w_gd = np.array((InitWeights(xk_train.shape[1])))\n",
    "        (w,loss) = ridge_regression(yk_train, xk_train, lambda_)\n",
    "        ls_tr_pred = predict_labels(w, xk_valid)\n",
    "        average_acc += (ls_tr_pred == yk_valid).mean()/k\n",
    "        w_vec.append(w)\n",
    "        \n",
    "    print(\"Cross Validation Accuracy for Category \",cat_,\": \",average_acc)\n",
    "    w_final = w_vec[0]\n",
    "    for weight in range(1,len(w_vec)):\n",
    "        w_final += w_vec[weight]\n",
    "    w_final = w_final / len(w_vec)\n",
    "    \n",
    "    return w_final, average_acc\n",
    "    \n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Train(y, tX_old, ids):\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize_Train(y, tX_old, ids)  \n",
    "    for cat_ in range(tX_cat.shape[0]):\n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_], False)  \n",
    "         \n",
    "    '''TRAIN SET'''\n",
    "    tX_tr_cat = [[] for j in range(3)]\n",
    "    y_tr_cat = [[] for j in range(3)]\n",
    "    id_tr_cat = [[] for j in range(3)]\n",
    "    ind_tr_cat = [[] for j in range(3)]\n",
    "\n",
    "    '''VALID SET'''\n",
    "    tX_val_cat = [[] for j in range(3)]\n",
    "    y_val_cat = [[] for j in range(3)]\n",
    "    id_val_cat = [[] for j in range(3)]\n",
    "    ind_val_cat = [[] for j in range(3)]\n",
    "\n",
    "    for i in range(len(tX_cat)):\n",
    "        (tX_tr_cat[i], y_tr_cat[i],id_tr_cat[i],ind_tr_cat[i]), (tX_val_cat[i], y_val_cat[i],id_val_cat[i], ind_val_cat[i]) = RandomizedDataSplit(tX_cat[i], y_cat[i], id_cat[i], ind_cat[i])\n",
    "\n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_tr_cat = np.array((tX_tr_cat))\n",
    "    y_tr_cat = np.array((y_tr_cat))\n",
    "    id_tr_cat = np.array((id_tr_cat))\n",
    "    ind_tr_cat = np.array((ind_tr_cat))\n",
    "\n",
    "    tX_val_cat = np.array((tX_val_cat))\n",
    "    y_val_cat = np.array((y_val_cat))\n",
    "    id_val_cat = np.array((id_val_cat))\n",
    "    ind_val_cat = np.array((ind_val_cat))\n",
    "\n",
    "    return (y_tr_cat, tX_tr_cat, id_tr_cat, ind_tr_cat), (y_val_cat, tX_val_cat, id_val_cat, ind_val_cat)\n",
    "\n",
    "'''CROSS VALIDATION DATA MODEL'''\n",
    "def BuildDataModel_CV(y, tX_old, ids):\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize_Train(y, tX_old, ids)\n",
    "    \n",
    "    for cat_ in range(tX_cat.shape[0]): \n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_], True)\n",
    "        \n",
    "    return y_cat, tX_cat, id_cat, ind_cat\n",
    "\n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Test(tX_old, ids):\n",
    "    tX_cat, id_cat, ind_cat = Categorize_Test(tX_old, ids)\n",
    "    \n",
    "    for cat_ in range(tX_cat.shape[0]):\n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_])\n",
    "        \n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_cat = np.array((tX_cat))\n",
    "    id_cat = np.array((id_cat))\n",
    "    ind_cat = np.array((ind_cat))\n",
    "    \n",
    "    return tX_cat, id_cat, ind_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tx@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(w.T@w))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    z = 1 / (1 + np.exp(-1*(tx@w)))\n",
    "    return z\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    sigm2 = 1 - sigm\n",
    "    N = y.shape[0]\n",
    "    '''GIVEN THAT WE HAVE A VALUE THAT IS NEGATIVE OR REALLY SMALL(PYTHON CONVERTS IT TO ZERO DURING COMPUTATION)'''\n",
    "    '''CONVERT THEM TO 1e-100'''\n",
    "    sigm[sigm < 1e-50] = 1e-50\n",
    "    sigm2[sigm2 < 1e-50] = 1e-50\n",
    "    \n",
    "    loss = (-1/N)*np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(sigm2)))\n",
    "    \n",
    "    return loss\n",
    "def compute_log_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION FOR LR'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * (tx.T@(z - y))\n",
    "    return grad\n",
    "\n",
    "def compute_reg_log_loss(y, tx, w, lambda_):\n",
    "    '''LOGISTIC LOSS WITH REGULARIZATION'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    sigm2 = 1 - sigm\n",
    "    N = y.shape[0]\n",
    "    sigm[sigm < 1e-50] = 1e-50\n",
    "    sigm2[sigm2 < 1e-50] = 1e-50\n",
    "    loss = (-1/N)*(np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(sigm2))) + ((lambda_/2)*np.sum(w.T@w)))\n",
    "    \n",
    "    return loss\n",
    "def compute_reg_log_gradient(y, tx, w, lambda_):\n",
    "    '''GRADIENT COMPUTATION FOR LR WITH REGULARIZATION'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * ((tx.T@(z - y)) + (lambda_*w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma, cat_):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        if (n_iter % 100) == 0:\n",
    "            print(\"Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma, cat_):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            if (n_iter % 100) == 0:\n",
    "                print(\"Stochastic Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "                \n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    a = tx.shape[0]\n",
    "    m = (tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1])\n",
    "    i = np.eye(m.shape[0],m.shape[0])\n",
    "    w_ridge = np.linalg.lstsq(m,i)[0]@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, cat_, mod = 1):\n",
    "    if mod == 1:\n",
    "        '''FOR GRADIENT DESCENT'''\n",
    "        w = initial_w\n",
    "        for n_iter in range(max_iters):\n",
    "            loss = compute_log_loss(y, tx, w)\n",
    "            grad = compute_log_gradient(y, tx, w)\n",
    "            w = w - (gamma * grad)\n",
    "            if (n_iter % 100) == 0:\n",
    "                print(\"Logistic Regression Gradient Descent({bi}/{ti}) for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "\n",
    "        return (w, loss)\n",
    "    else:\n",
    "        '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "        w = initial_w \n",
    "        for n_iter in range(max_iters):\n",
    "            for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "                loss = compute_log_loss(minibatch_y, minibatch_tx, w)\n",
    "                grad = compute_log_gradient(minibatch_y, minibatch_tx, w)\n",
    "                w = w - gamma * grad\n",
    "                print(\"Logistic Regression Stochastic Gradient Descent({bi}/{ti}) for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "        return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma, cat_, mod = 1):\n",
    "    if mod == 1:\n",
    "        '''FOR GRADIENT DESCENT'''\n",
    "        w = initial_w\n",
    "        for n_iter in range(max_iters):\n",
    "            loss = compute_reg_log_loss(y, tx, w, lambda_)\n",
    "            grad = compute_reg_log_gradient(y, tx, w, lambda_)\n",
    "            w = w - (gamma * grad)\n",
    "            if (n_iter % 100) == 0:\n",
    "                print(\"Logistic Regression Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "\n",
    "        return (w, loss)\n",
    "    else:\n",
    "        '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "        w = initial_w \n",
    "        for n_iter in range(max_iters):\n",
    "            for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "                loss = compute_reg_log_loss(minibatch_y, minibatch_tx, w, lambda_)\n",
    "                grad = compute__reg_log_gradient(minibatch_y, minibatch_tx, w, lambda_)\n",
    "                w = w - gamma * grad\n",
    "                print(\"Logistic Regression Stochastic Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "        return (w, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(99913, 18)\n",
      "(77544, 30)\n",
      "(77544, 22)\n",
      "(72543, 30)\n",
      "(72543, 30)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (89922, 162) (89922,) (89922,) (89922,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (9991, 162) (9991,) (9991,) (9991,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (69790, 198) (69790,) (69790,) (69790,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (7754, 198) (7754,) (7754,) (7754,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (65289, 270) (65289,) (65289,) (65289,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (7254, 270) (7254,) (7254,) (7254,)\n"
     ]
    }
   ],
   "source": [
    "(y_cat, tX_cat, ids_cat, ind_cat), (y_val_cat, tX_val_cat, ids_val_cat, ind_val_cat) = BuildDataModel_Train(y,tX_old,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(99913, 18)\n",
      "(77544, 30)\n",
      "(77544, 22)\n",
      "(72543, 30)\n",
      "(72543, 30)\n"
     ]
    }
   ],
   "source": [
    "y_cv_cat, tX_cv_cat, ids_cv_cat, ind_cv_cat = BuildDataModel_CV(y,tX_old,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 30)\n",
      "(227458, 18)\n",
      "(175338, 30)\n",
      "(175338, 22)\n",
      "(165442, 30)\n",
      "(165442, 30)\n"
     ]
    }
   ],
   "source": [
    "tX_test_cat, id_test_cat, ind_test_cat = BuildDataModel_Test(tX_test_old,ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/799)for Category-0: loss=0.5\n",
      "Gradient Descent(100/799)for Category-0: loss=0.37235323101034923\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3691122369315538\n",
      "Gradient Descent(300/799)for Category-0: loss=0.36712117543664274\n",
      "Gradient Descent(400/799)for Category-0: loss=0.36564180974006355\n",
      "Gradient Descent(500/799)for Category-0: loss=0.36448654670771335\n",
      "Gradient Descent(600/799)for Category-0: loss=0.3635644049626767\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3628164983880047\n",
      "Gradient Descent(0/799)for Category-1: loss=0.5\n",
      "Gradient Descent(100/799)for Category-1: loss=0.37017359278586176\n",
      "Gradient Descent(200/799)for Category-1: loss=0.36305329054400437\n",
      "Gradient Descent(300/799)for Category-1: loss=0.35869594953391987\n",
      "Gradient Descent(400/799)for Category-1: loss=0.3557131039607014\n",
      "Gradient Descent(500/799)for Category-1: loss=0.35352976532773767\n",
      "Gradient Descent(600/799)for Category-1: loss=0.35184970168832086\n",
      "Gradient Descent(700/799)for Category-1: loss=0.35050787168619835\n",
      "Gradient Descent(0/799)for Category-2: loss=0.5\n",
      "Gradient Descent(100/799)for Category-2: loss=0.31087985283318215\n",
      "Gradient Descent(200/799)for Category-2: loss=0.3023418132152814\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2970685094591793\n",
      "Gradient Descent(400/799)for Category-2: loss=0.2934679012273125\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2908630290373668\n",
      "Gradient Descent(600/799)for Category-2: loss=0.2889007003903193\n",
      "Gradient Descent(700/799)for Category-2: loss=0.2873763709676986\n",
      "0.01\n",
      "Gradient Descent(0/799)for Category-0: loss=0.36220114485439747\n",
      "Gradient Descent(100/799)for Category-0: loss=0.3616879529831045\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3612544300561283\n",
      "Gradient Descent(300/799)for Category-0: loss=0.3608837381345581\n",
      "Gradient Descent(400/799)for Category-0: loss=0.3605631396839385\n",
      "Gradient Descent(500/799)for Category-0: loss=0.36028290093429977\n",
      "Gradient Descent(600/799)for Category-0: loss=0.36003550961447806\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3598151118338121\n",
      "Gradient Descent(0/799)for Category-1: loss=0.3494066485409626\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3484849167254519\n",
      "Gradient Descent(200/799)for Category-1: loss=0.3477022459196438\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34703047330825637\n",
      "Gradient Descent(400/799)for Category-1: loss=0.34644906931883684\n",
      "Gradient Descent(500/799)for Category-1: loss=0.3459424921381242\n",
      "Gradient Descent(600/799)for Category-1: loss=0.3454986166040878\n",
      "Gradient Descent(700/799)for Category-1: loss=0.34510776066419613\n",
      "Gradient Descent(0/799)for Category-2: loss=0.2861627912036399\n",
      "Gradient Descent(100/799)for Category-2: loss=0.28517661300705194\n",
      "Gradient Descent(200/799)for Category-2: loss=0.2843610962989702\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2836764250785116\n",
      "Gradient Descent(400/799)for Category-2: loss=0.28309395047310937\n",
      "Gradient Descent(500/799)for Category-2: loss=0.28259260904720973\n",
      "Gradient Descent(600/799)for Category-2: loss=0.2821566152189744\n",
      "Gradient Descent(700/799)for Category-2: loss=0.28177393298578585\n",
      "0.01\n",
      "Gradient Descent(0/799)for Category-0: loss=0.3596171032033852\n",
      "Gradient Descent(100/799)for Category-0: loss=0.35943952297089926\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3592774650762798\n",
      "Gradient Descent(300/799)for Category-0: loss=0.35912863084344815\n",
      "Gradient Descent(400/799)for Category-0: loss=0.35899114670876636\n",
      "Gradient Descent(500/799)for Category-0: loss=0.3588634744033233\n",
      "Gradient Descent(600/799)for Category-0: loss=0.3587443421599368\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3586326916000929\n",
      "Gradient Descent(0/799)for Category-1: loss=0.34476205509281305\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3444578870912421\n",
      "Gradient Descent(200/799)for Category-1: loss=0.3441863825673209\n",
      "Gradient Descent(300/799)for Category-1: loss=0.3439431374240651\n",
      "Gradient Descent(400/799)for Category-1: loss=0.3437244273498298\n",
      "Gradient Descent(500/799)for Category-1: loss=0.34352708684393407\n",
      "Gradient Descent(600/799)for Category-1: loss=0.34334841393752147\n",
      "Gradient Descent(700/799)for Category-1: loss=0.3431860939400688\n",
      "Gradient Descent(0/799)for Category-2: loss=0.2814352405591857\n",
      "Gradient Descent(100/799)for Category-2: loss=0.2811360464436587\n",
      "Gradient Descent(200/799)for Category-2: loss=0.2808671332945349\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2806239350385498\n",
      "Gradient Descent(400/799)for Category-2: loss=0.28040273097098284\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2802004614900432\n",
      "Gradient Descent(600/799)for Category-2: loss=0.28001459000930834\n",
      "Gradient Descent(700/799)for Category-2: loss=0.2798429980895535\n",
      "0.009900990099009901\n",
      "Gradient Descent(0/799)for Category-0: loss=0.35852763640618457\n",
      "Gradient Descent(100/799)for Category-0: loss=0.35843032335792313\n",
      "Gradient Descent(200/799)for Category-0: loss=0.35833803454173346\n",
      "Gradient Descent(300/799)for Category-0: loss=0.3582502599019586\n",
      "Gradient Descent(400/799)for Category-0: loss=0.35816656143277925\n",
      "Gradient Descent(500/799)for Category-0: loss=0.35808656117578785\n",
      "Gradient Descent(600/799)for Category-0: loss=0.35800993145724097\n",
      "Gradient Descent(700/799)for Category-0: loss=0.35793638690421364\n",
      "Gradient Descent(0/799)for Category-1: loss=0.3430381376255372\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3429053723822326\n",
      "Gradient Descent(200/799)for Category-1: loss=0.3427833666628979\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34267090646519766\n",
      "Gradient Descent(400/799)for Category-1: loss=0.3425669350467356\n",
      "Gradient Descent(500/799)for Category-1: loss=0.34247053063653254\n",
      "Gradient Descent(600/799)for Category-1: loss=0.34238088758064467\n",
      "Gradient Descent(700/799)for Category-1: loss=0.34229730034010963\n",
      "Gradient Descent(0/799)for Category-2: loss=0.27968390479484906\n",
      "Gradient Descent(100/799)for Category-2: loss=0.2795386111493237\n",
      "Gradient Descent(200/799)for Category-2: loss=0.2794026737030791\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2792750565127274\n",
      "Gradient Descent(400/799)for Category-2: loss=0.2791548676504891\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2790413359986618\n",
      "Gradient Descent(600/799)for Category-2: loss=0.2789337922955344\n",
      "Gradient Descent(700/799)for Category-2: loss=0.27883165355433764\n",
      "0.009706853038245\n",
      "Gradient Descent(0/799)for Category-0: loss=0.3578656778823649\n",
      "Gradient Descent(100/799)for Category-0: loss=0.3577995333964251\n",
      "Gradient Descent(200/799)for Category-0: loss=0.35773567743907575\n",
      "Gradient Descent(300/799)for Category-0: loss=0.357673952347422\n",
      "Gradient Descent(400/799)for Category-0: loss=0.3576142174353407\n",
      "Gradient Descent(500/799)for Category-0: loss=0.3575563466972775\n",
      "Gradient Descent(600/799)for Category-0: loss=0.35750022686457467\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3574457557549617\n",
      "Gradient Descent(0/799)for Category-1: loss=0.3422191498703808\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3421479614528987\n",
      "Gradient Descent(200/799)for Category-1: loss=0.34208094509995485\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34201771180510604\n",
      "Gradient Descent(400/799)for Category-1: loss=0.34195791651732227\n",
      "Gradient Descent(500/799)for Category-1: loss=0.34190125265046667\n",
      "Gradient Descent(600/799)for Category-1: loss=0.34184744733066613\n",
      "Gradient Descent(700/799)for Category-1: loss=0.34179625727584334\n",
      "Gradient Descent(0/799)for Category-2: loss=0.278734410180145\n",
      "Gradient Descent(100/799)for Category-2: loss=0.27864425918541386\n",
      "Gradient Descent(200/799)for Category-2: loss=0.2785579405709158\n",
      "Gradient Descent(300/799)for Category-2: loss=0.27847513516307354\n",
      "Gradient Descent(400/799)for Category-2: loss=0.27839555953665146\n",
      "Gradient Descent(500/799)for Category-2: loss=0.27831896134546674\n",
      "Gradient Descent(600/799)for Category-2: loss=0.27824511534009577\n",
      "Gradient Descent(700/799)for Category-2: loss=0.2781738199602561\n",
      "0.00942412916334466\n",
      "Gradient Descent(0/799)for Category-0: loss=0.3573928408667153\n",
      "Gradient Descent(100/799)for Category-0: loss=0.35734335059675143\n",
      "Gradient Descent(200/799)for Category-0: loss=0.35729515321823946\n",
      "Gradient Descent(300/799)for Category-0: loss=0.35724818618813853\n",
      "Gradient Descent(400/799)for Category-0: loss=0.357202391971703\n",
      "Gradient Descent(500/799)for Category-0: loss=0.3571577174948752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(600/799)for Category-0: loss=0.35711411366768875\n",
      "Gradient Descent(700/799)for Category-0: loss=0.35707153496850086\n",
      "Gradient Descent(0/799)for Category-1: loss=0.3417474652178276\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3417026302586186\n",
      "Gradient Descent(200/799)for Category-1: loss=0.3416596773914941\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34161846678722463\n",
      "Gradient Descent(400/799)for Category-1: loss=0.3415788722172633\n",
      "Gradient Descent(500/799)for Category-1: loss=0.34154077955156065\n",
      "Gradient Descent(600/799)for Category-1: loss=0.3415040854364736\n",
      "Gradient Descent(700/799)for Category-1: loss=0.34146869612972863\n",
      "Gradient Descent(0/799)for Category-2: loss=0.2781048944097608\n",
      "Gradient Descent(100/799)for Category-2: loss=0.27804070341916887\n",
      "Gradient Descent(200/799)for Category-2: loss=0.2779784226032939\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2779179336871731\n",
      "Gradient Descent(400/799)for Category-2: loss=0.2778591292252598\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2778019114172365\n",
      "Gradient Descent(600/799)for Category-2: loss=0.2777461910697219\n",
      "Gradient Descent(700/799)for Category-2: loss=0.2776918866839147\n",
      "0.009061662657062174\n",
      "Gradient Descent(0/799)for Category-0: loss=0.35702993908044645\n",
      "Gradient Descent(100/799)for Category-0: loss=0.3569912016705426\n",
      "Gradient Descent(200/799)for Category-0: loss=0.35695328804984705\n",
      "Gradient Descent(300/799)for Category-0: loss=0.35691616831029643\n",
      "Gradient Descent(400/799)for Category-0: loss=0.35687981432204063\n",
      "Gradient Descent(500/799)for Category-0: loss=0.35684419958056673\n",
      "Gradient Descent(600/799)for Category-0: loss=0.356809299070415\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3567750891434267\n",
      "Gradient Descent(0/799)for Category-1: loss=0.3414345264725052\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3414030470083116\n",
      "Gradient Descent(200/799)for Category-1: loss=0.34137254233691056\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34134295643070894\n",
      "Gradient Descent(400/799)for Category-1: loss=0.3413142378577518\n",
      "Gradient Descent(500/799)for Category-1: loss=0.3412863393371123\n",
      "Gradient Descent(600/799)for Category-1: loss=0.3412592173416071\n",
      "Gradient Descent(700/799)for Category-1: loss=0.3412328317424191\n",
      "Gradient Descent(0/799)for Category-2: loss=0.2776389236522117\n",
      "Gradient Descent(100/799)for Category-2: loss=0.27758966720364586\n",
      "Gradient Descent(200/799)for Category-2: loss=0.27754151087553264\n",
      "Gradient Descent(300/799)for Category-2: loss=0.27749440439105644\n",
      "Gradient Descent(400/799)for Category-2: loss=0.2774483012179482\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2774031582208121\n",
      "Gradient Descent(600/799)for Category-2: loss=0.27735893534993106\n",
      "Gradient Descent(700/799)for Category-2: loss=0.27731559536231787\n",
      "0.008630154911487784\n",
      "Gradient Descent(0/799)for Category-0: loss=0.3567415474097439\n",
      "Gradient Descent(100/799)for Category-0: loss=0.35671049777137803\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3566800069673947\n",
      "Gradient Descent(300/799)for Category-0: loss=0.35665005884057577\n",
      "Gradient Descent(400/799)for Category-0: loss=0.35662063796986027\n",
      "Gradient Descent(500/799)for Category-0: loss=0.35659172962129876\n",
      "Gradient Descent(600/799)for Category-0: loss=0.35656331970334393\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3565353947260155\n",
      "Gradient Descent(0/799)for Category-1: loss=0.3412071454908765\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3411835235114813\n",
      "Gradient Descent(200/799)for Category-1: loss=0.34116046675180217\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34113795036770006\n",
      "Gradient Descent(400/799)for Category-1: loss=0.34111595119754107\n",
      "Gradient Descent(500/799)for Category-1: loss=0.341094447621126\n",
      "Gradient Descent(600/799)for Category-1: loss=0.3410734194318566\n",
      "Gradient Descent(700/799)for Category-1: loss=0.34105284772079647\n",
      "Gradient Descent(0/799)for Category-2: loss=0.27727310357131457\n",
      "Gradient Descent(100/799)for Category-2: loss=0.27723376549177753\n",
      "Gradient Descent(200/799)for Category-2: loss=0.27719512805826035\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2771571673951977\n",
      "Gradient Descent(400/799)for Category-2: loss=0.27711986105185393\n",
      "Gradient Descent(500/799)for Category-2: loss=0.277083187889559\n",
      "Gradient Descent(600/799)for Category-2: loss=0.27704712797916076\n",
      "Gradient Descent(700/799)for Category-2: loss=0.2770116625076753\n",
      "0.008141655576875267\n",
      "Gradient Descent(0/799)for Category-0: loss=0.35650794176353423\n",
      "Gradient Descent(100/799)for Category-0: loss=0.35648270059463033\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3564578511425756\n",
      "Gradient Descent(300/799)for Category-0: loss=0.3564333840427779\n",
      "Gradient Descent(400/799)for Category-0: loss=0.3564092902710216\n",
      "Gradient Descent(500/799)for Category-0: loss=0.356385561125677\n",
      "Gradient Descent(600/799)for Category-0: loss=0.356362188211166\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3563391634225715\n",
      "Gradient Descent(0/799)for Category-1: loss=0.34103271477142433\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3410142809261178\n",
      "Gradient Descent(200/799)for Category-1: loss=0.34099620301523786\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34097846898563017\n",
      "Gradient Descent(400/799)for Category-1: loss=0.34096106744595234\n",
      "Gradient Descent(500/799)for Category-1: loss=0.3409439876191235\n",
      "Gradient Descent(600/799)for Category-1: loss=0.3409272192986835\n",
      "Gradient Descent(700/799)for Category-1: loss=0.3409107528087129\n",
      "Gradient Descent(0/799)for Category-2: loss=0.2769767736932283\n",
      "Gradient Descent(100/799)for Category-2: loss=0.2769446738168708\n",
      "Gradient Descent(200/799)for Category-2: loss=0.2769130498658574\n",
      "Gradient Descent(300/799)for Category-2: loss=0.27688188942930364\n",
      "Gradient Descent(400/799)for Category-2: loss=0.2768511806808087\n",
      "Gradient Descent(500/799)for Category-2: loss=0.27682091233929085\n",
      "Gradient Descent(600/799)for Category-2: loss=0.2767910736329093\n",
      "Gradient Descent(700/799)for Category-2: loss=0.27676165426580573\n",
      "0.0076090239036217455\n",
      "Gradient Descent(0/799)for Category-0: loss=0.35631647893129137\n",
      "Gradient Descent(100/799)for Category-0: loss=0.35629577166537524\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3562753438442778\n",
      "Gradient Descent(300/799)for Category-0: loss=0.3562551898300278\n",
      "Gradient Descent(400/799)for Category-0: loss=0.35623530415317467\n",
      "Gradient Descent(500/799)for Category-0: loss=0.35621568150569555\n",
      "Gradient Descent(600/799)for Category-0: loss=0.35619631673430213\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3561772048341187\n",
      "Gradient Descent(0/799)for Category-1: loss=0.34089457896699416\n",
      "Gradient Descent(100/799)for Category-1: loss=0.3408798565774928\n",
      "Gradient Descent(200/799)for Category-1: loss=0.34086537100612835\n",
      "Gradient Descent(300/799)for Category-1: loss=0.34085111595284406\n",
      "Gradient Descent(400/799)for Category-1: loss=0.34083708539480356\n",
      "Gradient Descent(500/799)for Category-1: loss=0.3408232735695251\n",
      "Gradient Descent(600/799)for Category-1: loss=0.340809674959225\n",
      "Gradient Descent(700/799)for Category-1: loss=0.34079628427627373\n",
      "Gradient Descent(0/799)for Category-2: loss=0.27673264438742295\n",
      "Gradient Descent(100/799)for Category-2: loss=0.27670614035949664\n",
      "Gradient Descent(200/799)for Category-2: loss=0.27667997211838663\n",
      "Gradient Descent(300/799)for Category-2: loss=0.27665413274572415\n",
      "Gradient Descent(400/799)for Category-2: loss=0.27662861557848667\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2766034141947003\n",
      "Gradient Descent(600/799)for Category-2: loss=0.27657852240012265\n",
      "Gradient Descent(700/799)for Category-2: loss=0.276553934215831\n",
      "0.0070453925033534676\n",
      "[ 1.  1. -1. ... -1.  1. -1.]\n",
      "[ 1. -1.  1. ... -1.  1. -1.]\n",
      "[ 1. -1.  1. ...  1. -1. -1.]\n",
      "Accuracy of Model: 0.765510620424817\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "def Main(y_cat, tX_cat, y_val_cat, tX_val_cat):\n",
    "    init_w_gd = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[1].shape[1]),InitWeights(tX_cat[2].shape[1])))\n",
    "    gd_tr_pred = np.copy((y_val_cat))\n",
    "    w_gd = np.copy((init_w_gd))\n",
    "    max_iters, epochs, gamma, lambda_,decay,k_fold = HyperParameters()\n",
    "    cat_lst = [0, 1, 2]\n",
    "    \n",
    "    for epoch_ in range(epochs):\n",
    "        \n",
    "        for cat_ in cat_lst:   \n",
    "            (w_gd[cat_],loss1) = least_squares_GD(y_cat[cat_], tX_cat[cat_],w_gd[cat_], max_iters, gamma, cat_)\n",
    "        \n",
    "        gamma = GammaScheduler(gamma, decay, epoch_)\n",
    "        \n",
    "    '''PREDICTIONS'''\n",
    "    for cat_ in cat_lst:\n",
    "        gd_tr_pred[cat_] = predict_labels(w_gd[cat_], tX_val_cat[cat_])\n",
    "        print(gd_tr_pred[cat_])\n",
    "            \n",
    "    acc = WeightedAverage(gd_tr_pred, y_val_cat)\n",
    "    print(\"Accuracy of Model:\", acc)\n",
    "    return w_gd\n",
    "w_normal = Main(y_cat, tX_cat, y_val_cat, tX_val_cat)\n",
    "print(w_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy for Category  0 :  0.8448748748748749\n",
      "Final weight vector shape:  (162,)\n",
      "Cross Validation Accuracy for Category  1 :  0.807544979686593\n",
      "Final weight vector shape:  (198,)\n",
      "Cross Validation Accuracy for Category  2 :  0.8340639647091261\n",
      "Final weight vector shape:  (270,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "'''CROSS VALIDATION WEIGHT'''\n",
    "'''WEIGHTS ARE TAKEN AS MEAN OF VARIOUS MEANS BY TRAINING ON DIFFERENT TRAIN-VALID PARTITIONS'''\n",
    "'''ACCURACY INCREASE IS NOT THE PURPOSE BUT WEIGHT AND ACCURACY VALIDATION ARE FIXED'''\n",
    "def CV_Main(y_cat, tX_cat):\n",
    "    cat_lst = [0, 1, 2]\n",
    "    max_iters, epochs, gamma, lambda_, decay, k_fold = HyperParameters()\n",
    "    w_res = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[1].shape[1]),InitWeights(tX_cat[2].shape[1])))\n",
    "    cv_tr_pred = [[] for i in range(len(cat_lst))]\n",
    "    for cat_ in cat_lst:\n",
    "        w_final, avg_acc = CrossValidation(y_cat[cat_], tX_cat[cat_], k_fold, cat_, lambda_)\n",
    "        w_res[cat_] = w_final\n",
    "        print(\"Final weight vector shape: \",w_final.shape)\n",
    "    return w_res\n",
    "\n",
    "\n",
    "w_cv = CV_Main(y_cv_cat, tX_cv_cat)\n",
    "print(w_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/results.csv' # TODO: fill in desired name of output file for submission\n",
    "cat_lst = [0, 1, 2]\n",
    "y_pred = [[] for i in range(3)]\n",
    "for cat_ in cat_lst:\n",
    "    y_pred[cat_] = predict_labels(w_res[cat_], tX_test_cat[cat_])\n",
    "y_pred = np.array((y_pred))\n",
    "pred_vec = Decategorize(y_pred, ind_test_cat)\n",
    "pred_ids = Decategorize(id_test_cat, ind_test_cat)\n",
    "create_csv_submission(pred_ids, pred_vec, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
