{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "            \n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "print(\"Targets: \", y)\n",
    "print(\"Ids: \",ids)\n",
    "print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE'''\n",
    "#Feature extraction\n",
    "tX = np.hstack((tX_old[:,1:4], tX_old[:,7:12]))\n",
    "tX = np.hstack((tX, tX_old[:,13:23]))\n",
    "#Standardize\n",
    "tX, tX_mean, tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size:  25000\n",
      "Shapes of tX, y & Ids for Training:  (25000, 18) (25000,) (25000,)\n",
      "Shapes of tX, y & Ids for Validation:  (225000, 18) (225000,) (225000,)\n"
     ]
    }
   ],
   "source": [
    "'''SPLIT INTO TRAIN AND VALIDATION'''\n",
    "\n",
    "train_valid_split = int(tX.shape[0] / 10)\n",
    "print(\"Validation data size: \", train_valid_split)\n",
    "tX_valid = tX[train_valid_split:,:]\n",
    "y_valid = y[train_valid_split:]\n",
    "id_valid = ids[train_valid_split:]\n",
    "\n",
    "tX = tX[:train_valid_split]\n",
    "y = y[:train_valid_split]\n",
    "ids = ids[:train_valid_split]\n",
    "\n",
    "print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, id_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7360888888888889\n"
     ]
    }
   ],
   "source": [
    "'''CHECK MODEL SUCCESS: DELETE BEFORE SUBMISSION'''\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "clf = linear_model.SGDClassifier(max_iter=500, tol=1e-3)\n",
    "clf.fit(tX, y)\n",
    "print(metrics.accuracy_score(clf.predict(tX_valid), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tX@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(np.square(w)))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    return 1 / (1 + np.exp(-1*(tx@w)))\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(np.log(sigm).T * -y - (np.log(1 - sigm).T * (1-y))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.solve((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]), tx.T@y)\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of tX & Ids for Testing:  (568238, 18) (25000,)\n"
     ]
    }
   ],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE FOR TEST'''\n",
    "#Feature extraction\n",
    "tX_test = np.hstack((tX_test_old[:,1:4], tX_test_old[:,7:12]))\n",
    "tX_test = np.hstack((tX_test, tX_test_old[:,13:23]))\n",
    "#Standardize\n",
    "tX_test, tX_test_mean, tX_test_std = standardize(tX_test)\n",
    "\n",
    "print(\"Shapes of tX & Ids for Testing: \", tX_test.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "[0.19199625 0.37676237 0.39117816 0.32511406 0.19301523 0.05617632\n",
      " 0.11564319 0.69647416 0.767847   0.65848925 0.71485873 0.39977526\n",
      " 0.41875458 0.0414966  0.82033364 0.04348927 0.68064777 0.83111796]\n"
     ]
    }
   ],
   "source": [
    "ww = np.random.rand(tX.shape[1])\n",
    "init_w = np.array(ww, dtype=np.float64)\n",
    "#init_w = np.zeros(tX.shape[1])\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HYPER PARAMETERS FOR TUNING'''\n",
    "max_iter = 500\n",
    "alpha = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=5.93796690085545\n",
      "Gradient Descent(1/499): loss=5.488485544046356\n",
      "Gradient Descent(2/499): loss=5.080486109511487\n",
      "Gradient Descent(3/499): loss=4.709989595829887\n",
      "Gradient Descent(4/499): loss=4.37340344398495\n",
      "Gradient Descent(5/499): loss=4.0674838460652\n",
      "Gradient Descent(6/499): loss=3.789301735791361\n",
      "Gradient Descent(7/499): loss=3.5362121010068757\n",
      "Gradient Descent(8/499): loss=3.305826293450161\n",
      "Gradient Descent(9/499): loss=3.095987042868281\n",
      "Gradient Descent(10/499): loss=2.9047459111697096\n",
      "Gradient Descent(11/499): loss=2.7303429481519164\n",
      "Gradient Descent(12/499): loss=2.571188333651333\n",
      "Gradient Descent(13/499): loss=2.4258458119959476\n",
      "Gradient Descent(14/499): loss=2.2930177436170442\n",
      "Gradient Descent(15/499): loss=2.171531615797656\n",
      "Gradient Descent(16/499): loss=2.0603278699823973\n",
      "Gradient Descent(17/499): loss=1.9584489170102783\n",
      "Gradient Descent(18/499): loss=1.865029224206388\n",
      "Gradient Descent(19/499): loss=1.779286369613382\n",
      "Gradient Descent(20/499): loss=1.7005129688794982\n",
      "Gradient Descent(21/499): loss=1.628069389554996\n",
      "Gradient Descent(22/499): loss=1.561377175881187\n",
      "Gradient Descent(23/499): loss=1.499913114673898\n",
      "Gradient Descent(24/499): loss=1.4432038796859763\n",
      "Gradient Descent(25/499): loss=1.3908211979530951\n",
      "Gradient Descent(26/499): loss=1.3423774871485101\n",
      "Gradient Descent(27/499): loss=1.2975219179540816\n",
      "Gradient Descent(28/499): loss=1.2559368599494871\n",
      "Gradient Descent(29/499): loss=1.2173346735768529\n",
      "Gradient Descent(30/499): loss=1.1814548143968413\n",
      "Gradient Descent(31/499): loss=1.1480612191534394\n",
      "Gradient Descent(32/499): loss=1.1169399461431637\n",
      "Gradient Descent(33/499): loss=1.0878970450717316\n",
      "Gradient Descent(34/499): loss=1.0607566340059145\n",
      "Gradient Descent(35/499): loss=1.0353591632159338\n",
      "Gradient Descent(36/499): loss=1.0115598476775745\n",
      "Gradient Descent(37/499): loss=0.9892272517841079\n",
      "Gradient Descent(38/499): loss=0.9682420114249236\n",
      "Gradient Descent(39/499): loss=0.9484956800375618\n",
      "Gradient Descent(40/499): loss=0.9298896865479335\n",
      "Gradient Descent(41/499): loss=0.9123343942937417\n",
      "Gradient Descent(42/499): loss=0.8957482510910372\n",
      "Gradient Descent(43/499): loss=0.880057021564654\n",
      "Gradient Descent(44/499): loss=0.865193093730224\n",
      "Gradient Descent(45/499): loss=0.8510948525976691\n",
      "Gradient Descent(46/499): loss=0.8377061142718679\n",
      "Gradient Descent(47/499): loss=0.824975614663003\n",
      "Gradient Descent(48/499): loss=0.8128565474936956\n",
      "Gradient Descent(49/499): loss=0.8013061468084892\n",
      "Gradient Descent(50/499): loss=0.7902853096590481\n",
      "Gradient Descent(51/499): loss=0.7797582550605362\n",
      "Gradient Descent(52/499): loss=0.7696922156955116\n",
      "Gradient Descent(53/499): loss=0.7600571591853311\n",
      "Gradient Descent(54/499): loss=0.7508255360591597\n",
      "Gradient Descent(55/499): loss=0.7419720518304787\n",
      "Gradient Descent(56/499): loss=0.7334734608434726\n",
      "Gradient Descent(57/499): loss=0.7253083797795006\n",
      "Gradient Descent(58/499): loss=0.7174571189194234\n",
      "Gradient Descent(59/499): loss=0.7099015294430789\n",
      "Gradient Descent(60/499): loss=0.7026248652145722\n",
      "Gradient Descent(61/499): loss=0.6956116576531166\n",
      "Gradient Descent(62/499): loss=0.6888476024254514\n",
      "Gradient Descent(63/499): loss=0.6823194568188814\n",
      "Gradient Descent(64/499): loss=0.676014946764967\n",
      "Gradient Descent(65/499): loss=0.6699226825840707\n",
      "Gradient Descent(66/499): loss=0.6640320826113589\n",
      "Gradient Descent(67/499): loss=0.6583333039464304\n",
      "Gradient Descent(68/499): loss=0.6528171796423623\n",
      "Gradient Descent(69/499): loss=0.6474751617164116\n",
      "Gradient Descent(70/499): loss=0.6422992694245573\n",
      "Gradient Descent(71/499): loss=0.6372820422961978\n",
      "Gradient Descent(72/499): loss=0.6324164974741483\n",
      "Gradient Descent(73/499): loss=0.6276960909491697\n",
      "Gradient Descent(74/499): loss=0.6231146823180431\n",
      "Gradient Descent(75/499): loss=0.6186665027301174\n",
      "Gradient Descent(76/499): loss=0.6143461257196662\n",
      "Gradient Descent(77/499): loss=0.6101484406506522\n",
      "Gradient Descent(78/499): loss=0.6060686285269039\n",
      "Gradient Descent(79/499): loss=0.6021021399445482\n",
      "Gradient Descent(80/499): loss=0.5982446749850676\n",
      "Gradient Descent(81/499): loss=0.5944921648667721\n",
      "Gradient Descent(82/499): loss=0.590840755190019\n",
      "Gradient Descent(83/499): loss=0.5872867906273496\n",
      "Gradient Descent(84/499): loss=0.5838268009240043\n",
      "Gradient Descent(85/499): loss=0.5804574880871894\n",
      "Gradient Descent(86/499): loss=0.5771757146541227\n",
      "Gradient Descent(87/499): loss=0.5739784929394112\n",
      "Gradient Descent(88/499): loss=0.5708629751718185\n",
      "Gradient Descent(89/499): loss=0.5678264444390613\n",
      "Gradient Descent(90/499): loss=0.5648663063670295\n",
      "Gradient Descent(91/499): loss=0.5619800814668229\n",
      "Gradient Descent(92/499): loss=0.559165398089325\n",
      "Gradient Descent(93/499): loss=0.5564199859327456\n",
      "Gradient Descent(94/499): loss=0.553741670053727\n",
      "Gradient Descent(95/499): loss=0.5511283653372698\n",
      "Gradient Descent(96/499): loss=0.5485780713849493\n",
      "Gradient Descent(97/499): loss=0.5460888677847009\n",
      "Gradient Descent(98/499): loss=0.5436589097288953\n",
      "Gradient Descent(99/499): loss=0.5412864239505295\n",
      "Gradient Descent(100/499): loss=0.53896970495018\n",
      "Gradient Descent(101/499): loss=0.5367071114888946\n",
      "Gradient Descent(102/499): loss=0.5344970633245109\n",
      "Gradient Descent(103/499): loss=0.5323380381709536\n",
      "Gradient Descent(104/499): loss=0.5302285688619565\n",
      "Gradient Descent(105/499): loss=0.5281672407023403\n",
      "Gradient Descent(106/499): loss=0.5261526889915317\n",
      "Gradient Descent(107/499): loss=0.5241835967053857\n",
      "Gradient Descent(108/499): loss=0.522258692323645\n",
      "Gradient Descent(109/499): loss=0.5203767477915007\n",
      "Gradient Descent(110/499): loss=0.5185365766047623\n",
      "Gradient Descent(111/499): loss=0.516737032009065\n",
      "Gradient Descent(112/499): loss=0.5149770053044058\n",
      "Gradient Descent(113/499): loss=0.5132554242470525\n",
      "Gradient Descent(114/499): loss=0.5115712515415755\n",
      "Gradient Descent(115/499): loss=0.509923483416377\n",
      "Gradient Descent(116/499): loss=0.508311148276666\n",
      "Gradient Descent(117/499): loss=0.5067333054293445\n",
      "Gradient Descent(118/499): loss=0.5051890438747435\n",
      "Gradient Descent(119/499): loss=0.5036774811605708\n",
      "Gradient Descent(120/499): loss=0.5021977622938227\n",
      "Gradient Descent(121/499): loss=0.500749058706765\n",
      "Gradient Descent(122/499): loss=0.4993305672734028\n",
      "Gradient Descent(123/499): loss=0.49794150937315784\n",
      "Gradient Descent(124/499): loss=0.4965811299987309\n",
      "Gradient Descent(125/499): loss=0.4952486969053677\n",
      "Gradient Descent(126/499): loss=0.4939434997989711\n",
      "Gradient Descent(127/499): loss=0.4926648495606942\n",
      "Gradient Descent(128/499): loss=0.49141207750583615\n",
      "Gradient Descent(129/499): loss=0.49018453467503004\n",
      "Gradient Descent(130/499): loss=0.4889815911558559\n",
      "Gradient Descent(131/499): loss=0.4878026354331579\n",
      "Gradient Descent(132/499): loss=0.48664707376646704\n",
      "Gradient Descent(133/499): loss=0.48551432959304736\n",
      "Gradient Descent(134/499): loss=0.4844038429551867\n",
      "Gradient Descent(135/499): loss=0.4833150699504545\n",
      "Gradient Descent(136/499): loss=0.48224748220373254\n",
      "Gradient Descent(137/499): loss=0.4812005663599097\n",
      "Gradient Descent(138/499): loss=0.48017382359620175\n",
      "Gradient Descent(139/499): loss=0.4791667691531317\n",
      "Gradient Descent(140/499): loss=0.4781789318832631\n",
      "Gradient Descent(141/499): loss=0.47720985381683995\n",
      "Gradient Descent(142/499): loss=0.47625908974353903\n",
      "Gradient Descent(143/499): loss=0.4753262068095898\n",
      "Gradient Descent(144/499): loss=0.47441078412956256\n",
      "Gradient Descent(145/499): loss=0.4735124124121652\n",
      "Gradient Descent(146/499): loss=0.4726306935994316\n",
      "Gradient Descent(147/499): loss=0.4717652405187169\n",
      "Gradient Descent(148/499): loss=0.47091567654694827\n",
      "Gradient Descent(149/499): loss=0.47008163528661423\n",
      "Gradient Descent(150/499): loss=0.4692627602529971\n",
      "Gradient Descent(151/499): loss=0.4684587045721885\n",
      "Gradient Descent(152/499): loss=0.46766913068944344\n",
      "Gradient Descent(153/499): loss=0.46689371008745895\n",
      "Gradient Descent(154/499): loss=0.4661321230141798\n",
      "Gradient Descent(155/499): loss=0.46538405821975526\n",
      "Gradient Descent(156/499): loss=0.46464921270229154\n",
      "Gradient Descent(157/499): loss=0.46392729146205647\n",
      "Gradient Descent(158/499): loss=0.4632180072638181\n",
      "Gradient Descent(159/499): loss=0.4625210804070038\n",
      "Gradient Descent(160/499): loss=0.46183623850338995\n",
      "Gradient Descent(161/499): loss=0.4611632162620395\n",
      "Gradient Descent(162/499): loss=0.4605017552812214\n",
      "Gradient Descent(163/499): loss=0.45985160384705587\n",
      "Gradient Descent(164/499): loss=0.45921251673863944\n",
      "Gradient Descent(165/499): loss=0.45858425503941924\n",
      "Gradient Descent(166/499): loss=0.45796658595458994\n",
      "Gradient Descent(167/499): loss=0.4573592826343\n",
      "Gradient Descent(168/499): loss=0.4567621240024629\n",
      "Gradient Descent(169/499): loss=0.45617489459097527\n",
      "Gradient Descent(170/499): loss=0.45559738437915354\n",
      "Gradient Descent(171/499): loss=0.45502938863820835\n",
      "Gradient Descent(172/499): loss=0.45447070778058307\n",
      "Gradient Descent(173/499): loss=0.4539211472139884\n",
      "Gradient Descent(174/499): loss=0.4533805171999738\n",
      "Gradient Descent(175/499): loss=0.4528486327168809\n",
      "Gradient Descent(176/499): loss=0.45232531332703124\n",
      "Gradient Descent(177/499): loss=0.4518103830480053\n",
      "Gradient Descent(178/499): loss=0.4513036702278756\n",
      "Gradient Descent(179/499): loss=0.4508050074242625\n",
      "Gradient Descent(180/499): loss=0.4503142312870846\n",
      "Gradient Descent(181/499): loss=0.44983118244488307\n",
      "Gradient Descent(182/499): loss=0.4493557053945997\n",
      "Gradient Descent(183/499): loss=0.44888764839469697\n",
      "Gradient Descent(184/499): loss=0.4484268633615103\n",
      "Gradient Descent(185/499): loss=0.447973205768726\n",
      "Gradient Descent(186/499): loss=0.4475265345498844\n",
      "Gradient Descent(187/499): loss=0.4470867120038096\n",
      "Gradient Descent(188/499): loss=0.4466536037028711\n",
      "Gradient Descent(189/499): loss=0.44622707840398534\n",
      "Gradient Descent(190/499): loss=0.4458070079622712\n",
      "Gradient Descent(191/499): loss=0.44539326724727185\n",
      "Gradient Descent(192/499): loss=0.4449857340616624\n",
      "Gradient Descent(193/499): loss=0.4445842890623641\n",
      "Gradient Descent(194/499): loss=0.4441888156839873\n",
      "Gradient Descent(195/499): loss=0.4437992000645314\n",
      "Gradient Descent(196/499): loss=0.4434153309732684\n",
      "Gradient Descent(197/499): loss=0.4430370997407422\n",
      "Gradient Descent(198/499): loss=0.4426644001908165\n",
      "Gradient Descent(199/499): loss=0.4422971285747079\n",
      "Gradient Descent(200/499): loss=0.44193518350694083\n",
      "Gradient Descent(201/499): loss=0.4415784659031646\n",
      "Gradient Descent(202/499): loss=0.4412268789197744\n",
      "Gradient Descent(203/499): loss=0.44088032789528164\n",
      "Gradient Descent(204/499): loss=0.44053872029337576\n",
      "Gradient Descent(205/499): loss=0.4402019656476292\n",
      "Gradient Descent(206/499): loss=0.4398699755077911\n",
      "Gradient Descent(207/499): loss=0.4395426633876238\n",
      "Gradient Descent(208/499): loss=0.43921994471423165\n",
      "Gradient Descent(209/499): loss=0.4389017367788393\n",
      "Gradient Descent(210/499): loss=0.4385879586889719\n",
      "Gradient Descent(211/499): loss=0.4382785313219968\n",
      "Gradient Descent(212/499): loss=0.4379733772799834\n",
      "Gradient Descent(213/499): loss=0.4376724208458425\n",
      "Gradient Descent(214/499): loss=0.4373755879407043\n",
      "Gradient Descent(215/499): loss=0.4370828060824997\n",
      "Gradient Descent(216/499): loss=0.4367940043457058\n",
      "Gradient Descent(217/499): loss=0.43650911332222275\n",
      "Gradient Descent(218/499): loss=0.4362280650833463\n",
      "Gradient Descent(219/499): loss=0.43595079314280333\n",
      "Gradient Descent(220/499): loss=0.43567723242081907\n",
      "Gradient Descent(221/499): loss=0.4354073192091829\n",
      "Gradient Descent(222/499): loss=0.43514099113728644\n",
      "Gradient Descent(223/499): loss=0.43487818713910054\n",
      "Gradient Descent(224/499): loss=0.4346188474210665\n",
      "Gradient Descent(225/499): loss=0.43436291343087247\n",
      "Gradient Descent(226/499): loss=0.4341103278270892\n",
      "Gradient Descent(227/499): loss=0.43386103444963964\n",
      "Gradient Descent(228/499): loss=0.4336149782910776\n",
      "Gradient Descent(229/499): loss=0.43337210546865096\n",
      "Gradient Descent(230/499): loss=0.4331323631971275\n",
      "Gradient Descent(231/499): loss=0.4328956997623595\n",
      "Gradient Descent(232/499): loss=0.432662064495566\n",
      "Gradient Descent(233/499): loss=0.43243140774831207\n",
      "Gradient Descent(234/499): loss=0.4322036808681633\n",
      "Gradient Descent(235/499): loss=0.43197883617499677\n",
      "Gradient Descent(236/499): loss=0.4317568269379499\n",
      "Gradient Descent(237/499): loss=0.4315376073529862\n",
      "Gradient Descent(238/499): loss=0.4313211325210628\n",
      "Gradient Descent(239/499): loss=0.4311073584268802\n",
      "Gradient Descent(240/499): loss=0.4308962419181982\n",
      "Gradient Descent(241/499): loss=0.4306877406857015\n",
      "Gradient Descent(242/499): loss=0.43048181324339924\n",
      "Gradient Descent(243/499): loss=0.43027841890954244\n",
      "Gradient Descent(244/499): loss=0.4300775177880444\n",
      "Gradient Descent(245/499): loss=0.4298790707503918\n",
      "Gradient Descent(246/499): loss=0.4296830394180283\n",
      "Gradient Descent(247/499): loss=0.42948938614520094\n",
      "Gradient Descent(248/499): loss=0.4292980740022541\n",
      "Gradient Descent(249/499): loss=0.42910906675935845\n",
      "Gradient Descent(250/499): loss=0.4289223288706633\n",
      "Gradient Descent(251/499): loss=0.42873782545885947\n",
      "Gradient Descent(252/499): loss=0.42855552230014093\n",
      "Gradient Descent(253/499): loss=0.4283753858095546\n",
      "Gradient Descent(254/499): loss=0.4281973830267275\n",
      "Gradient Descent(255/499): loss=0.42802148160195913\n",
      "Gradient Descent(256/499): loss=0.42784764978267015\n",
      "Gradient Descent(257/499): loss=0.42767585640019734\n",
      "Gradient Descent(258/499): loss=0.4275060708569243\n",
      "Gradient Descent(259/499): loss=0.4273382631137385\n",
      "Gradient Descent(260/499): loss=0.42717240367780707\n",
      "Gradient Descent(261/499): loss=0.4270084635906599\n",
      "Gradient Descent(262/499): loss=0.4268464144165745\n",
      "Gradient Descent(263/499): loss=0.4266862282312509\n",
      "Gradient Descent(264/499): loss=0.42652787761077166\n",
      "Gradient Descent(265/499): loss=0.4263713356208368\n",
      "Gradient Descent(266/499): loss=0.4262165758062676\n",
      "Gradient Descent(267/499): loss=0.42606357218077096\n",
      "Gradient Descent(268/499): loss=0.4259122992169567\n",
      "Gradient Descent(269/499): loss=0.425762731836603\n",
      "Gradient Descent(270/499): loss=0.42561484540116046\n",
      "Gradient Descent(271/499): loss=0.4254686157024897\n",
      "Gradient Descent(272/499): loss=0.42532401895382654\n",
      "Gradient Descent(273/499): loss=0.42518103178096756\n",
      "Gradient Descent(274/499): loss=0.42503963121367005\n",
      "Gradient Descent(275/499): loss=0.4248997946772616\n",
      "Gradient Descent(276/499): loss=0.4247614999844526\n",
      "Gradient Descent(277/499): loss=0.4246247253273464\n",
      "Gradient Descent(278/499): loss=0.42448944926964244\n",
      "Gradient Descent(279/499): loss=0.4243556507390258\n",
      "Gradient Descent(280/499): loss=0.42422330901974015\n",
      "Gradient Descent(281/499): loss=0.4240924037453366\n",
      "Gradient Descent(282/499): loss=0.42396291489159693\n",
      "Gradient Descent(283/499): loss=0.4238348227696231\n",
      "Gradient Descent(284/499): loss=0.42370810801909153\n",
      "Gradient Descent(285/499): loss=0.4235827516016659\n",
      "Gradient Descent(286/499): loss=0.4234587347945653\n",
      "Gradient Descent(287/499): loss=0.42333603918428275\n",
      "Gradient Descent(288/499): loss=0.42321464666045167\n",
      "Gradient Descent(289/499): loss=0.4230945394098541\n",
      "Gradient Descent(290/499): loss=0.4229756999105687\n",
      "Gradient Descent(291/499): loss=0.4228581109262547\n",
      "Gradient Descent(292/499): loss=0.4227417555005677\n",
      "Gradient Descent(293/499): loss=0.4226266169517037\n",
      "Gradient Descent(294/499): loss=0.42251267886706895\n",
      "Gradient Descent(295/499): loss=0.4223999250980717\n",
      "Gradient Descent(296/499): loss=0.42228833975503255\n",
      "Gradient Descent(297/499): loss=0.4221779072022104\n",
      "Gradient Descent(298/499): loss=0.42206861205294227\n",
      "Gradient Descent(299/499): loss=0.4219604391648919\n",
      "Gradient Descent(300/499): loss=0.4218533736354057\n",
      "Gradient Descent(301/499): loss=0.42174740079697326\n",
      "Gradient Descent(302/499): loss=0.42164250621278954\n",
      "Gradient Descent(303/499): loss=0.42153867567241554\n",
      "Gradient Descent(304/499): loss=0.4214358951875366\n",
      "Gradient Descent(305/499): loss=0.4213341509878138\n",
      "Gradient Descent(306/499): loss=0.4212334295168273\n",
      "Gradient Descent(307/499): loss=0.42113371742810984\n",
      "Gradient Descent(308/499): loss=0.4210350015812662\n",
      "Gradient Descent(309/499): loss=0.4209372690381788\n",
      "Gradient Descent(310/499): loss=0.42084050705929493\n",
      "Gradient Descent(311/499): loss=0.420744703099996\n",
      "Gradient Descent(312/499): loss=0.4206498448070445\n",
      "Gradient Descent(313/499): loss=0.42055592001510866\n",
      "Gradient Descent(314/499): loss=0.42046291674336117\n",
      "Gradient Descent(315/499): loss=0.4203708231921512\n",
      "Gradient Descent(316/499): loss=0.42027962773974836\n",
      "Gradient Descent(317/499): loss=0.42018931893915484\n",
      "Gradient Descent(318/499): loss=0.4200998855149868\n",
      "Gradient Descent(319/499): loss=0.4200113163604202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(320/499): loss=0.419923600534203\n",
      "Gradient Descent(321/499): loss=0.41983672725772875\n",
      "Gradient Descent(322/499): loss=0.41975068591217224\n",
      "Gradient Descent(323/499): loss=0.4196654660356852\n",
      "Gradient Descent(324/499): loss=0.41958105732065026\n",
      "Gradient Descent(325/499): loss=0.41949744961099183\n",
      "Gradient Descent(326/499): loss=0.419414632899543\n",
      "Gradient Descent(327/499): loss=0.41933259732546635\n",
      "Gradient Descent(328/499): loss=0.41925133317172875\n",
      "Gradient Descent(329/499): loss=0.4191708308626262\n",
      "Gradient Descent(330/499): loss=0.4190910809613614\n",
      "Gradient Descent(331/499): loss=0.4190120741676689\n",
      "Gradient Descent(332/499): loss=0.4189338013154894\n",
      "Gradient Descent(333/499): loss=0.4188562533706912\n",
      "Gradient Descent(334/499): loss=0.41877942142883645\n",
      "Gradient Descent(335/499): loss=0.41870329671299356\n",
      "Gradient Descent(336/499): loss=0.4186278705715933\n",
      "Gradient Descent(337/499): loss=0.41855313447632664\n",
      "Gradient Descent(338/499): loss=0.41847908002008544\n",
      "Gradient Descent(339/499): loss=0.4184056989149439\n",
      "Gradient Descent(340/499): loss=0.41833298299017946\n",
      "Gradient Descent(341/499): loss=0.4182609241903327\n",
      "Gradient Descent(342/499): loss=0.41818951457330567\n",
      "Gradient Descent(343/499): loss=0.41811874630849766\n",
      "Gradient Descent(344/499): loss=0.41804861167497603\n",
      "Gradient Descent(345/499): loss=0.4179791030596842\n",
      "Gradient Descent(346/499): loss=0.41791021295568376\n",
      "Gradient Descent(347/499): loss=0.4178419339604298\n",
      "Gradient Descent(348/499): loss=0.41777425877408\n",
      "Gradient Descent(349/499): loss=0.417707180197837\n",
      "Gradient Descent(350/499): loss=0.41764069113232083\n",
      "Gradient Descent(351/499): loss=0.4175747845759734\n",
      "Gradient Descent(352/499): loss=0.4175094536234928\n",
      "Gradient Descent(353/499): loss=0.4174446914642978\n",
      "Gradient Descent(354/499): loss=0.4173804913810208\n",
      "Gradient Descent(355/499): loss=0.4173168467480289\n",
      "Gradient Descent(356/499): loss=0.4172537510299737\n",
      "Gradient Descent(357/499): loss=0.4171911977803666\n",
      "Gradient Descent(358/499): loss=0.41712918064018273\n",
      "Gradient Descent(359/499): loss=0.41706769333648824\n",
      "Gradient Descent(360/499): loss=0.41700672968109537\n",
      "Gradient Descent(361/499): loss=0.4169462835692405\n",
      "Gradient Descent(362/499): loss=0.4168863489782878\n",
      "Gradient Descent(363/499): loss=0.41682691996645566\n",
      "Gradient Descent(364/499): loss=0.41676799067156667\n",
      "Gradient Descent(365/499): loss=0.4167095553098207\n",
      "Gradient Descent(366/499): loss=0.41665160817458957\n",
      "Gradient Descent(367/499): loss=0.41659414363523445\n",
      "Gradient Descent(368/499): loss=0.4165371561359431\n",
      "Gradient Descent(369/499): loss=0.41648064019458964\n",
      "Gradient Descent(370/499): loss=0.4164245904016131\n",
      "Gradient Descent(371/499): loss=0.416369001418918\n",
      "Gradient Descent(372/499): loss=0.41631386797879216\n",
      "Gradient Descent(373/499): loss=0.41625918488284513\n",
      "Gradient Descent(374/499): loss=0.4162049470009655\n",
      "Gradient Descent(375/499): loss=0.4161511492702957\n",
      "Gradient Descent(376/499): loss=0.41609778669422554\n",
      "Gradient Descent(377/499): loss=0.41604485434140265\n",
      "Gradient Descent(378/499): loss=0.41599234734476104\n",
      "Gradient Descent(379/499): loss=0.4159402609005659\n",
      "Gradient Descent(380/499): loss=0.41588859026747527\n",
      "Gradient Descent(381/499): loss=0.41583733076561746\n",
      "Gradient Descent(382/499): loss=0.41578647777568567\n",
      "Gradient Descent(383/499): loss=0.4157360267380462\n",
      "Gradient Descent(384/499): loss=0.41568597315186356\n",
      "Gradient Descent(385/499): loss=0.4156363125742396\n",
      "Gradient Descent(386/499): loss=0.415587040619368\n",
      "Gradient Descent(387/499): loss=0.4155381529577024\n",
      "Gradient Descent(388/499): loss=0.41548964531513904\n",
      "Gradient Descent(389/499): loss=0.41544151347221303\n",
      "Gradient Descent(390/499): loss=0.41539375326330813\n",
      "Gradient Descent(391/499): loss=0.41534636057588015\n",
      "Gradient Descent(392/499): loss=0.4152993313496924\n",
      "Gradient Descent(393/499): loss=0.41525266157606505\n",
      "Gradient Descent(394/499): loss=0.41520634729713596\n",
      "Gradient Descent(395/499): loss=0.41516038460513455\n",
      "Gradient Descent(396/499): loss=0.4151147696416672\n",
      "Gradient Descent(397/499): loss=0.415069498597014\n",
      "Gradient Descent(398/499): loss=0.4150245677094386\n",
      "Gradient Descent(399/499): loss=0.4149799732645075\n",
      "Gradient Descent(400/499): loss=0.41493571159442133\n",
      "Gradient Descent(401/499): loss=0.4148917790773576\n",
      "Gradient Descent(402/499): loss=0.41484817213682285\n",
      "Gradient Descent(403/499): loss=0.41480488724101594\n",
      "Gradient Descent(404/499): loss=0.4147619209022022\n",
      "Gradient Descent(405/499): loss=0.41471926967609646\n",
      "Gradient Descent(406/499): loss=0.41467693016125673\n",
      "Gradient Descent(407/499): loss=0.4146348989984877\n",
      "Gradient Descent(408/499): loss=0.4145931728702532\n",
      "Gradient Descent(409/499): loss=0.41455174850009846\n",
      "Gradient Descent(410/499): loss=0.4145106226520811\n",
      "Gradient Descent(411/499): loss=0.4144697921302122\n",
      "Gradient Descent(412/499): loss=0.41442925377790435\n",
      "Gradient Descent(413/499): loss=0.4143890044774306\n",
      "Gradient Descent(414/499): loss=0.41434904114938964\n",
      "Gradient Descent(415/499): loss=0.41430936075218133\n",
      "Gradient Descent(416/499): loss=0.4142699602814894\n",
      "Gradient Descent(417/499): loss=0.41423083676977174\n",
      "Gradient Descent(418/499): loss=0.4141919872857607\n",
      "Gradient Descent(419/499): loss=0.41415340893396785\n",
      "Gradient Descent(420/499): loss=0.41411509885419984\n",
      "Gradient Descent(421/499): loss=0.41407705422107927\n",
      "Gradient Descent(422/499): loss=0.41403927224357406\n",
      "Gradient Descent(423/499): loss=0.41400175016453356\n",
      "Gradient Descent(424/499): loss=0.41396448526023233\n",
      "Gradient Descent(425/499): loss=0.41392747483992015\n",
      "Gradient Descent(426/499): loss=0.4138907162453797\n",
      "Gradient Descent(427/499): loss=0.4138542068504899\n",
      "Gradient Descent(428/499): loss=0.4138179440607968\n",
      "Gradient Descent(429/499): loss=0.41378192531309116\n",
      "Gradient Descent(430/499): loss=0.41374614807499066\n",
      "Gradient Descent(431/499): loss=0.41371060984453095\n",
      "Gradient Descent(432/499): loss=0.4136753081497605\n",
      "Gradient Descent(433/499): loss=0.41364024054834286\n",
      "Gradient Descent(434/499): loss=0.4136054046271648\n",
      "Gradient Descent(435/499): loss=0.4135707980019494\n",
      "Gradient Descent(436/499): loss=0.413536418316876\n",
      "Gradient Descent(437/499): loss=0.4135022632442051\n",
      "Gradient Descent(438/499): loss=0.4134683304839087\n",
      "Gradient Descent(439/499): loss=0.4134346177633073\n",
      "Gradient Descent(440/499): loss=0.4134011228367098\n",
      "Gradient Descent(441/499): loss=0.41336784348506184\n",
      "Gradient Descent(442/499): loss=0.4133347775155968\n",
      "Gradient Descent(443/499): loss=0.4133019227614924\n",
      "Gradient Descent(444/499): loss=0.4132692770815342\n",
      "Gradient Descent(445/499): loss=0.4132368383597805\n",
      "Gradient Descent(446/499): loss=0.4132046045052358\n",
      "Gradient Descent(447/499): loss=0.4131725734515265\n",
      "Gradient Descent(448/499): loss=0.41314074315658206\n",
      "Gradient Descent(449/499): loss=0.4131091116023211\n",
      "Gradient Descent(450/499): loss=0.4130776767943415\n",
      "Gradient Descent(451/499): loss=0.41304643676161545\n",
      "Gradient Descent(452/499): loss=0.4130153895561882\n",
      "Gradient Descent(453/499): loss=0.4129845332528819\n",
      "Gradient Descent(454/499): loss=0.4129538659490032\n",
      "Gradient Descent(455/499): loss=0.41292338576405474\n",
      "Gradient Descent(456/499): loss=0.4128930908394518\n",
      "Gradient Descent(457/499): loss=0.4128629793382419\n",
      "Gradient Descent(458/499): loss=0.4128330494448285\n",
      "Gradient Descent(459/499): loss=0.41280329936469967\n",
      "Gradient Descent(460/499): loss=0.41277372732415907\n",
      "Gradient Descent(461/499): loss=0.41274433157006163\n",
      "Gradient Descent(462/499): loss=0.4127151103695531\n",
      "Gradient Descent(463/499): loss=0.4126860620098124\n",
      "Gradient Descent(464/499): loss=0.41265718479779867\n",
      "Gradient Descent(465/499): loss=0.4126284770600007\n",
      "Gradient Descent(466/499): loss=0.41259993714219106\n",
      "Gradient Descent(467/499): loss=0.4125715634091824\n",
      "Gradient Descent(468/499): loss=0.41254335424458843\n",
      "Gradient Descent(469/499): loss=0.41251530805058695\n",
      "Gradient Descent(470/499): loss=0.41248742324768717\n",
      "Gradient Descent(471/499): loss=0.4124596982744998\n",
      "Gradient Descent(472/499): loss=0.41243213158751035\n",
      "Gradient Descent(473/499): loss=0.4124047216608554\n",
      "Gradient Descent(474/499): loss=0.4123774669861024\n",
      "Gradient Descent(475/499): loss=0.41235036607203196\n",
      "Gradient Descent(476/499): loss=0.4123234174444236\n",
      "Gradient Descent(477/499): loss=0.4122966196458439\n",
      "Gradient Descent(478/499): loss=0.4122699712354384\n",
      "Gradient Descent(479/499): loss=0.41224347078872486\n",
      "Gradient Descent(480/499): loss=0.4122171168973913\n",
      "Gradient Descent(481/499): loss=0.41219090816909526\n",
      "Gradient Descent(482/499): loss=0.41216484322726576\n",
      "Gradient Descent(483/499): loss=0.4121389207109096\n",
      "Gradient Descent(484/499): loss=0.41211313927441795\n",
      "Gradient Descent(485/499): loss=0.4120874975873774\n",
      "Gradient Descent(486/499): loss=0.412061994334383\n",
      "Gradient Descent(487/499): loss=0.4120366282148532\n",
      "Gradient Descent(488/499): loss=0.4120113979428478\n",
      "Gradient Descent(489/499): loss=0.411986302246889\n",
      "Gradient Descent(490/499): loss=0.41196133986978317\n",
      "Gradient Descent(491/499): loss=0.4119365095684465\n",
      "Gradient Descent(492/499): loss=0.41191181011373307\n",
      "Gradient Descent(493/499): loss=0.4118872402902632\n",
      "Gradient Descent(494/499): loss=0.41186279889625665\n",
      "Gradient Descent(495/499): loss=0.41183848474336665\n",
      "Gradient Descent(496/499): loss=0.4118142966565158\n",
      "Gradient Descent(497/499): loss=0.4117902334737356\n",
      "Gradient Descent(498/499): loss=0.41176629404600623\n",
      "Gradient Descent(499/499): loss=0.4117424772371001\n",
      "0.7060266666666667\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "(w1,loss1) = least_squares_GD(y, tX, init_w, max_iter, alpha)\n",
    "gd_tr_pred = predict_labels(w1, tX_valid)\n",
    "print((gd_tr_pred == y_valid).mean())\n",
    "gd_pred = predict_labels(w1, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/499): loss=0.019737475721124543\n",
      "Stochastic Gradient Descent(1/499): loss=8.74648505546766\n",
      "Stochastic Gradient Descent(2/499): loss=2.6693925002566634\n",
      "Stochastic Gradient Descent(3/499): loss=0.028992837559970387\n",
      "Stochastic Gradient Descent(4/499): loss=0.45982636084022105\n",
      "Stochastic Gradient Descent(5/499): loss=2.066410350005182\n",
      "Stochastic Gradient Descent(6/499): loss=10.770847131757902\n",
      "Stochastic Gradient Descent(7/499): loss=12.39264073947729\n",
      "Stochastic Gradient Descent(8/499): loss=0.14530440635441372\n",
      "Stochastic Gradient Descent(9/499): loss=2.751803002495881\n",
      "Stochastic Gradient Descent(10/499): loss=13.66593493946173\n",
      "Stochastic Gradient Descent(11/499): loss=3.2617310943876383\n",
      "Stochastic Gradient Descent(12/499): loss=0.030252668998152623\n",
      "Stochastic Gradient Descent(13/499): loss=4.999194817489299\n",
      "Stochastic Gradient Descent(14/499): loss=0.13994436000579077\n",
      "Stochastic Gradient Descent(15/499): loss=3.4413051919807707\n",
      "Stochastic Gradient Descent(16/499): loss=0.7704279929919047\n",
      "Stochastic Gradient Descent(17/499): loss=13.735778773157366\n",
      "Stochastic Gradient Descent(18/499): loss=3.3395657705218387\n",
      "Stochastic Gradient Descent(19/499): loss=0.025088913408462605\n",
      "Stochastic Gradient Descent(20/499): loss=2.078964881884594\n",
      "Stochastic Gradient Descent(21/499): loss=0.0008588992378303705\n",
      "Stochastic Gradient Descent(22/499): loss=1.785619220024648\n",
      "Stochastic Gradient Descent(23/499): loss=24.0793557890192\n",
      "Stochastic Gradient Descent(24/499): loss=2.4885246117332955\n",
      "Stochastic Gradient Descent(25/499): loss=3.061471577134322\n",
      "Stochastic Gradient Descent(26/499): loss=22.350911678626602\n",
      "Stochastic Gradient Descent(27/499): loss=10.275272195332748\n",
      "Stochastic Gradient Descent(28/499): loss=4.198651463965405\n",
      "Stochastic Gradient Descent(29/499): loss=0.8803879738041878\n",
      "Stochastic Gradient Descent(30/499): loss=0.5336369074066171\n",
      "Stochastic Gradient Descent(31/499): loss=1.130784922912613\n",
      "Stochastic Gradient Descent(32/499): loss=4.292061200478701\n",
      "Stochastic Gradient Descent(33/499): loss=31.079268273945605\n",
      "Stochastic Gradient Descent(34/499): loss=0.39656290040438935\n",
      "Stochastic Gradient Descent(35/499): loss=0.330994611988027\n",
      "Stochastic Gradient Descent(36/499): loss=0.9562230109954148\n",
      "Stochastic Gradient Descent(37/499): loss=6.976234798073913\n",
      "Stochastic Gradient Descent(38/499): loss=2.9025676412560104\n",
      "Stochastic Gradient Descent(39/499): loss=3.157126320874037\n",
      "Stochastic Gradient Descent(40/499): loss=5.887506374557708\n",
      "Stochastic Gradient Descent(41/499): loss=0.01420305056179549\n",
      "Stochastic Gradient Descent(42/499): loss=0.4035768788224788\n",
      "Stochastic Gradient Descent(43/499): loss=11.207956305419811\n",
      "Stochastic Gradient Descent(44/499): loss=0.25208912583403004\n",
      "Stochastic Gradient Descent(45/499): loss=0.8268552692934236\n",
      "Stochastic Gradient Descent(46/499): loss=0.24092884374826556\n",
      "Stochastic Gradient Descent(47/499): loss=0.4865616400368696\n",
      "Stochastic Gradient Descent(48/499): loss=0.11822616210823302\n",
      "Stochastic Gradient Descent(49/499): loss=7.561559362249429\n",
      "Stochastic Gradient Descent(50/499): loss=0.017355908457787093\n",
      "Stochastic Gradient Descent(51/499): loss=7.055609710321597\n",
      "Stochastic Gradient Descent(52/499): loss=0.2628923409882754\n",
      "Stochastic Gradient Descent(53/499): loss=7.237974941291608\n",
      "Stochastic Gradient Descent(54/499): loss=0.07371031189705977\n",
      "Stochastic Gradient Descent(55/499): loss=5.770979610246077\n",
      "Stochastic Gradient Descent(56/499): loss=1.0547383819968104\n",
      "Stochastic Gradient Descent(57/499): loss=1.2376017500192653\n",
      "Stochastic Gradient Descent(58/499): loss=1.475925954844018\n",
      "Stochastic Gradient Descent(59/499): loss=0.06698064284790053\n",
      "Stochastic Gradient Descent(60/499): loss=3.4058891090487178\n",
      "Stochastic Gradient Descent(61/499): loss=19.971014664396755\n",
      "Stochastic Gradient Descent(62/499): loss=7.2262422768265395\n",
      "Stochastic Gradient Descent(63/499): loss=0.33883817549081074\n",
      "Stochastic Gradient Descent(64/499): loss=2.2309727180997223\n",
      "Stochastic Gradient Descent(65/499): loss=0.4377179590772767\n",
      "Stochastic Gradient Descent(66/499): loss=1.9219786426132501\n",
      "Stochastic Gradient Descent(67/499): loss=0.05245447157627594\n",
      "Stochastic Gradient Descent(68/499): loss=7.511686090529869\n",
      "Stochastic Gradient Descent(69/499): loss=9.959789234365875\n",
      "Stochastic Gradient Descent(70/499): loss=26.270725979766727\n",
      "Stochastic Gradient Descent(71/499): loss=3.240686460027375\n",
      "Stochastic Gradient Descent(72/499): loss=6.982504976738904\n",
      "Stochastic Gradient Descent(73/499): loss=12.821855407285069\n",
      "Stochastic Gradient Descent(74/499): loss=6.559469855813776\n",
      "Stochastic Gradient Descent(75/499): loss=0.48590363517965174\n",
      "Stochastic Gradient Descent(76/499): loss=0.7792270825120787\n",
      "Stochastic Gradient Descent(77/499): loss=0.2636658883722854\n",
      "Stochastic Gradient Descent(78/499): loss=7.312508932083554\n",
      "Stochastic Gradient Descent(79/499): loss=0.4867373523238679\n",
      "Stochastic Gradient Descent(80/499): loss=0.8180761143816717\n",
      "Stochastic Gradient Descent(81/499): loss=2.5670501876438694\n",
      "Stochastic Gradient Descent(82/499): loss=0.2653553538205797\n",
      "Stochastic Gradient Descent(83/499): loss=5.336051128245218\n",
      "Stochastic Gradient Descent(84/499): loss=1.817152483749451\n",
      "Stochastic Gradient Descent(85/499): loss=0.3138308057794535\n",
      "Stochastic Gradient Descent(86/499): loss=0.17298848467883837\n",
      "Stochastic Gradient Descent(87/499): loss=0.010753818427230399\n",
      "Stochastic Gradient Descent(88/499): loss=0.15686150972099894\n",
      "Stochastic Gradient Descent(89/499): loss=4.648230824739715\n",
      "Stochastic Gradient Descent(90/499): loss=6.44433207993973\n",
      "Stochastic Gradient Descent(91/499): loss=2.060308334404833\n",
      "Stochastic Gradient Descent(92/499): loss=0.08558285749398074\n",
      "Stochastic Gradient Descent(93/499): loss=1.1758215089371689\n",
      "Stochastic Gradient Descent(94/499): loss=0.6995036844978669\n",
      "Stochastic Gradient Descent(95/499): loss=3.868633703098723\n",
      "Stochastic Gradient Descent(96/499): loss=6.9249684966798934\n",
      "Stochastic Gradient Descent(97/499): loss=2.4605438028618676\n",
      "Stochastic Gradient Descent(98/499): loss=12.937320231681904\n",
      "Stochastic Gradient Descent(99/499): loss=19.18235712486599\n",
      "Stochastic Gradient Descent(100/499): loss=15.285322778998351\n",
      "Stochastic Gradient Descent(101/499): loss=1.646112684196802\n",
      "Stochastic Gradient Descent(102/499): loss=10.629157575898764\n",
      "Stochastic Gradient Descent(103/499): loss=0.642667868555889\n",
      "Stochastic Gradient Descent(104/499): loss=0.1820568733604522\n",
      "Stochastic Gradient Descent(105/499): loss=1.8436559610308823\n",
      "Stochastic Gradient Descent(106/499): loss=1.4118329417273274\n",
      "Stochastic Gradient Descent(107/499): loss=0.2476304536485879\n",
      "Stochastic Gradient Descent(108/499): loss=0.00757958143947752\n",
      "Stochastic Gradient Descent(109/499): loss=6.42158749821556\n",
      "Stochastic Gradient Descent(110/499): loss=0.5086530565650721\n",
      "Stochastic Gradient Descent(111/499): loss=1.4938442270828318\n",
      "Stochastic Gradient Descent(112/499): loss=0.013448638965995154\n",
      "Stochastic Gradient Descent(113/499): loss=5.514324473028929\n",
      "Stochastic Gradient Descent(114/499): loss=2.21755055596272\n",
      "Stochastic Gradient Descent(115/499): loss=1.8812624784459595\n",
      "Stochastic Gradient Descent(116/499): loss=0.036285100661554485\n",
      "Stochastic Gradient Descent(117/499): loss=2.295479817158839\n",
      "Stochastic Gradient Descent(118/499): loss=7.021904282857235\n",
      "Stochastic Gradient Descent(119/499): loss=13.193777275478894\n",
      "Stochastic Gradient Descent(120/499): loss=3.3336810203723175\n",
      "Stochastic Gradient Descent(121/499): loss=0.9641254144896263\n",
      "Stochastic Gradient Descent(122/499): loss=0.05373721159912023\n",
      "Stochastic Gradient Descent(123/499): loss=0.6743699519460215\n",
      "Stochastic Gradient Descent(124/499): loss=5.686464492305344\n",
      "Stochastic Gradient Descent(125/499): loss=7.890115709491078\n",
      "Stochastic Gradient Descent(126/499): loss=63.24270889701042\n",
      "Stochastic Gradient Descent(127/499): loss=1.0121299163695918\n",
      "Stochastic Gradient Descent(128/499): loss=1.8495517561692776\n",
      "Stochastic Gradient Descent(129/499): loss=1.608772130997854\n",
      "Stochastic Gradient Descent(130/499): loss=6.398957907982565\n",
      "Stochastic Gradient Descent(131/499): loss=3.8905803300395765\n",
      "Stochastic Gradient Descent(132/499): loss=1.3066733486725688\n",
      "Stochastic Gradient Descent(133/499): loss=4.400833771063235\n",
      "Stochastic Gradient Descent(134/499): loss=0.8561828143154722\n",
      "Stochastic Gradient Descent(135/499): loss=6.142643430363562\n",
      "Stochastic Gradient Descent(136/499): loss=13.151740785764625\n",
      "Stochastic Gradient Descent(137/499): loss=0.002082341619301834\n",
      "Stochastic Gradient Descent(138/499): loss=0.00026495476062919854\n",
      "Stochastic Gradient Descent(139/499): loss=0.5343334017311456\n",
      "Stochastic Gradient Descent(140/499): loss=24.17301674958882\n",
      "Stochastic Gradient Descent(141/499): loss=2.4734749849605673\n",
      "Stochastic Gradient Descent(142/499): loss=0.19036288235120788\n",
      "Stochastic Gradient Descent(143/499): loss=9.691673898856978\n",
      "Stochastic Gradient Descent(144/499): loss=0.2999280438650959\n",
      "Stochastic Gradient Descent(145/499): loss=8.354415850099919\n",
      "Stochastic Gradient Descent(146/499): loss=0.7228112566677631\n",
      "Stochastic Gradient Descent(147/499): loss=3.3610604181765513\n",
      "Stochastic Gradient Descent(148/499): loss=28.53907662122733\n",
      "Stochastic Gradient Descent(149/499): loss=1.1349959843640514\n",
      "Stochastic Gradient Descent(150/499): loss=1.6964116940538385\n",
      "Stochastic Gradient Descent(151/499): loss=0.43009067608764673\n",
      "Stochastic Gradient Descent(152/499): loss=9.79794714756732\n",
      "Stochastic Gradient Descent(153/499): loss=0.09129579663105626\n",
      "Stochastic Gradient Descent(154/499): loss=0.0014684067359105567\n",
      "Stochastic Gradient Descent(155/499): loss=0.08148696916198908\n",
      "Stochastic Gradient Descent(156/499): loss=1.1654011291467576\n",
      "Stochastic Gradient Descent(157/499): loss=16.845002864780767\n",
      "Stochastic Gradient Descent(158/499): loss=0.2005381138878405\n",
      "Stochastic Gradient Descent(159/499): loss=1.1187669316064155\n",
      "Stochastic Gradient Descent(160/499): loss=3.476114577134082\n",
      "Stochastic Gradient Descent(161/499): loss=0.007504617651010043\n",
      "Stochastic Gradient Descent(162/499): loss=12.628927897254663\n",
      "Stochastic Gradient Descent(163/499): loss=0.1464351941899495\n",
      "Stochastic Gradient Descent(164/499): loss=1.7700344514048714\n",
      "Stochastic Gradient Descent(165/499): loss=0.03368270324336067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(166/499): loss=1.0109907973807157\n",
      "Stochastic Gradient Descent(167/499): loss=0.5897399044829541\n",
      "Stochastic Gradient Descent(168/499): loss=0.13889494421195642\n",
      "Stochastic Gradient Descent(169/499): loss=0.34864809706348043\n",
      "Stochastic Gradient Descent(170/499): loss=5.242197448136968\n",
      "Stochastic Gradient Descent(171/499): loss=2.6217487442953926\n",
      "Stochastic Gradient Descent(172/499): loss=0.002760883796889363\n",
      "Stochastic Gradient Descent(173/499): loss=5.215890330798556\n",
      "Stochastic Gradient Descent(174/499): loss=0.59195000895803\n",
      "Stochastic Gradient Descent(175/499): loss=14.223422146424731\n",
      "Stochastic Gradient Descent(176/499): loss=26.32705046642058\n",
      "Stochastic Gradient Descent(177/499): loss=7.455334880905274\n",
      "Stochastic Gradient Descent(178/499): loss=0.3187295972104611\n",
      "Stochastic Gradient Descent(179/499): loss=16.047046971218744\n",
      "Stochastic Gradient Descent(180/499): loss=5.373981517366374\n",
      "Stochastic Gradient Descent(181/499): loss=0.7623450535595617\n",
      "Stochastic Gradient Descent(182/499): loss=0.7178800946514787\n",
      "Stochastic Gradient Descent(183/499): loss=0.4099309975468415\n",
      "Stochastic Gradient Descent(184/499): loss=1.182106886336665\n",
      "Stochastic Gradient Descent(185/499): loss=8.964242669256707\n",
      "Stochastic Gradient Descent(186/499): loss=0.008295554089991813\n",
      "Stochastic Gradient Descent(187/499): loss=0.09753302502052107\n",
      "Stochastic Gradient Descent(188/499): loss=6.741288772825176\n",
      "Stochastic Gradient Descent(189/499): loss=0.006084762211069053\n",
      "Stochastic Gradient Descent(190/499): loss=4.591025284733959\n",
      "Stochastic Gradient Descent(191/499): loss=6.573709580107772\n",
      "Stochastic Gradient Descent(192/499): loss=3.65704349387965\n",
      "Stochastic Gradient Descent(193/499): loss=0.668032294969016\n",
      "Stochastic Gradient Descent(194/499): loss=0.0033494524723132815\n",
      "Stochastic Gradient Descent(195/499): loss=0.868418998654443\n",
      "Stochastic Gradient Descent(196/499): loss=0.42008348492128367\n",
      "Stochastic Gradient Descent(197/499): loss=5.537864309274766\n",
      "Stochastic Gradient Descent(198/499): loss=7.871213394308728\n",
      "Stochastic Gradient Descent(199/499): loss=9.627042490544163\n",
      "Stochastic Gradient Descent(200/499): loss=3.2549364859773684\n",
      "Stochastic Gradient Descent(201/499): loss=0.0037393276751994413\n",
      "Stochastic Gradient Descent(202/499): loss=0.3294084994432796\n",
      "Stochastic Gradient Descent(203/499): loss=3.283889859944088e-05\n",
      "Stochastic Gradient Descent(204/499): loss=2.6081077365377\n",
      "Stochastic Gradient Descent(205/499): loss=0.1205185769757381\n",
      "Stochastic Gradient Descent(206/499): loss=7.501182937183054\n",
      "Stochastic Gradient Descent(207/499): loss=2.9500781211311704\n",
      "Stochastic Gradient Descent(208/499): loss=0.05357660607776942\n",
      "Stochastic Gradient Descent(209/499): loss=1.1618766045117206\n",
      "Stochastic Gradient Descent(210/499): loss=3.6540110751437895\n",
      "Stochastic Gradient Descent(211/499): loss=3.0624875622199084\n",
      "Stochastic Gradient Descent(212/499): loss=1.2404970440561771\n",
      "Stochastic Gradient Descent(213/499): loss=2.547252383480466\n",
      "Stochastic Gradient Descent(214/499): loss=76.90186435148806\n",
      "Stochastic Gradient Descent(215/499): loss=1.8112156868091036e-05\n",
      "Stochastic Gradient Descent(216/499): loss=0.3302885028951026\n",
      "Stochastic Gradient Descent(217/499): loss=10.26687274704193\n",
      "Stochastic Gradient Descent(218/499): loss=0.08330289591514557\n",
      "Stochastic Gradient Descent(219/499): loss=1.3695260984898456\n",
      "Stochastic Gradient Descent(220/499): loss=2.7731754650420055\n",
      "Stochastic Gradient Descent(221/499): loss=3.153920484261881\n",
      "Stochastic Gradient Descent(222/499): loss=14.023949562117751\n",
      "Stochastic Gradient Descent(223/499): loss=11.296006391144163\n",
      "Stochastic Gradient Descent(224/499): loss=1.2313355738451104\n",
      "Stochastic Gradient Descent(225/499): loss=17.444636353284288\n",
      "Stochastic Gradient Descent(226/499): loss=2.419796219503265\n",
      "Stochastic Gradient Descent(227/499): loss=0.5547772864352902\n",
      "Stochastic Gradient Descent(228/499): loss=7.797823365779064\n",
      "Stochastic Gradient Descent(229/499): loss=0.67458613092269\n",
      "Stochastic Gradient Descent(230/499): loss=4.552921353783874\n",
      "Stochastic Gradient Descent(231/499): loss=1.4139304179565357\n",
      "Stochastic Gradient Descent(232/499): loss=1.5482131054609432\n",
      "Stochastic Gradient Descent(233/499): loss=4.3108773970009775\n",
      "Stochastic Gradient Descent(234/499): loss=5.073807787351915\n",
      "Stochastic Gradient Descent(235/499): loss=0.2508654197038313\n",
      "Stochastic Gradient Descent(236/499): loss=0.6619767244537738\n",
      "Stochastic Gradient Descent(237/499): loss=9.916231562393941\n",
      "Stochastic Gradient Descent(238/499): loss=0.13530566795281843\n",
      "Stochastic Gradient Descent(239/499): loss=4.690274180337343\n",
      "Stochastic Gradient Descent(240/499): loss=2.2735350813934563\n",
      "Stochastic Gradient Descent(241/499): loss=0.7556987168976786\n",
      "Stochastic Gradient Descent(242/499): loss=0.05487359050915895\n",
      "Stochastic Gradient Descent(243/499): loss=0.2921794788573902\n",
      "Stochastic Gradient Descent(244/499): loss=1.1248767625693818\n",
      "Stochastic Gradient Descent(245/499): loss=8.535912711782315\n",
      "Stochastic Gradient Descent(246/499): loss=3.4218975407372225\n",
      "Stochastic Gradient Descent(247/499): loss=0.009492136163078634\n",
      "Stochastic Gradient Descent(248/499): loss=0.1879025869262124\n",
      "Stochastic Gradient Descent(249/499): loss=0.2496297222278626\n",
      "Stochastic Gradient Descent(250/499): loss=6.284749420628673\n",
      "Stochastic Gradient Descent(251/499): loss=8.603121050908515\n",
      "Stochastic Gradient Descent(252/499): loss=1.3124623549735432\n",
      "Stochastic Gradient Descent(253/499): loss=10.0201426674671\n",
      "Stochastic Gradient Descent(254/499): loss=2.4091561607022665\n",
      "Stochastic Gradient Descent(255/499): loss=6.586272573488483\n",
      "Stochastic Gradient Descent(256/499): loss=0.31291194189541666\n",
      "Stochastic Gradient Descent(257/499): loss=2.3917039036466647\n",
      "Stochastic Gradient Descent(258/499): loss=0.4846023688583753\n",
      "Stochastic Gradient Descent(259/499): loss=0.7293745062346779\n",
      "Stochastic Gradient Descent(260/499): loss=0.295882778666601\n",
      "Stochastic Gradient Descent(261/499): loss=1.9586055118265246\n",
      "Stochastic Gradient Descent(262/499): loss=0.3301781009168239\n",
      "Stochastic Gradient Descent(263/499): loss=0.21455336766970717\n",
      "Stochastic Gradient Descent(264/499): loss=6.403881641002597\n",
      "Stochastic Gradient Descent(265/499): loss=1.6424654659089164\n",
      "Stochastic Gradient Descent(266/499): loss=2.2290354231372786\n",
      "Stochastic Gradient Descent(267/499): loss=3.67534420854321\n",
      "Stochastic Gradient Descent(268/499): loss=2.677674534841271\n",
      "Stochastic Gradient Descent(269/499): loss=0.08986928900084559\n",
      "Stochastic Gradient Descent(270/499): loss=3.891232296838314\n",
      "Stochastic Gradient Descent(271/499): loss=12.814257250409428\n",
      "Stochastic Gradient Descent(272/499): loss=0.009311823588911389\n",
      "Stochastic Gradient Descent(273/499): loss=21.05248528648553\n",
      "Stochastic Gradient Descent(274/499): loss=2.1415691841375586\n",
      "Stochastic Gradient Descent(275/499): loss=0.006696454486047422\n",
      "Stochastic Gradient Descent(276/499): loss=1.2036613043449949\n",
      "Stochastic Gradient Descent(277/499): loss=0.009636384575731854\n",
      "Stochastic Gradient Descent(278/499): loss=2.0554369570234647\n",
      "Stochastic Gradient Descent(279/499): loss=3.297485402557917\n",
      "Stochastic Gradient Descent(280/499): loss=0.4203792921153934\n",
      "Stochastic Gradient Descent(281/499): loss=1.0678212758439534\n",
      "Stochastic Gradient Descent(282/499): loss=2.749493477446991\n",
      "Stochastic Gradient Descent(283/499): loss=0.33839397652910164\n",
      "Stochastic Gradient Descent(284/499): loss=1.1656383424291414\n",
      "Stochastic Gradient Descent(285/499): loss=2.0051114581732756\n",
      "Stochastic Gradient Descent(286/499): loss=4.40478722775799\n",
      "Stochastic Gradient Descent(287/499): loss=1.7752121262021474\n",
      "Stochastic Gradient Descent(288/499): loss=3.8880536669812464\n",
      "Stochastic Gradient Descent(289/499): loss=0.4856055941105722\n",
      "Stochastic Gradient Descent(290/499): loss=5.098667093775344\n",
      "Stochastic Gradient Descent(291/499): loss=0.6107887854445294\n",
      "Stochastic Gradient Descent(292/499): loss=5.430503696972228\n",
      "Stochastic Gradient Descent(293/499): loss=2.1758674890839584\n",
      "Stochastic Gradient Descent(294/499): loss=0.7291451051488221\n",
      "Stochastic Gradient Descent(295/499): loss=4.916788582704461\n",
      "Stochastic Gradient Descent(296/499): loss=0.023112449981480227\n",
      "Stochastic Gradient Descent(297/499): loss=3.492795740737133\n",
      "Stochastic Gradient Descent(298/499): loss=5.382507970150429\n",
      "Stochastic Gradient Descent(299/499): loss=0.1322753479485891\n",
      "Stochastic Gradient Descent(300/499): loss=1.7702599273975044\n",
      "Stochastic Gradient Descent(301/499): loss=8.325658008676172\n",
      "Stochastic Gradient Descent(302/499): loss=10.522029749448302\n",
      "Stochastic Gradient Descent(303/499): loss=0.01802092616407093\n",
      "Stochastic Gradient Descent(304/499): loss=0.13484512268011556\n",
      "Stochastic Gradient Descent(305/499): loss=2.331785810370728\n",
      "Stochastic Gradient Descent(306/499): loss=8.963973329202851\n",
      "Stochastic Gradient Descent(307/499): loss=1.9072116882619372\n",
      "Stochastic Gradient Descent(308/499): loss=8.717472484408647\n",
      "Stochastic Gradient Descent(309/499): loss=12.422308169602836\n",
      "Stochastic Gradient Descent(310/499): loss=0.19795849823815903\n",
      "Stochastic Gradient Descent(311/499): loss=2.319043337912147\n",
      "Stochastic Gradient Descent(312/499): loss=2.8150089874373756\n",
      "Stochastic Gradient Descent(313/499): loss=2.92277661756895\n",
      "Stochastic Gradient Descent(314/499): loss=47.055096134812295\n",
      "Stochastic Gradient Descent(315/499): loss=1.8072128647759853\n",
      "Stochastic Gradient Descent(316/499): loss=7.929640512892485\n",
      "Stochastic Gradient Descent(317/499): loss=1.3838028516927574\n",
      "Stochastic Gradient Descent(318/499): loss=2.3606226253617777\n",
      "Stochastic Gradient Descent(319/499): loss=2.4776408233906517\n",
      "Stochastic Gradient Descent(320/499): loss=1.8478656294731854\n",
      "Stochastic Gradient Descent(321/499): loss=2.4719589699868583\n",
      "Stochastic Gradient Descent(322/499): loss=0.003320540577738032\n",
      "Stochastic Gradient Descent(323/499): loss=1.527608560412861\n",
      "Stochastic Gradient Descent(324/499): loss=0.008237115041586557\n",
      "Stochastic Gradient Descent(325/499): loss=37.57592587343764\n",
      "Stochastic Gradient Descent(326/499): loss=7.430891243247648\n",
      "Stochastic Gradient Descent(327/499): loss=2.5064106986970924\n",
      "Stochastic Gradient Descent(328/499): loss=0.5937984784517658\n",
      "Stochastic Gradient Descent(329/499): loss=2.7872858837094023\n",
      "Stochastic Gradient Descent(330/499): loss=0.6511267483968356\n",
      "Stochastic Gradient Descent(331/499): loss=4.888880038864688\n",
      "Stochastic Gradient Descent(332/499): loss=1.0858325609686863\n",
      "Stochastic Gradient Descent(333/499): loss=9.36906877474251\n",
      "Stochastic Gradient Descent(334/499): loss=5.597635401095005\n",
      "Stochastic Gradient Descent(335/499): loss=1.4967579198604855\n",
      "Stochastic Gradient Descent(336/499): loss=0.016673816454843414\n",
      "Stochastic Gradient Descent(337/499): loss=1.2877126622731878\n",
      "Stochastic Gradient Descent(338/499): loss=9.088117139879408\n",
      "Stochastic Gradient Descent(339/499): loss=1.9225440198100776\n",
      "Stochastic Gradient Descent(340/499): loss=8.412419843496366\n",
      "Stochastic Gradient Descent(341/499): loss=0.0018878532614279788\n",
      "Stochastic Gradient Descent(342/499): loss=0.005545238155823468\n",
      "Stochastic Gradient Descent(343/499): loss=9.919662001528804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(344/499): loss=2.261993197280613\n",
      "Stochastic Gradient Descent(345/499): loss=0.3353253223166464\n",
      "Stochastic Gradient Descent(346/499): loss=0.0001248982942883417\n",
      "Stochastic Gradient Descent(347/499): loss=2.5565865079355063\n",
      "Stochastic Gradient Descent(348/499): loss=3.4057711305540623\n",
      "Stochastic Gradient Descent(349/499): loss=0.8574153681768067\n",
      "Stochastic Gradient Descent(350/499): loss=83.49935179961429\n",
      "Stochastic Gradient Descent(351/499): loss=1.3381399222226558\n",
      "Stochastic Gradient Descent(352/499): loss=0.2729691800371401\n",
      "Stochastic Gradient Descent(353/499): loss=0.012337408107425816\n",
      "Stochastic Gradient Descent(354/499): loss=0.17233953182391393\n",
      "Stochastic Gradient Descent(355/499): loss=0.8226091188467377\n",
      "Stochastic Gradient Descent(356/499): loss=13.424511072498474\n",
      "Stochastic Gradient Descent(357/499): loss=18.664356266459315\n",
      "Stochastic Gradient Descent(358/499): loss=0.12420617276476896\n",
      "Stochastic Gradient Descent(359/499): loss=5.703835488479696\n",
      "Stochastic Gradient Descent(360/499): loss=0.005928139896438719\n",
      "Stochastic Gradient Descent(361/499): loss=1.2142362706551586\n",
      "Stochastic Gradient Descent(362/499): loss=0.013148299449337137\n",
      "Stochastic Gradient Descent(363/499): loss=1.6310794006487308\n",
      "Stochastic Gradient Descent(364/499): loss=0.29068703568276705\n",
      "Stochastic Gradient Descent(365/499): loss=0.6648154685536759\n",
      "Stochastic Gradient Descent(366/499): loss=1.1398315520868583\n",
      "Stochastic Gradient Descent(367/499): loss=0.883301476002122\n",
      "Stochastic Gradient Descent(368/499): loss=6.995895232521601\n",
      "Stochastic Gradient Descent(369/499): loss=0.7130275058955827\n",
      "Stochastic Gradient Descent(370/499): loss=19.794226654402998\n",
      "Stochastic Gradient Descent(371/499): loss=13.963833581969723\n",
      "Stochastic Gradient Descent(372/499): loss=15.532822161305884\n",
      "Stochastic Gradient Descent(373/499): loss=6.346183125798691e-06\n",
      "Stochastic Gradient Descent(374/499): loss=1.836920186545141\n",
      "Stochastic Gradient Descent(375/499): loss=0.9154973798540296\n",
      "Stochastic Gradient Descent(376/499): loss=0.8459074097478466\n",
      "Stochastic Gradient Descent(377/499): loss=11.591710775223797\n",
      "Stochastic Gradient Descent(378/499): loss=0.3377329145974622\n",
      "Stochastic Gradient Descent(379/499): loss=0.0001812068219620163\n",
      "Stochastic Gradient Descent(380/499): loss=4.685769270131718\n",
      "Stochastic Gradient Descent(381/499): loss=0.7070008059160724\n",
      "Stochastic Gradient Descent(382/499): loss=1.629069903055316\n",
      "Stochastic Gradient Descent(383/499): loss=10.72840190687873\n",
      "Stochastic Gradient Descent(384/499): loss=0.2295191793673261\n",
      "Stochastic Gradient Descent(385/499): loss=0.3366618773156894\n",
      "Stochastic Gradient Descent(386/499): loss=7.552781836147269\n",
      "Stochastic Gradient Descent(387/499): loss=11.553012262951212\n",
      "Stochastic Gradient Descent(388/499): loss=0.38925803629561034\n",
      "Stochastic Gradient Descent(389/499): loss=0.20629940844907807\n",
      "Stochastic Gradient Descent(390/499): loss=16.03452503139863\n",
      "Stochastic Gradient Descent(391/499): loss=0.014037493827054114\n",
      "Stochastic Gradient Descent(392/499): loss=0.28770583914842024\n",
      "Stochastic Gradient Descent(393/499): loss=8.510773882425024\n",
      "Stochastic Gradient Descent(394/499): loss=1.3593350555602044\n",
      "Stochastic Gradient Descent(395/499): loss=5.9022568245514\n",
      "Stochastic Gradient Descent(396/499): loss=1.875858910625074\n",
      "Stochastic Gradient Descent(397/499): loss=11.290923149791546\n",
      "Stochastic Gradient Descent(398/499): loss=24.1060692336119\n",
      "Stochastic Gradient Descent(399/499): loss=0.435237681562478\n",
      "Stochastic Gradient Descent(400/499): loss=0.0014331368409637969\n",
      "Stochastic Gradient Descent(401/499): loss=1.856218598374774\n",
      "Stochastic Gradient Descent(402/499): loss=2.5938335912299704\n",
      "Stochastic Gradient Descent(403/499): loss=0.7305208506754753\n",
      "Stochastic Gradient Descent(404/499): loss=0.02171637667063003\n",
      "Stochastic Gradient Descent(405/499): loss=1.905564428602417\n",
      "Stochastic Gradient Descent(406/499): loss=0.19867206854820912\n",
      "Stochastic Gradient Descent(407/499): loss=0.29438002733260643\n",
      "Stochastic Gradient Descent(408/499): loss=7.36692423651546\n",
      "Stochastic Gradient Descent(409/499): loss=0.15380384200188651\n",
      "Stochastic Gradient Descent(410/499): loss=0.21350090020198068\n",
      "Stochastic Gradient Descent(411/499): loss=4.506844418552788\n",
      "Stochastic Gradient Descent(412/499): loss=9.86294817440834\n",
      "Stochastic Gradient Descent(413/499): loss=0.44142068183726696\n",
      "Stochastic Gradient Descent(414/499): loss=3.1636595159112746\n",
      "Stochastic Gradient Descent(415/499): loss=2.294379574094756\n",
      "Stochastic Gradient Descent(416/499): loss=0.4467016285882376\n",
      "Stochastic Gradient Descent(417/499): loss=10.114312616972512\n",
      "Stochastic Gradient Descent(418/499): loss=1.2573846328551195\n",
      "Stochastic Gradient Descent(419/499): loss=1.0496544859281094\n",
      "Stochastic Gradient Descent(420/499): loss=11.099610990292351\n",
      "Stochastic Gradient Descent(421/499): loss=3.932675849419405\n",
      "Stochastic Gradient Descent(422/499): loss=0.020723847789686275\n",
      "Stochastic Gradient Descent(423/499): loss=2.7355247926024733\n",
      "Stochastic Gradient Descent(424/499): loss=8.84398915347925\n",
      "Stochastic Gradient Descent(425/499): loss=0.013633732558102508\n",
      "Stochastic Gradient Descent(426/499): loss=2.2417253231289247\n",
      "Stochastic Gradient Descent(427/499): loss=6.987527676745495\n",
      "Stochastic Gradient Descent(428/499): loss=11.107116186689543\n",
      "Stochastic Gradient Descent(429/499): loss=2.3238605232204406\n",
      "Stochastic Gradient Descent(430/499): loss=2.4455526901466142\n",
      "Stochastic Gradient Descent(431/499): loss=18.395282308208227\n",
      "Stochastic Gradient Descent(432/499): loss=0.1471341949355188\n",
      "Stochastic Gradient Descent(433/499): loss=0.25377347237461084\n",
      "Stochastic Gradient Descent(434/499): loss=4.12505164055773\n",
      "Stochastic Gradient Descent(435/499): loss=1.7411481590387632\n",
      "Stochastic Gradient Descent(436/499): loss=0.32459515273354683\n",
      "Stochastic Gradient Descent(437/499): loss=20.71585736855657\n",
      "Stochastic Gradient Descent(438/499): loss=3.836964004929867\n",
      "Stochastic Gradient Descent(439/499): loss=7.473153193427016\n",
      "Stochastic Gradient Descent(440/499): loss=5.418786826812934\n",
      "Stochastic Gradient Descent(441/499): loss=0.24965738695493636\n",
      "Stochastic Gradient Descent(442/499): loss=11.778650358868656\n",
      "Stochastic Gradient Descent(443/499): loss=8.109888572538457\n",
      "Stochastic Gradient Descent(444/499): loss=0.40829888755265925\n",
      "Stochastic Gradient Descent(445/499): loss=1.0818709237196005\n",
      "Stochastic Gradient Descent(446/499): loss=2.218015765466537\n",
      "Stochastic Gradient Descent(447/499): loss=2.348106915865521\n",
      "Stochastic Gradient Descent(448/499): loss=0.1922371889241939\n",
      "Stochastic Gradient Descent(449/499): loss=5.27112533448406\n",
      "Stochastic Gradient Descent(450/499): loss=0.9530507059021839\n",
      "Stochastic Gradient Descent(451/499): loss=2.6510194327152647\n",
      "Stochastic Gradient Descent(452/499): loss=0.9265379655858154\n",
      "Stochastic Gradient Descent(453/499): loss=1.6939819290321474\n",
      "Stochastic Gradient Descent(454/499): loss=0.03462783991158708\n",
      "Stochastic Gradient Descent(455/499): loss=11.54875415985844\n",
      "Stochastic Gradient Descent(456/499): loss=1.4008271464849444\n",
      "Stochastic Gradient Descent(457/499): loss=3.0492962086217625\n",
      "Stochastic Gradient Descent(458/499): loss=2.2702132595957587\n",
      "Stochastic Gradient Descent(459/499): loss=7.51616196441369\n",
      "Stochastic Gradient Descent(460/499): loss=7.278935144895417\n",
      "Stochastic Gradient Descent(461/499): loss=0.17195165144875377\n",
      "Stochastic Gradient Descent(462/499): loss=0.019471358665236766\n",
      "Stochastic Gradient Descent(463/499): loss=2.1462834688486425\n",
      "Stochastic Gradient Descent(464/499): loss=1.1604740867270642\n",
      "Stochastic Gradient Descent(465/499): loss=0.13110726399687997\n",
      "Stochastic Gradient Descent(466/499): loss=2.4221375874774327\n",
      "Stochastic Gradient Descent(467/499): loss=0.08727379678041523\n",
      "Stochastic Gradient Descent(468/499): loss=0.051669968388706154\n",
      "Stochastic Gradient Descent(469/499): loss=2.609397699503342\n",
      "Stochastic Gradient Descent(470/499): loss=0.13806158291531445\n",
      "Stochastic Gradient Descent(471/499): loss=1.941928719067122\n",
      "Stochastic Gradient Descent(472/499): loss=3.990049784785838\n",
      "Stochastic Gradient Descent(473/499): loss=3.574661042616742\n",
      "Stochastic Gradient Descent(474/499): loss=3.3495711001704476\n",
      "Stochastic Gradient Descent(475/499): loss=0.032036650196026355\n",
      "Stochastic Gradient Descent(476/499): loss=0.3909072595986358\n",
      "Stochastic Gradient Descent(477/499): loss=0.023755001666738162\n",
      "Stochastic Gradient Descent(478/499): loss=0.5530722479904312\n",
      "Stochastic Gradient Descent(479/499): loss=4.353025181925211\n",
      "Stochastic Gradient Descent(480/499): loss=1.5392422512396073\n",
      "Stochastic Gradient Descent(481/499): loss=3.4132398291031962\n",
      "Stochastic Gradient Descent(482/499): loss=0.7458891761178102\n",
      "Stochastic Gradient Descent(483/499): loss=1.0880188532991468\n",
      "Stochastic Gradient Descent(484/499): loss=0.5185748323025162\n",
      "Stochastic Gradient Descent(485/499): loss=3.0081601149893975e-05\n",
      "Stochastic Gradient Descent(486/499): loss=0.865329467715929\n",
      "Stochastic Gradient Descent(487/499): loss=3.129421735077187\n",
      "Stochastic Gradient Descent(488/499): loss=1.7204330311205265\n",
      "Stochastic Gradient Descent(489/499): loss=0.04051404470795623\n",
      "Stochastic Gradient Descent(490/499): loss=0.03169660086621798\n",
      "Stochastic Gradient Descent(491/499): loss=1.093967040989444\n",
      "Stochastic Gradient Descent(492/499): loss=1.3429375192942672\n",
      "Stochastic Gradient Descent(493/499): loss=2.109735075630276\n",
      "Stochastic Gradient Descent(494/499): loss=0.7685930097107694\n",
      "Stochastic Gradient Descent(495/499): loss=0.06101898751203411\n",
      "Stochastic Gradient Descent(496/499): loss=0.9660624130643475\n",
      "Stochastic Gradient Descent(497/499): loss=0.4978739489243411\n",
      "Stochastic Gradient Descent(498/499): loss=0.6414057717160103\n",
      "Stochastic Gradient Descent(499/499): loss=0.6719495582026253\n",
      "0.5958711111111111\n"
     ]
    }
   ],
   "source": [
    "'''SGD'''\n",
    "(w2,loss2) = least_squares_SGD(y, tX, init_w, max_iter, alpha)\n",
    "sgd_tr_pred = predict_labels(w2, tX_valid)\n",
    "print((sgd_tr_pred == y_valid).mean())\n",
    "sgd_pred = predict_labels(w2, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "(w3,loss3) = least_squares(y, tX)\n",
    "ls_tr_pred = predict_labels(w3, tX_valid)\n",
    "print((ls_tr_pred == y_valid).mean())\n",
    "ls_pred = predict_labels(w3, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "lambda_ = np.logspace(-1, -6, 30)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        min_loss = loss4\n",
    "        ind = i\n",
    "(w4,loss4) = ridge_regression(y, tX, lambda_[ind])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())\n",
    "rd_pred = predict_labels(w4, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOGISTIC REGRESSION'''\n",
    "(w5,loss5) = logistic_regression(y, tX, init_w, max_iter, alpha)\n",
    "log_tr_pred = predict_labels(w5, tX_valid)\n",
    "print((log_tr_pred == y_valid).mean())\n",
    "log_pred = predict_labels(w5, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/results_least_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w1, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
