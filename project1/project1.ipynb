{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '/Users/wifinaynay/downloads/data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "'''DATASET INTRINSICS AND SHAPE (TARGETS AND IDS INCLUDED)'''\n",
    "def DataSetInfo(y, tX_old, ids):\n",
    "    print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "    print(\"Targets: \", y)\n",
    "    print(\"Ids: \",ids)\n",
    "    print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)\n",
    "DataSetInfo(y, tX_old, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIALIZE WEIGHTS'''\n",
    "def InitWeights(feat):\n",
    "    ww = np.random.rand(feat)\n",
    "    init_w = np.array(ww, dtype=np.float64)\n",
    "    return init_w\n",
    "\n",
    "'''HYPER PARAMETERS'''\n",
    "def HyperParameters():\n",
    "    max_iter = 800\n",
    "    epochs = 10\n",
    "    gamma = 1e-1\n",
    "    lambda_ = 1e-2\n",
    "    return max_iter, epochs, gamma, lambda_\n",
    "\n",
    "'''TAKE LOG TRANSFORMATION OF FEATURES'''\n",
    "def LogTransformData(tX, features):  \n",
    "    data = tX[:, features]\n",
    "    indices = np.where(data > -999)\n",
    "    data[indices] = np.log(1 + data[indices])\n",
    "    tX = ManipulateFeatures(tX, data, features)    \n",
    "    return tX\n",
    "\n",
    "'''DELETE GIVEN FEATURE VECTOR FROM FEATURE AND CONCETENATE WITH NEW DATA '''\n",
    "def ManipulateFeatures(tX, data,features):\n",
    "    tX = np.delete(tX, features, 1)\n",
    "    return np.hstack((tX, data))\n",
    "\n",
    "'''IMPUTE DATA WITH MEANS'''\n",
    "def ImputeData(tX):\n",
    "    tX = np.where(tX == -999, np.nan, tX)\n",
    "    #Remove all columns with NAN\n",
    "    tX = tX[:, ~np.all(np.isnan(tX), axis=0)]\n",
    "    \n",
    "    #Remove highly correlated features\n",
    "    tX = tX[:, ~np.all(tX[1:] == tX[:-1], axis=0)]\n",
    "\n",
    "    #Find Mean excluding NAN values\n",
    "    tX_mean = np.nanmean(tX, axis=0)\n",
    "\n",
    "    # NAN = MEAN\n",
    "    tX[np.where(np.isnan(tX))] = np.take(tX_mean, np.where(np.isnan(tX))[1])\n",
    "    print(tX.shape)\n",
    "    return tX\n",
    "\n",
    "'''STANDARDIZE'''\n",
    "def Standardize(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "'''PREPROCESS'''\n",
    "def PreProcess(tX):\n",
    "    print(tX.shape)\n",
    "    '''FEATURES PICKED BY HAND'''\n",
    "    log_feature_vec = np.array(([0, 2, 5, 9, 13, 16, 19, 21, 23, 26, 29]))\n",
    "    '''LUCKY FEATURE OF THE WEEK: 30 :)'''\n",
    "    lucky_feature = np.array(([30]))\n",
    "    tX = LogTransformData(tX, log_feature_vec)\n",
    "    tX = ImputeData(tX)\n",
    "    tX = Standardize(tX)[0]\n",
    "    return tX\n",
    "\n",
    "'''DATASET SEPERATED IN TERMS OF CATEGORIES IN COLUMN 22'''\n",
    "def Categorize_Train(y, tX, ids):\n",
    "    '''CATEGORIES '''\n",
    "    ind = [[] for j in range(3)]\n",
    "    xx = [[] for j in range(3)]\n",
    "    yy = [[] for j in range(3)]\n",
    "    iids = [[] for j in range(3)]\n",
    "    \n",
    "    for i in range(3): \n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        xx[i] = tX[ind[i]]\n",
    "        yy[i] = y[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return np.array((yy)), np.array((xx)), np.array((iids)), np.array((ind))\n",
    "\n",
    "def Categorize_Test(tX, ids):\n",
    "    '''CATEGORIES '''\n",
    "    ind = [[] for j in range(3)]\n",
    "    xx = [[] for j in range(3)]\n",
    "    iids = [[] for j in range(3)]\n",
    "    \n",
    "    for i in range(3): \n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        xx[i] = tX[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return xx, iids, ind\n",
    "\n",
    "'''PREDICTIONS INTO COMPARABLE FORM'''\n",
    "def Decategorize(y_cat, ind):\n",
    "    size = y_cat[0].shape[0] + y_cat[1].shape[0] + y_cat[2].shape[0]\n",
    "    y = np.zeros((size,), dtype=np.float)\n",
    "    for i in range(len(y_cat)):\n",
    "        y[ind[i]] = y_cat[i]\n",
    "    return y\n",
    "\n",
    "'''CHECK VALIDATION SCORE'''\n",
    "def WeightedAverage(pred, target):\n",
    "    total_count = pred[0].shape[0] + pred[1].shape[0] + pred[2].shape[0]\n",
    "    true_count = 0\n",
    "    for i in range(3):\n",
    "        true_count +=  np.sum(pred[i] == target[i])\n",
    "    acc = true_count / total_count\n",
    "    return acc\n",
    "'''FEATURE CORRELATION MAP: ONLY FOR VISUALIZATION'''\n",
    "'''CORRELATED FEATURES: CORR > THRESHOLD : USE FOR SYNTHESIS'''\n",
    "def CorrMap(tX):\n",
    "    df = pd.DataFrame(tX)\n",
    "    corr = df.corr()\n",
    "    return corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "'''FEATURE ENGINEERING'''\n",
    "def FeatureSynthesis(tX_pp, tX_old):\n",
    "    '''CORRELATED FEATURES WILL BE USED FOR NEW FEATURE ADDITION'''\n",
    "    '''MIN PART'''\n",
    "    #tX = np.hstack((tX_pp, np.minimum((tX_old[:,15:16] - tX_old[:,18:19]),np.minimum((tX_old[:,15:16] - tX_old[:,20:21]),(tX_old[:,18:19] - tX_old[:,20:21])))))                                           \n",
    "    tX = np.hstack((tX_pp, np.minimum((tX_old[:,15:16] - tX_old[:,20:21]),(tX_old[:,18:19] - tX_old[:,20:21]))))\n",
    "    tX = np.hstack((tX, np.minimum((tX_old[:,15:16] - tX_old[:,18:19]),(tX_old[:,15:16] - tX_old[:,20:21]))))\n",
    "    tX = np.hstack((tX, (tX_old[:,18:19] - tX_old[:,20:21])))\n",
    "    '''LN PART'''\n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,13:14]*tX_old[:,14:15])))+(tX_old[:,13:14]*tX_old[:,14:15]))))                                                                                         \n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,16:17]*tX_old[:,17:18])))+(tX_old[:,16:17]*tX_old[:,16:17]))))                                                                                         \n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,23:24]*tX_old[:,24:25])))+(tX_old[:,23:24]*tX_old[:,24:25]))))                                                                                         \n",
    "    return tX\n",
    "\n",
    "'''RANDOM DATA SPLIT'''\n",
    "def RandomizedDataSplit(tX, y, ids, inds, split_size = 0.1, my_seed=1):\n",
    "    '''SET SEED FOR REMOVING RANDOMNESS'''\n",
    "    #np.random.seed(my_seed)\n",
    "    '''RANDOM INDEXES'''\n",
    "    size = y.shape[0]\n",
    "    ind = np.random.permutation(size)\n",
    "    split = int(np.floor(split_size * size))\n",
    "    \n",
    "    ind_train = ind[split:]\n",
    "    ind_valid = ind[:split]\n",
    "    \n",
    "    \n",
    "    '''SPLIT DATA ACCORDING TO RANDOM INDICES'''\n",
    "    tX_train = tX[ind_train]\n",
    "    tX_valid = tX[ind_valid]\n",
    "    y_train = y[ind_train]\n",
    "    y_valid = y[ind_valid]\n",
    "    ids_train = ids[ind_train]\n",
    "    ids_valid = ids[ind_valid]\n",
    "    inds_train = inds[ind_train]\n",
    "    inds_valid = inds[ind_valid]\n",
    "    \n",
    "    print(\"Shapes of tX, y, Ids & Indices for Training: \", tX_train.shape, y_train.shape, ids_train.shape, inds_train.shape)\n",
    "    print(\"Shapes of tX, y, Ids & Indices for Validation: \", tX_valid.shape, y_valid.shape, ids_valid.shape, inds_valid.shape)\n",
    "    return (tX_train, y_train, ids_train, inds_train),(tX_valid, y_valid, ids_valid, inds_valid)\n",
    "\n",
    "'''BACKWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def BackwardSelection(y, tX, tX_valid, y_valid, model = \"RR\"):\n",
    "    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0\n",
    "    improved = True     \n",
    "    while improved:\n",
    "        improved = False\n",
    "        worst_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features:\n",
    "                \n",
    "                diff = set(list(range(tX.shape[1]))) - set(selected_features + [i])            \n",
    "                #calculate accuracy\n",
    "                #print(tX[:,list(diff)].shape,y.shape)\n",
    "                \n",
    "                cur_acc = CrossValidation(y, tX[:,list(diff)],10)\n",
    "                #print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    #print(\"best so far: \",cur_best_acc)\n",
    "                    improved = True\n",
    "                    cur_best_acc = cur_acc\n",
    "                    worst_ft = i                    \n",
    "        if improved:\n",
    "            selected_features.append(worst_ft)  \n",
    "        #print(\"burada\",improved)\n",
    "            \n",
    "    return list(set(list(range(tX.shape[1]))) - set(selected_features )), cur_best_acc\n",
    "\n",
    "'''FORWARD OF BACKWARD SELECTION'''\n",
    "def Selection(y, tX, tX_valid, y_valid, typ = \"BS\", model = \"RR\"):\n",
    "    if typ == \"BS\":\n",
    "        selected_features, cur_best_acc = BackwardSelection(y, tX, tX_valid, y_valid,model)\n",
    "        return selected_features, cur_best_acc\n",
    "    elif typ == \"FS\":\n",
    "        selected_features, cur_best_acc = ForwardSelection(y, tX, tX_valid, y_valid,model) \n",
    "        return selected_features, cur_best_acc\n",
    "    \n",
    "'''FORWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def ForwardSelection(y, tX, tX_valid, y_valid, model = \"RR\"):    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0  \n",
    "    improved = True\n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        best_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features: \n",
    "                \n",
    "                #calculate accuracy\n",
    "                \n",
    "                cur_acc = CrossValidation(y, tX[:,selected_features+[i]],5)\n",
    "                #print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    improved = True                   \n",
    "                    cur_best_acc = cur_acc\n",
    "                    best_ft = i                 \n",
    "                    \n",
    "        if improved:\n",
    "            selected_features.append(best_ft)\n",
    "            #print(selected_features)\n",
    "         \n",
    "    return selected_features, cur_best_acc\n",
    "\n",
    "'''CROSS VALIDATION HELPER FUNCTION'''\n",
    "def SelectIndices(y, k_fold, seed):\n",
    "    row_count = y.shape[0]\n",
    "    window_size = int((row_count / k_fold))\n",
    "    remainder = row_count % k_fold\n",
    "    np.random.seed(seed)\n",
    "    rand_indices = np.random.permutation(row_count)\n",
    "    indices = [[] for i in range(k_fold)]\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        \n",
    "            indices[k] = [rand_indices[k*window_size:(k+1)*window_size]]\n",
    "            \n",
    "    return np.array(indices)\n",
    "\n",
    "'''ADD NEW FUTURES'''\n",
    "def AddFeatures(tX):\n",
    "    prime_numbers = [2,3]\n",
    "    #ADD COS / SIN , SQRT \n",
    "    #CHECK FEATURE SYNTHESIS\n",
    "    pm = 0\n",
    "    loop_count = tX.shape[1]\n",
    "    for pm in range(len(prime_numbers)):\n",
    "        for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.power(tX[:,i], prime_numbers[pm]).reshape(-1,1)))\n",
    "    for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.cos(tX[:,i]).reshape(-1,1)))\n",
    "    for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.sin(tX[:,i]).reshape(-1,1)))\n",
    "    \n",
    "    return tX\n",
    "\n",
    "'''REMOVE USELESS FEATURES'''\n",
    "def RemoveFeatures(tX, ratio = 0.5):\n",
    "    \n",
    "    tX = np.hstack((tX[:,22].reshape(-1,1),np.delete(tX,22,1)))\n",
    "    selected_indices = []\n",
    "    \n",
    "    for i in range(tX.shape[1]):\n",
    "        res = np.count_nonzero(tX[:,i] == -999)\n",
    "        \n",
    "        if (res / tX.shape[0]) > ratio:\n",
    "            print(i)\n",
    "            selected_indices.append(i)\n",
    "    diff = list(set(list(range(tX.shape[1])))- set(selected_indices))\n",
    "    return tX[:,diff]\n",
    "\n",
    "'''CROSS VALIDATION'''\n",
    "def CrossValidation(y, tX, k):\n",
    "    seed = np.random.randint(10)\n",
    "    indices = SelectIndices(y,k,seed)\n",
    "    average_acc = 0\n",
    "    for i in range (k):        \n",
    "\n",
    "        xk_train = tX[~indices[i]]\n",
    "        xk_valid = tX[indices[i]]\n",
    "        yk_train = y[~indices[i]]\n",
    "        yk_valid = y[indices[i]]\n",
    "        #print(yk_train.shape,xk_train.shape)\n",
    "        #print(xk_valid.shape)\n",
    "        max_iter, epochs, gamma, lambda_ = HyperParameters()\n",
    "        init_w_gd = np.array((InitWeights(xk_train[0].shape[1])))\n",
    "        \n",
    "        (w3,loss3) = ridge_regression(yk_train[0], xk_train[0], 0.05)\n",
    "        ls_tr_pred = predict_labels(w3, xk_valid[0])\n",
    "        #print(\"heree\",(ls_tr_pred == yk_valid[0]).mean())\n",
    "        average_acc += (ls_tr_pred == yk_valid[0]).mean()/k\n",
    "\n",
    "    return average_acc\n",
    "    \n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Train(y, tX_old, ids):\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize_Train(y, tX_old, ids)\n",
    "    \n",
    "    for cat_ in range(tX_cat.shape[0]):\n",
    "        \n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_])\n",
    "        #tX_cat[cat_] = RemoveFeatures(tX_cat[cat_])\n",
    "        tX_cat[cat_] = AddFeatures(tX_cat[cat_])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    '''TRAIN SET'''\n",
    "    tX_tr_cat = [[] for j in range(3)]\n",
    "    y_tr_cat = [[] for j in range(3)]\n",
    "    id_tr_cat = [[] for j in range(3)]\n",
    "    ind_tr_cat = [[] for j in range(3)]\n",
    "\n",
    "    '''VALID SET'''\n",
    "    tX_val_cat = [[] for j in range(3)]\n",
    "    y_val_cat = [[] for j in range(3)]\n",
    "    id_val_cat = [[] for j in range(3)]\n",
    "    ind_val_cat = [[] for j in range(3)]\n",
    "\n",
    "    for i in range(len(tX_cat)):\n",
    "        (tX_tr_cat[i], y_tr_cat[i],id_tr_cat[i],ind_tr_cat[i]), (tX_val_cat[i], y_val_cat[i],id_val_cat[i], ind_val_cat[i]) = RandomizedDataSplit(tX_cat[i], y_cat[i], id_cat[i], ind_cat[i])\n",
    "\n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_tr_cat = np.array((tX_tr_cat))\n",
    "    y_tr_cat = np.array((y_tr_cat))\n",
    "    id_tr_cat = np.array((id_tr_cat))\n",
    "    ind_tr_cat = np.array((ind_tr_cat))\n",
    "\n",
    "    tX_val_cat = np.array((tX_val_cat))\n",
    "    y_val_cat = np.array((y_val_cat))\n",
    "    id_val_cat = np.array((id_val_cat))\n",
    "    ind_val_cat = np.array((ind_val_cat))\n",
    "\n",
    "    return (y_tr_cat, tX_tr_cat, id_tr_cat, ind_tr_cat), (y_val_cat, tX_val_cat, id_val_cat, ind_val_cat)\n",
    "   \n",
    "\n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Test(tX_old, ids):\n",
    "    tX_cat, id_cat, ind_cat = Categorize_Test(tX_old, ids)\n",
    "    \n",
    "    for i in range(len(tX_cat)):\n",
    "        tX_cat[i] = PreProcess(tX_cat[i])\n",
    "       \n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_cat = np.array((tX_cat))\n",
    "    id_cat = np.array((id_cat))\n",
    "    ind_cat = np.array((ind_cat))\n",
    "    \n",
    "    return tX_cat, id_cat, ind_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(99913, 18)\n",
      "(77544, 30)\n",
      "(77544, 22)\n",
      "(50379, 30)\n",
      "(50379, 29)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (89922, 90) (89922,) (89922,) (89922,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (9991, 90) (9991,) (9991,) (9991,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (69790, 110) (69790,) (69790,) (69790,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (7754, 110) (7754,) (7754,) (7754,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (45342, 145) (45342,) (45342,) (45342,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (5037, 145) (5037,) (5037,) (5037,)\n",
      "Categorized y shape: (3,)\n",
      "Categorized tX shape: (3,)\n",
      "Categorized ids shape: (3,)\n",
      "Categorized ids shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "(y_cat, tX_cat, ids_cat, ind_cat), (y_val_cat, tX_val_cat, ids_val_cat, ind_val_cat) = BuildDataModel_Train(y,tX_old,ids)\n",
    "y_cat = np.array((y_cat))\n",
    "tX_cat = np.array((tX_cat))\n",
    "ids_cat = np.array((ids_cat))\n",
    "ind_cat = np.array((ind_cat))\n",
    "print(\"Categorized y shape:\", y_cat.shape)\n",
    "print(\"Categorized tX shape:\", tX_cat.shape)\n",
    "print(\"Categorized ids shape:\", ids_cat.shape)\n",
    "print(\"Categorized ids shape:\", ind_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tx@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(w.T@w))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    z = 1 / (1 + np.exp(-1*(tx@w)))\n",
    "    return z\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm)))\n",
    "    \n",
    "    return loss\n",
    "def compute_log_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION FOR LR'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * (tx.T@(z - y))\n",
    "    return grad\n",
    "\n",
    "def compute_reg_log_loss(y, tx, w, lambda_):\n",
    "    '''LOGISTIC LOSS WITH REGULARIZATION'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm))) + ((lambda_/2)*np.sum(w.T@w)))\n",
    "    \n",
    "    return loss\n",
    "def compute_reg_log_gradient(y, tx, w, lambda_):\n",
    "    '''GRADIENT COMPUTATION FOR LR WITH REGULARIZATION'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * ((tx.T@(z - y)) + (lambda_*w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    a = tx.shape[0]\n",
    "    m = (tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1])\n",
    "    i = np.eye(m.shape[0],m.shape[0])\n",
    "    w_ridge = np.linalg.lstsq(m,i)[0]@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_log_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_log_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad = compute_log_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT WITH REGULARIZATION'''\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_reg_log_loss(y, tx, w, lambda_)\n",
    "        grad = compute_reg_log_gradient(y, tx, w, lambda_)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_reg_log_loss(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            grad = compute__reg_log_gradient(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/wifinaynay/downloads/data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_cat, tX_cat, ids_cat, ind_cat), (y_val_cat, tX_val_cat, ids_val_cat, ind_val_cat) = BuildDataModel_Train(y,tX_old,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wifinaynay/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n",
      "[-1. -1. -1. ...  1. -1.  1.]\n",
      "[ 1. -1.  1. ...  1. -1.  1.]\n",
      "Accuracy of Model: 0.823588798173997\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "init_w_gd = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[1].shape[1]),InitWeights(tX_cat[2].shape[1])))\n",
    "gd_tr_pred = np.copy((y_val_cat))\n",
    "w_gd = np.copy((init_w_gd))\n",
    "max_iter, epochs, gamma, lambda_ = HyperParameters()\n",
    "cat_lst = [0, 1, 2]\n",
    "for cat in cat_lst:\n",
    "    #res = ForwardSelection(y_cat[cat], tX_cat[cat], tX_val_cat[cat], y_val_cat[cat], \"RR\")\n",
    "    #print(\"after forward:\",res[0], res[1])\n",
    "    #print(\"debugg\", init_w_gd[cat][res[0]].shape)\n",
    "    (w_gd[cat],loss1) = ridge_regression(y_cat[cat], tX_cat[cat],lambda_)\n",
    "    #print(ww2.shape, tX_val_cat[cat][:,res[0]].shape )\n",
    "    \n",
    "    gd_tr_pred[cat] = predict_labels(w_gd[cat], tX_val_cat[cat])\n",
    "    print(gd_tr_pred[cat])\n",
    "#pred = Decategorize(gd_tr_pred, ind_val_cat)\n",
    "acc = WeightedAverage(gd_tr_pred, y_val_cat)\n",
    "print(\"Accuracy of Model:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-3e6106e6c186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/wifinaynay/downloads/data_project1/lr.csv'\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_grad' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '/Users/wifinaynay/downloads/data_project1/lr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(best_grad[0], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
