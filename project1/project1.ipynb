{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "            \n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "print(\"Targets: \", y)\n",
    "print(\"Ids: \",ids)\n",
    "print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE'''\n",
    "#Feature extraction\n",
    "tX = np.hstack((tX_old[:,1:4], tX_old[:,7:12]))\n",
    "tX = np.hstack((tX, tX_old[:,13:23]))\n",
    "#Standardize\n",
    "tX, tX_mean, tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size:  25000\n",
      "Shapes of tX, y & Ids for Training:  (25000, 18) (25000,) (25000,)\n",
      "Shapes of tX, y & Ids for Validation:  (225000, 18) (225000,) (225000,)\n"
     ]
    }
   ],
   "source": [
    "'''SPLIT INTO TRAIN AND VALIDATION'''\n",
    "\n",
    "train_valid_split = int(tX.shape[0] / 10)\n",
    "print(\"Validation data size: \", train_valid_split)\n",
    "tX_valid = tX[train_valid_split:,:]\n",
    "y_valid = y[train_valid_split:]\n",
    "id_valid = ids[train_valid_split:]\n",
    "\n",
    "tX = tX[:train_valid_split]\n",
    "y = y[:train_valid_split]\n",
    "ids = ids[:train_valid_split]\n",
    "\n",
    "print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, id_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''typ = <LOSS_TYPE>(CAPITAL LETTERS)'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(np.square(w)))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    return 1 / (1 + np.exp(-1*(tx@w)))\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(np.log(sigm).T * -y - (np.log(1 - sigm).T * (1-y))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_loss(y, tx, w_star, \"MSE\")\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.solve((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]), tx.T@y)\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of tX & Ids for Testing:  (568238, 18) (25000,)\n"
     ]
    }
   ],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE FOR TEST'''\n",
    "#Feature extraction\n",
    "tX_test = np.hstack((tX_test_old[:,1:4], tX_test_old[:,7:12]))\n",
    "tX_test = np.hstack((tX_test, tX_test_old[:,13:23]))\n",
    "#Standardize\n",
    "tX_test, tX_test_mean, tX_test_std = standardize(tX_test)\n",
    "\n",
    "print(\"Shapes of tX & Ids for Testing: \", tX_test.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "[0.90203684 0.34983954 0.76132583 0.86388524 0.45509147 0.20819977\n",
      " 0.74543495 0.19886324 0.30431185 0.3341806  0.18792616 0.18441746\n",
      " 0.38060497 0.97849309 0.36505012 0.27333579 0.59694204 0.3378332 ]\n"
     ]
    }
   ],
   "source": [
    "ww = np.random.rand(tX.shape[1])\n",
    "init_w = np.array(ww, dtype=np.float64)\n",
    "#init_w = np.zeros(tX.shape[1])\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 400\n",
    "alpha = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/399): loss=5.105606672513272\n",
      "Gradient Descent(1/399): loss=2.826748829811302\n",
      "Gradient Descent(2/399): loss=1.883358262645287\n",
      "Gradient Descent(3/399): loss=1.4029824044237982\n",
      "Gradient Descent(4/399): loss=1.120321572245087\n",
      "Gradient Descent(5/399): loss=0.939459135995151\n",
      "Gradient Descent(6/399): loss=0.8178877599728769\n",
      "Gradient Descent(7/399): loss=0.7333247434979017\n",
      "Gradient Descent(8/399): loss=0.6728059239857433\n",
      "Gradient Descent(9/399): loss=0.6283494028022666\n",
      "Gradient Descent(10/399): loss=0.5948799089245966\n",
      "Gradient Descent(11/399): loss=0.5690975593084511\n",
      "Gradient Descent(12/399): loss=0.5488151270670463\n",
      "Gradient Descent(13/399): loss=0.5325551418925759\n",
      "Gradient Descent(14/399): loss=0.5192998942540765\n",
      "Gradient Descent(15/399): loss=0.508334131537826\n",
      "Gradient Descent(16/399): loss=0.4991448647626128\n",
      "Gradient Descent(17/399): loss=0.4913567543799342\n",
      "Gradient Descent(18/399): loss=0.4846898766212975\n",
      "Gradient Descent(19/399): loss=0.47893170851475303\n",
      "Gradient Descent(20/399): loss=0.4739182453413069\n",
      "Gradient Descent(21/399): loss=0.46952105563225455\n",
      "Gradient Descent(22/399): loss=0.46563824903868106\n",
      "Gradient Descent(23/399): loss=0.4621880611149467\n",
      "Gradient Descent(24/399): loss=0.45910421609102986\n",
      "Gradient Descent(25/399): loss=0.4563325177047919\n",
      "Gradient Descent(26/399): loss=0.45382830262122775\n",
      "Gradient Descent(27/399): loss=0.4515545099474482\n",
      "Gradient Descent(28/399): loss=0.44948019800886463\n",
      "Gradient Descent(29/399): loss=0.4475793908946232\n",
      "Gradient Descent(30/399): loss=0.44583017169887057\n",
      "Gradient Descent(31/399): loss=0.44421396279647\n",
      "Gradient Descent(32/399): loss=0.44271494965797453\n",
      "Gradient Descent(33/399): loss=0.44131961604184666\n",
      "Gradient Descent(34/399): loss=0.4400163664665606\n",
      "Gradient Descent(35/399): loss=0.43879521768764107\n",
      "Gradient Descent(36/399): loss=0.4376475451665286\n",
      "Gradient Descent(37/399): loss=0.43656587367819377\n",
      "Gradient Descent(38/399): loss=0.43554370357562644\n",
      "Gradient Descent(39/399): loss=0.4345753660282142\n",
      "Gradient Descent(40/399): loss=0.4336559019294227\n",
      "Gradient Descent(41/399): loss=0.4327809602350236\n",
      "Gradient Descent(42/399): loss=0.43194671232408005\n",
      "Gradient Descent(43/399): loss=0.43114977962763396\n",
      "Gradient Descent(44/399): loss=0.4303871722862853\n",
      "Gradient Descent(45/399): loss=0.4296562370087217\n",
      "Gradient Descent(46/399): loss=0.4289546126321419\n",
      "Gradient Descent(47/399): loss=0.42828019215018354\n",
      "Gradient Descent(48/399): loss=0.4276310901880112\n",
      "Gradient Descent(49/399): loss=0.4270056150781164\n",
      "Gradient Descent(50/399): loss=0.42640224483227274\n",
      "Gradient Descent(51/399): loss=0.4258196064213371\n",
      "Gradient Descent(52/399): loss=0.4252564578701745\n",
      "Gradient Descent(53/399): loss=0.4247116727538819\n",
      "Gradient Descent(54/399): loss=0.42418422674680606\n",
      "Gradient Descent(55/399): loss=0.4236731859301263\n",
      "Gradient Descent(56/399): loss=0.4231776966089813\n",
      "Gradient Descent(57/399): loss=0.4226969764279067\n",
      "Gradient Descent(58/399): loss=0.42223030660500244\n",
      "Gradient Descent(59/399): loss=0.4217770251318406\n",
      "Gradient Descent(60/399): loss=0.4213365208085165\n",
      "Gradient Descent(61/399): loss=0.420908228002142\n",
      "Gradient Descent(62/399): loss=0.42049162203306917\n",
      "Gradient Descent(63/399): loss=0.4200862151066805\n",
      "Gradient Descent(64/399): loss=0.4196915527200963\n",
      "Gradient Descent(65/399): loss=0.41930721048294295\n",
      "Gradient Descent(66/399): loss=0.4189327912996821\n",
      "Gradient Descent(67/399): loss=0.4185679228681322\n",
      "Gradient Descent(68/399): loss=0.4182122554549231\n",
      "Gradient Descent(69/399): loss=0.4178654599138543\n",
      "Gradient Descent(70/399): loss=0.41752722591762115\n",
      "Gradient Descent(71/399): loss=0.4171972603772359\n",
      "Gradient Descent(72/399): loss=0.41687528602679547\n",
      "Gradient Descent(73/399): loss=0.4165610401541192\n",
      "Gradient Descent(74/399): loss=0.41625427346025334\n",
      "Gradient Descent(75/399): loss=0.415954749032985\n",
      "Gradient Descent(76/399): loss=0.41566224142136277\n",
      "Gradient Descent(77/399): loss=0.41537653579982786\n",
      "Gradient Descent(78/399): loss=0.4150974272119635\n",
      "Gradient Descent(79/399): loss=0.4148247198850755\n",
      "Gradient Descent(80/399): loss=0.41455822660788344\n",
      "Gradient Descent(81/399): loss=0.41429776816451913\n",
      "Gradient Descent(82/399): loss=0.41404317281883285\n",
      "Gradient Descent(83/399): loss=0.41379427584371486\n",
      "Gradient Descent(84/399): loss=0.41355091909074776\n",
      "Gradient Descent(85/399): loss=0.41331295059605183\n",
      "Gradient Descent(86/399): loss=0.4130802242186514\n",
      "Gradient Descent(87/399): loss=0.4128525993081057\n",
      "Gradient Descent(88/399): loss=0.4126299403985142\n",
      "Gradient Descent(89/399): loss=0.4124121169263262\n",
      "Gradient Descent(90/399): loss=0.41219900296966255\n",
      "Gradient Descent(91/399): loss=0.41199047700711294\n",
      "Gradient Descent(92/399): loss=0.41178642169418483\n",
      "Gradient Descent(93/399): loss=0.41158672365578\n",
      "Gradient Descent(94/399): loss=0.41139127329324054\n",
      "Gradient Descent(95/399): loss=0.41119996460466557\n",
      "Gradient Descent(96/399): loss=0.4110126950173233\n",
      "Gradient Descent(97/399): loss=0.41082936523111707\n",
      "Gradient Descent(98/399): loss=0.41064987907215467\n",
      "Gradient Descent(99/399): loss=0.41047414335557875\n",
      "Gradient Descent(100/399): loss=0.4103020677568881\n",
      "Gradient Descent(101/399): loss=0.4101335646910641\n",
      "Gradient Descent(102/399): loss=0.4099685491988756\n",
      "Gradient Descent(103/399): loss=0.40980693883980046\n",
      "Gradient Descent(104/399): loss=0.409648653591053\n",
      "Gradient Descent(105/399): loss=0.4094936157522535\n",
      "Gradient Descent(106/399): loss=0.4093417498553195\n",
      "Gradient Descent(107/399): loss=0.40919298257919895\n",
      "Gradient Descent(108/399): loss=0.4090472426690932\n",
      "Gradient Descent(109/399): loss=0.4089044608598573\n",
      "Gradient Descent(110/399): loss=0.40876456980328524\n",
      "Gradient Descent(111/399): loss=0.40862750399901654\n",
      "Gradient Descent(112/399): loss=0.4084931997288232\n",
      "Gradient Descent(113/399): loss=0.40836159499405383\n",
      "Gradient Descent(114/399): loss=0.4082326294560328\n",
      "Gradient Descent(115/399): loss=0.4081062443792266\n",
      "Gradient Descent(116/399): loss=0.40798238257700675\n",
      "Gradient Descent(117/399): loss=0.40786098835984985\n",
      "Gradient Descent(118/399): loss=0.4077420074858293\n",
      "Gradient Descent(119/399): loss=0.4076253871132636\n",
      "Gradient Descent(120/399): loss=0.4075110757553976\n",
      "Gradient Descent(121/399): loss=0.4073990232369993\n",
      "Gradient Descent(122/399): loss=0.4072891806527675\n",
      "Gradient Descent(123/399): loss=0.40718150032744904\n",
      "Gradient Descent(124/399): loss=0.4070759357775747\n",
      "Gradient Descent(125/399): loss=0.4069724416747269\n",
      "Gradient Descent(126/399): loss=0.4068709738102604\n",
      "Gradient Descent(127/399): loss=0.4067714890613989\n",
      "Gradient Descent(128/399): loss=0.4066739453586423\n",
      "Gradient Descent(129/399): loss=0.4065783016544138\n",
      "Gradient Descent(130/399): loss=0.4064845178928901\n",
      "Gradient Descent(131/399): loss=0.40639255498095694\n",
      "Gradient Descent(132/399): loss=0.4063023747602339\n",
      "Gradient Descent(133/399): loss=0.406213939980121\n",
      "Gradient Descent(134/399): loss=0.4061272142718182\n",
      "Gradient Descent(135/399): loss=0.4060421621232738\n",
      "Gradient Descent(136/399): loss=0.4059587488550197\n",
      "Gradient Descent(137/399): loss=0.4058769405968534\n",
      "Gradient Descent(138/399): loss=0.40579670426533115\n",
      "Gradient Descent(139/399): loss=0.4057180075420345\n",
      "Gradient Descent(140/399): loss=0.40564081885257863\n",
      "Gradient Descent(141/399): loss=0.4055651073463321\n",
      "Gradient Descent(142/399): loss=0.40549084287681375\n",
      "Gradient Descent(143/399): loss=0.40541799598274375\n",
      "Gradient Descent(144/399): loss=0.40534653786971886\n",
      "Gradient Descent(145/399): loss=0.4052764403924871\n",
      "Gradient Descent(146/399): loss=0.4052076760377972\n",
      "Gradient Descent(147/399): loss=0.4051402179078022\n",
      "Gradient Descent(148/399): loss=0.40507403970399075\n",
      "Gradient Descent(149/399): loss=0.4050091157116312\n",
      "Gradient Descent(150/399): loss=0.40494542078470297\n",
      "Gradient Descent(151/399): loss=0.404882930331301\n",
      "Gradient Descent(152/399): loss=0.40482162029949204\n",
      "Gradient Descent(153/399): loss=0.4047614671636074\n",
      "Gradient Descent(154/399): loss=0.4047024479109551\n",
      "Gradient Descent(155/399): loss=0.4046445400289366\n",
      "Gradient Descent(156/399): loss=0.4045877214925514\n",
      "Gradient Descent(157/399): loss=0.40453197075227787\n",
      "Gradient Descent(158/399): loss=0.40447726672231443\n",
      "Gradient Descent(159/399): loss=0.4044235887691686\n",
      "Gradient Descent(160/399): loss=0.40437091670058306\n",
      "Gradient Descent(161/399): loss=0.40431923075478376\n",
      "Gradient Descent(162/399): loss=0.4042685115900416\n",
      "Gradient Descent(163/399): loss=0.40421874027453525\n",
      "Gradient Descent(164/399): loss=0.40416989827650435\n",
      "Gradient Descent(165/399): loss=0.40412196745468343\n",
      "Gradient Descent(166/399): loss=0.4040749300490085\n",
      "Gradient Descent(167/399): loss=0.40402876867158366\n",
      "Gradient Descent(168/399): loss=0.4039834662979015\n",
      "Gradient Descent(169/399): loss=0.40393900625830853\n",
      "Gradient Descent(170/399): loss=0.40389537222970595\n",
      "Gradient Descent(171/399): loss=0.40385254822747974\n",
      "Gradient Descent(172/399): loss=0.403810518597651\n",
      "Gradient Descent(173/399): loss=0.4037692680092407\n",
      "Gradient Descent(174/399): loss=0.4037287814468398\n",
      "Gradient Descent(175/399): loss=0.40368904420338164\n",
      "Gradient Descent(176/399): loss=0.40365004187310494\n",
      "Gradient Descent(177/399): loss=0.40361176034470614\n",
      "Gradient Descent(178/399): loss=0.4035741857946733\n",
      "Gradient Descent(179/399): loss=0.4035373046807936\n",
      "Gradient Descent(180/399): loss=0.40350110373583314\n",
      "Gradient Descent(181/399): loss=0.4034655699613801\n",
      "Gradient Descent(182/399): loss=0.4034306906218482\n",
      "Gradient Descent(183/399): loss=0.40339645323863466\n",
      "Gradient Descent(184/399): loss=0.40336284558442764\n",
      "Gradient Descent(185/399): loss=0.40332985567766\n",
      "Gradient Descent(186/399): loss=0.40329747177710285\n",
      "Gradient Descent(187/399): loss=0.403265682376597\n",
      "Gradient Descent(188/399): loss=0.40323447619991576\n",
      "Gradient Descent(189/399): loss=0.4032038421957571\n",
      "Gradient Descent(190/399): loss=0.40317376953286044\n",
      "Gradient Descent(191/399): loss=0.40314424759524403\n",
      "Gradient Descent(192/399): loss=0.403115265977561\n",
      "Gradient Descent(193/399): loss=0.4030868144805677\n",
      "Gradient Descent(194/399): loss=0.4030588831067051\n",
      "Gradient Descent(195/399): loss=0.4030314620557854\n",
      "Gradient Descent(196/399): loss=0.4030045417207848\n",
      "Gradient Descent(197/399): loss=0.40297811268373707\n",
      "Gradient Descent(198/399): loss=0.4029521657117261\n",
      "Gradient Descent(199/399): loss=0.40292669175297297\n",
      "Gradient Descent(200/399): loss=0.4029016819330187\n",
      "Gradient Descent(201/399): loss=0.40287712755099475\n",
      "Gradient Descent(202/399): loss=0.4028530200759831\n",
      "Gradient Descent(203/399): loss=0.4028293511434617\n",
      "Gradient Descent(204/399): loss=0.4028061125518326\n",
      "Gradient Descent(205/399): loss=0.40278329625903136\n",
      "Gradient Descent(206/399): loss=0.4027608943792155\n",
      "Gradient Descent(207/399): loss=0.4027388991795285\n",
      "Gradient Descent(208/399): loss=0.4027173030769396\n",
      "Gradient Descent(209/399): loss=0.40269609863515526\n",
      "Gradient Descent(210/399): loss=0.4026752785616011\n",
      "Gradient Descent(211/399): loss=0.40265483570447347\n",
      "Gradient Descent(212/399): loss=0.4026347630498572\n",
      "Gradient Descent(213/399): loss=0.40261505371890755\n",
      "Gradient Descent(214/399): loss=0.4025957009650986\n",
      "Gradient Descent(215/399): loss=0.4025766981715295\n",
      "Gradient Descent(216/399): loss=0.4025580388482942\n",
      "Gradient Descent(217/399): loss=0.4025397166299077\n",
      "Gradient Descent(218/399): loss=0.4025217252727906\n",
      "Gradient Descent(219/399): loss=0.4025040586528087\n",
      "Gradient Descent(220/399): loss=0.4024867107628672\n",
      "Gradient Descent(221/399): loss=0.4024696757105576\n",
      "Gradient Descent(222/399): loss=0.4024529477158559\n",
      "Gradient Descent(223/399): loss=0.4024365211088719\n",
      "Gradient Descent(224/399): loss=0.40242039032764665\n",
      "Gradient Descent(225/399): loss=0.4024045499159981\n",
      "Gradient Descent(226/399): loss=0.4023889945214129\n",
      "Gradient Descent(227/399): loss=0.40237371889298507\n",
      "Gradient Descent(228/399): loss=0.40235871787939675\n",
      "Gradient Descent(229/399): loss=0.40234398642694436\n",
      "Gradient Descent(230/399): loss=0.40232951957760565\n",
      "Gradient Descent(231/399): loss=0.4023153124671488\n",
      "Gradient Descent(232/399): loss=0.402301360323281\n",
      "Gradient Descent(233/399): loss=0.4022876584638369\n",
      "Gradient Descent(234/399): loss=0.4022742022950048\n",
      "Gradient Descent(235/399): loss=0.40226098730959126\n",
      "Gradient Descent(236/399): loss=0.4022480090853204\n",
      "Gradient Descent(237/399): loss=0.4022352632831714\n",
      "Gradient Descent(238/399): loss=0.4022227456457482\n",
      "Gradient Descent(239/399): loss=0.4022104519956852\n",
      "Gradient Descent(240/399): loss=0.4021983782340855\n",
      "Gradient Descent(241/399): loss=0.4021865203389907\n",
      "Gradient Descent(242/399): loss=0.40217487436388427\n",
      "Gradient Descent(243/399): loss=0.4021634364362244\n",
      "Gradient Descent(244/399): loss=0.4021522027560073\n",
      "Gradient Descent(245/399): loss=0.40214116959436125\n",
      "Gradient Descent(246/399): loss=0.4021303332921677\n",
      "Gradient Descent(247/399): loss=0.4021196902587121\n",
      "Gradient Descent(248/399): loss=0.402109236970362\n",
      "Gradient Descent(249/399): loss=0.4020989699692715\n",
      "Gradient Descent(250/399): loss=0.4020888858621131\n",
      "Gradient Descent(251/399): loss=0.40207898131883446\n",
      "Gradient Descent(252/399): loss=0.4020692530714407\n",
      "Gradient Descent(253/399): loss=0.4020596979128018\n",
      "Gradient Descent(254/399): loss=0.40205031269548336\n",
      "Gradient Descent(255/399): loss=0.4020410943306014\n",
      "Gradient Descent(256/399): loss=0.40203203978670066\n",
      "Gradient Descent(257/399): loss=0.40202314608865397\n",
      "Gradient Descent(258/399): loss=0.4020144103165856\n",
      "Gradient Descent(259/399): loss=0.4020058296048144\n",
      "Gradient Descent(260/399): loss=0.40199740114082017\n",
      "Gradient Descent(261/399): loss=0.4019891221642282\n",
      "Gradient Descent(262/399): loss=0.40198098996581605\n",
      "Gradient Descent(263/399): loss=0.4019730018865392\n",
      "Gradient Descent(264/399): loss=0.4019651553165762\n",
      "Gradient Descent(265/399): loss=0.40195744769439257\n",
      "Gradient Descent(266/399): loss=0.4019498765058241\n",
      "Gradient Descent(267/399): loss=0.4019424392831769\n",
      "Gradient Descent(268/399): loss=0.40193513360434674\n",
      "Gradient Descent(269/399): loss=0.4019279570919541\n",
      "Gradient Descent(270/399): loss=0.4019209074124985\n",
      "Gradient Descent(271/399): loss=0.40191398227552644\n",
      "Gradient Descent(272/399): loss=0.4019071794328191\n",
      "Gradient Descent(273/399): loss=0.4019004966775933\n",
      "Gradient Descent(274/399): loss=0.4018939318437188\n",
      "Gradient Descent(275/399): loss=0.4018874828049521\n",
      "Gradient Descent(276/399): loss=0.401881147474184\n",
      "Gradient Descent(277/399): loss=0.4018749238027017\n",
      "Gradient Descent(278/399): loss=0.40186880977946643\n",
      "Gradient Descent(279/399): loss=0.4018628034304039\n",
      "Gradient Descent(280/399): loss=0.40185690281770975\n",
      "Gradient Descent(281/399): loss=0.40185110603916707\n",
      "Gradient Descent(282/399): loss=0.40184541122747885\n",
      "Gradient Descent(283/399): loss=0.40183981654961176\n",
      "Gradient Descent(284/399): loss=0.40183432020615384\n",
      "Gradient Descent(285/399): loss=0.4018289204306838\n",
      "Gradient Descent(286/399): loss=0.40182361548915335\n",
      "Gradient Descent(287/399): loss=0.4018184036792802\n",
      "Gradient Descent(288/399): loss=0.40181328332995436\n",
      "Gradient Descent(289/399): loss=0.4018082528006546\n",
      "Gradient Descent(290/399): loss=0.40180331048087636\n",
      "Gradient Descent(291/399): loss=0.40179845478957166\n",
      "Gradient Descent(292/399): loss=0.40179368417459804\n",
      "Gradient Descent(293/399): loss=0.4017889971121799\n",
      "Gradient Descent(294/399): loss=0.40178439210637884\n",
      "Gradient Descent(295/399): loss=0.40177986768857454\n",
      "Gradient Descent(296/399): loss=0.40177542241695624\n",
      "Gradient Descent(297/399): loss=0.40177105487602255\n",
      "Gradient Descent(298/399): loss=0.4017667636760925\n",
      "Gradient Descent(299/399): loss=0.4017625474528244\n",
      "Gradient Descent(300/399): loss=0.4017584048667447\n",
      "Gradient Descent(301/399): loss=0.40175433460278615\n",
      "Gradient Descent(302/399): loss=0.4017503353698335\n",
      "Gradient Descent(303/399): loss=0.4017464059002794\n",
      "Gradient Descent(304/399): loss=0.4017425449495875\n",
      "Gradient Descent(305/399): loss=0.4017387512958645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(306/399): loss=0.4017350237394402\n",
      "Gradient Descent(307/399): loss=0.4017313611024557\n",
      "Gradient Descent(308/399): loss=0.40172776222845863\n",
      "Gradient Descent(309/399): loss=0.4017242259820073\n",
      "Gradient Descent(310/399): loss=0.4017207512482812\n",
      "Gradient Descent(311/399): loss=0.4017173369326997\n",
      "Gradient Descent(312/399): loss=0.40171398196054725\n",
      "Gradient Descent(313/399): loss=0.40171068527660664\n",
      "Gradient Descent(314/399): loss=0.4017074458447977\n",
      "Gradient Descent(315/399): loss=0.4017042626478246\n",
      "Gradient Descent(316/399): loss=0.4017011346868284\n",
      "Gradient Descent(317/399): loss=0.4016980609810467\n",
      "Gradient Descent(318/399): loss=0.40169504056748\n",
      "Gradient Descent(319/399): loss=0.4016920725005636\n",
      "Gradient Descent(320/399): loss=0.40168915585184656\n",
      "Gradient Descent(321/399): loss=0.40168628970967546\n",
      "Gradient Descent(322/399): loss=0.4016834731788859\n",
      "Gradient Descent(323/399): loss=0.4016807053804984\n",
      "Gradient Descent(324/399): loss=0.40167798545141964\n",
      "Gradient Descent(325/399): loss=0.4016753125441511\n",
      "Gradient Descent(326/399): loss=0.40167268582650206\n",
      "Gradient Descent(327/399): loss=0.40167010448130724\n",
      "Gradient Descent(328/399): loss=0.40166756770615136\n",
      "Gradient Descent(329/399): loss=0.40166507471309765\n",
      "Gradient Descent(330/399): loss=0.4016626247284222\n",
      "Gradient Descent(331/399): loss=0.40166021699235216\n",
      "Gradient Descent(332/399): loss=0.40165785075881083\n",
      "Gradient Descent(333/399): loss=0.40165552529516524\n",
      "Gradient Descent(334/399): loss=0.40165323988197993\n",
      "Gradient Descent(335/399): loss=0.4016509938127748\n",
      "Gradient Descent(336/399): loss=0.40164878639378787\n",
      "Gradient Descent(337/399): loss=0.401646616943742\n",
      "Gradient Descent(338/399): loss=0.40164448479361564\n",
      "Gradient Descent(339/399): loss=0.40164238928641977\n",
      "Gradient Descent(340/399): loss=0.4016403297769757\n",
      "Gradient Descent(341/399): loss=0.4016383056317006\n",
      "Gradient Descent(342/399): loss=0.4016363162283943\n",
      "Gradient Descent(343/399): loss=0.4016343609560316\n",
      "Gradient Descent(344/399): loss=0.40163243921455777\n",
      "Gradient Descent(345/399): loss=0.40163055041468776\n",
      "Gradient Descent(346/399): loss=0.4016286939777102\n",
      "Gradient Descent(347/399): loss=0.40162686933529307\n",
      "Gradient Descent(348/399): loss=0.4016250759292953\n",
      "Gradient Descent(349/399): loss=0.4016233132115802\n",
      "Gradient Descent(350/399): loss=0.40162158064383274\n",
      "Gradient Descent(351/399): loss=0.4016198776973808\n",
      "Gradient Descent(352/399): loss=0.40161820385301883\n",
      "Gradient Descent(353/399): loss=0.40161655860083567\n",
      "Gradient Descent(354/399): loss=0.40161494144004506\n",
      "Gradient Descent(355/399): loss=0.4016133518788192\n",
      "Gradient Descent(356/399): loss=0.4016117894341256\n",
      "Gradient Descent(357/399): loss=0.4016102536315672\n",
      "Gradient Descent(358/399): loss=0.4016087440052249\n",
      "Gradient Descent(359/399): loss=0.40160726009750314\n",
      "Gradient Descent(360/399): loss=0.40160580145897923\n",
      "Gradient Descent(361/399): loss=0.40160436764825364\n",
      "Gradient Descent(362/399): loss=0.40160295823180503\n",
      "Gradient Descent(363/399): loss=0.40160157278384667\n",
      "Gradient Descent(364/399): loss=0.40160021088618575\n",
      "Gradient Descent(365/399): loss=0.40159887212808587\n",
      "Gradient Descent(366/399): loss=0.40159755610613135\n",
      "Gradient Descent(367/399): loss=0.40159626242409435\n",
      "Gradient Descent(368/399): loss=0.40159499069280435\n",
      "Gradient Descent(369/399): loss=0.40159374053002067\n",
      "Gradient Descent(370/399): loss=0.40159251156030606\n",
      "Gradient Descent(371/399): loss=0.4015913034149036\n",
      "Gradient Descent(372/399): loss=0.4015901157316159\n",
      "Gradient Descent(373/399): loss=0.4015889481546858\n",
      "Gradient Descent(374/399): loss=0.40158780033468006\n",
      "Gradient Descent(375/399): loss=0.4015866719283747\n",
      "Gradient Descent(376/399): loss=0.40158556259864253\n",
      "Gradient Descent(377/399): loss=0.401584472014343\n",
      "Gradient Descent(378/399): loss=0.4015833998502136\n",
      "Gradient Descent(379/399): loss=0.40158234578676366\n",
      "Gradient Descent(380/399): loss=0.4015813095101704\n",
      "Gradient Descent(381/399): loss=0.4015802907121758\n",
      "Gradient Descent(382/399): loss=0.40157928908998625\n",
      "Gradient Descent(383/399): loss=0.4015783043461742\n",
      "Gradient Descent(384/399): loss=0.401577336188581\n",
      "Gradient Descent(385/399): loss=0.4015763843302215\n",
      "Gradient Descent(386/399): loss=0.40157544848919113\n",
      "Gradient Descent(387/399): loss=0.401574528388574\n",
      "Gradient Descent(388/399): loss=0.4015736237563529\n",
      "Gradient Descent(389/399): loss=0.40157273432532103\n",
      "Gradient Descent(390/399): loss=0.40157185983299526\n",
      "Gradient Descent(391/399): loss=0.40157100002153095\n",
      "Gradient Descent(392/399): loss=0.4015701546376387\n",
      "Gradient Descent(393/399): loss=0.40156932343250157\n",
      "Gradient Descent(394/399): loss=0.4015685061616956\n",
      "Gradient Descent(395/399): loss=0.40156770258510954\n",
      "Gradient Descent(396/399): loss=0.4015669124668683\n",
      "Gradient Descent(397/399): loss=0.4015661355752564\n",
      "Gradient Descent(398/399): loss=0.4015653716826426\n",
      "Gradient Descent(399/399): loss=0.40156462056540737\n",
      "0.7063955555555556\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "(w1,loss1) = least_squares_GD(y, tX, init_w, max_iter, alpha)\n",
    "gd_tr_pred = predict_labels(w1, tX_valid)\n",
    "print((gd_tr_pred == y_valid).mean())\n",
    "gd_pred = predict_labels(w1, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/399): loss=11.823464097974677\n",
      "Stochastic Gradient Descent(1/399): loss=0.41518127260574866\n",
      "Stochastic Gradient Descent(2/399): loss=5.890657992126775\n",
      "Stochastic Gradient Descent(3/399): loss=0.05306084813863654\n",
      "Stochastic Gradient Descent(4/399): loss=3.2288429288735983\n",
      "Stochastic Gradient Descent(5/399): loss=0.8852826178573585\n",
      "Stochastic Gradient Descent(6/399): loss=1.637918896735276\n",
      "Stochastic Gradient Descent(7/399): loss=0.4164419610637543\n",
      "Stochastic Gradient Descent(8/399): loss=16.62779421471415\n",
      "Stochastic Gradient Descent(9/399): loss=0.14370828911913866\n",
      "Stochastic Gradient Descent(10/399): loss=4.586253569029194\n",
      "Stochastic Gradient Descent(11/399): loss=0.2926383052955297\n",
      "Stochastic Gradient Descent(12/399): loss=0.6720134072005184\n",
      "Stochastic Gradient Descent(13/399): loss=4.511508033551851\n",
      "Stochastic Gradient Descent(14/399): loss=2.506630237873483\n",
      "Stochastic Gradient Descent(15/399): loss=2.3967125603175115\n",
      "Stochastic Gradient Descent(16/399): loss=1.2939984329438026\n",
      "Stochastic Gradient Descent(17/399): loss=1.3608520201630492\n",
      "Stochastic Gradient Descent(18/399): loss=2.709977376882628\n",
      "Stochastic Gradient Descent(19/399): loss=3.6225122983608817\n",
      "Stochastic Gradient Descent(20/399): loss=2.387903985581818\n",
      "Stochastic Gradient Descent(21/399): loss=0.1645363787165662\n",
      "Stochastic Gradient Descent(22/399): loss=1.937360939940804\n",
      "Stochastic Gradient Descent(23/399): loss=0.10771595015688404\n",
      "Stochastic Gradient Descent(24/399): loss=5.951837563876501\n",
      "Stochastic Gradient Descent(25/399): loss=0.5318315103204417\n",
      "Stochastic Gradient Descent(26/399): loss=6.746734924247721\n",
      "Stochastic Gradient Descent(27/399): loss=3.6596185749032593\n",
      "Stochastic Gradient Descent(28/399): loss=4.177297718293888\n",
      "Stochastic Gradient Descent(29/399): loss=0.36848448121919897\n",
      "Stochastic Gradient Descent(30/399): loss=0.005542866912445305\n",
      "Stochastic Gradient Descent(31/399): loss=0.5891363640599896\n",
      "Stochastic Gradient Descent(32/399): loss=0.3990536867039639\n",
      "Stochastic Gradient Descent(33/399): loss=0.25908014773832116\n",
      "Stochastic Gradient Descent(34/399): loss=0.21434488443788674\n",
      "Stochastic Gradient Descent(35/399): loss=0.7620941465643788\n",
      "Stochastic Gradient Descent(36/399): loss=0.9264761011534798\n",
      "Stochastic Gradient Descent(37/399): loss=1.1263513083289947\n",
      "Stochastic Gradient Descent(38/399): loss=11.220125294663923\n",
      "Stochastic Gradient Descent(39/399): loss=0.3000691335631455\n",
      "Stochastic Gradient Descent(40/399): loss=1.2905290414041262\n",
      "Stochastic Gradient Descent(41/399): loss=0.3725566654761578\n",
      "Stochastic Gradient Descent(42/399): loss=1.4001261832326615\n",
      "Stochastic Gradient Descent(43/399): loss=0.011314924133688618\n",
      "Stochastic Gradient Descent(44/399): loss=0.45316439189868923\n",
      "Stochastic Gradient Descent(45/399): loss=0.00023589539030440248\n",
      "Stochastic Gradient Descent(46/399): loss=1.1692001882696357\n",
      "Stochastic Gradient Descent(47/399): loss=0.059744950190382666\n",
      "Stochastic Gradient Descent(48/399): loss=0.6980576616842203\n",
      "Stochastic Gradient Descent(49/399): loss=4.683633968952818\n",
      "Stochastic Gradient Descent(50/399): loss=0.4456019078320883\n",
      "Stochastic Gradient Descent(51/399): loss=0.01146582598929576\n",
      "Stochastic Gradient Descent(52/399): loss=2.0903498415162307\n",
      "Stochastic Gradient Descent(53/399): loss=3.3435771770453324\n",
      "Stochastic Gradient Descent(54/399): loss=0.15663878293948497\n",
      "Stochastic Gradient Descent(55/399): loss=0.02131929013048208\n",
      "Stochastic Gradient Descent(56/399): loss=11.185381932058812\n",
      "Stochastic Gradient Descent(57/399): loss=2.0933297023363284\n",
      "Stochastic Gradient Descent(58/399): loss=9.511853989325445\n",
      "Stochastic Gradient Descent(59/399): loss=2.100471826911111\n",
      "Stochastic Gradient Descent(60/399): loss=2.1124344481273463\n",
      "Stochastic Gradient Descent(61/399): loss=0.015485374094890482\n",
      "Stochastic Gradient Descent(62/399): loss=0.3561525272543299\n",
      "Stochastic Gradient Descent(63/399): loss=0.406066343342294\n",
      "Stochastic Gradient Descent(64/399): loss=0.039251099462270116\n",
      "Stochastic Gradient Descent(65/399): loss=0.5033081560951231\n",
      "Stochastic Gradient Descent(66/399): loss=0.0412067918415971\n",
      "Stochastic Gradient Descent(67/399): loss=0.7013745557938004\n",
      "Stochastic Gradient Descent(68/399): loss=0.6992793770975964\n",
      "Stochastic Gradient Descent(69/399): loss=9.666938890395311\n",
      "Stochastic Gradient Descent(70/399): loss=0.17001151982677892\n",
      "Stochastic Gradient Descent(71/399): loss=0.11890381585050315\n",
      "Stochastic Gradient Descent(72/399): loss=1.1479820666785772\n",
      "Stochastic Gradient Descent(73/399): loss=0.559356724192749\n",
      "Stochastic Gradient Descent(74/399): loss=0.14398498439173743\n",
      "Stochastic Gradient Descent(75/399): loss=0.01607980286660691\n",
      "Stochastic Gradient Descent(76/399): loss=0.3596704956705822\n",
      "Stochastic Gradient Descent(77/399): loss=0.688266599382268\n",
      "Stochastic Gradient Descent(78/399): loss=0.0009449862579160378\n",
      "Stochastic Gradient Descent(79/399): loss=0.331509812694482\n",
      "Stochastic Gradient Descent(80/399): loss=0.11310266758046796\n",
      "Stochastic Gradient Descent(81/399): loss=0.7003730157872653\n",
      "Stochastic Gradient Descent(82/399): loss=4.692281774549373\n",
      "Stochastic Gradient Descent(83/399): loss=3.9361166629724855\n",
      "Stochastic Gradient Descent(84/399): loss=1.9671621587972665\n",
      "Stochastic Gradient Descent(85/399): loss=0.025957798107673995\n",
      "Stochastic Gradient Descent(86/399): loss=3.19661964913437\n",
      "Stochastic Gradient Descent(87/399): loss=0.11378757300791478\n",
      "Stochastic Gradient Descent(88/399): loss=0.014706689075153708\n",
      "Stochastic Gradient Descent(89/399): loss=1.6405282770815175\n",
      "Stochastic Gradient Descent(90/399): loss=0.5687984306375964\n",
      "Stochastic Gradient Descent(91/399): loss=0.05571244127078735\n",
      "Stochastic Gradient Descent(92/399): loss=0.22395648539783689\n",
      "Stochastic Gradient Descent(93/399): loss=0.016552388883205715\n",
      "Stochastic Gradient Descent(94/399): loss=1.8642814401118437\n",
      "Stochastic Gradient Descent(95/399): loss=12.451992271975714\n",
      "Stochastic Gradient Descent(96/399): loss=0.01659420556371619\n",
      "Stochastic Gradient Descent(97/399): loss=0.08121104880804778\n",
      "Stochastic Gradient Descent(98/399): loss=0.28327844327528917\n",
      "Stochastic Gradient Descent(99/399): loss=0.1342601122800543\n",
      "Stochastic Gradient Descent(100/399): loss=44.944168223220416\n",
      "Stochastic Gradient Descent(101/399): loss=16.731247256188773\n",
      "Stochastic Gradient Descent(102/399): loss=2.482973419384133\n",
      "Stochastic Gradient Descent(103/399): loss=9.742946693687154\n",
      "Stochastic Gradient Descent(104/399): loss=13.037037143794537\n",
      "Stochastic Gradient Descent(105/399): loss=40.52982019189669\n",
      "Stochastic Gradient Descent(106/399): loss=153.20426945144956\n",
      "Stochastic Gradient Descent(107/399): loss=31.26247106847883\n",
      "Stochastic Gradient Descent(108/399): loss=9.56270804464633\n",
      "Stochastic Gradient Descent(109/399): loss=9.437980295620463\n",
      "Stochastic Gradient Descent(110/399): loss=50.942689249879145\n",
      "Stochastic Gradient Descent(111/399): loss=34.018850099341016\n",
      "Stochastic Gradient Descent(112/399): loss=4.304784120822329\n",
      "Stochastic Gradient Descent(113/399): loss=0.45490456280036395\n",
      "Stochastic Gradient Descent(114/399): loss=0.5804544771193675\n",
      "Stochastic Gradient Descent(115/399): loss=78.726824900962\n",
      "Stochastic Gradient Descent(116/399): loss=16.334438247131406\n",
      "Stochastic Gradient Descent(117/399): loss=10.93942142512942\n",
      "Stochastic Gradient Descent(118/399): loss=47.67768011603951\n",
      "Stochastic Gradient Descent(119/399): loss=0.29717819050851335\n",
      "Stochastic Gradient Descent(120/399): loss=8.132140690184361\n",
      "Stochastic Gradient Descent(121/399): loss=12.192747453077786\n",
      "Stochastic Gradient Descent(122/399): loss=26.212294233846205\n",
      "Stochastic Gradient Descent(123/399): loss=31.973832760749087\n",
      "Stochastic Gradient Descent(124/399): loss=13.035246926195699\n",
      "Stochastic Gradient Descent(125/399): loss=4.126287512687246\n",
      "Stochastic Gradient Descent(126/399): loss=31.090844208602114\n",
      "Stochastic Gradient Descent(127/399): loss=0.31393348040810765\n",
      "Stochastic Gradient Descent(128/399): loss=14.992382639432364\n",
      "Stochastic Gradient Descent(129/399): loss=0.018827857420827515\n",
      "Stochastic Gradient Descent(130/399): loss=1.9794750642135246\n",
      "Stochastic Gradient Descent(131/399): loss=3.797748744947766\n",
      "Stochastic Gradient Descent(132/399): loss=6.498154886346384\n",
      "Stochastic Gradient Descent(133/399): loss=12.447667360747507\n",
      "Stochastic Gradient Descent(134/399): loss=12.170550988030831\n",
      "Stochastic Gradient Descent(135/399): loss=6.099853558507168\n",
      "Stochastic Gradient Descent(136/399): loss=2.392302892753545\n",
      "Stochastic Gradient Descent(137/399): loss=12.677659596296037\n",
      "Stochastic Gradient Descent(138/399): loss=0.36157288634603985\n",
      "Stochastic Gradient Descent(139/399): loss=5.1224913580667195\n",
      "Stochastic Gradient Descent(140/399): loss=4.542586758284906\n",
      "Stochastic Gradient Descent(141/399): loss=2.164997732471532\n",
      "Stochastic Gradient Descent(142/399): loss=0.9060183888394366\n",
      "Stochastic Gradient Descent(143/399): loss=0.08214090307622642\n",
      "Stochastic Gradient Descent(144/399): loss=6.998835716496271\n",
      "Stochastic Gradient Descent(145/399): loss=5.564453295695437\n",
      "Stochastic Gradient Descent(146/399): loss=8.452817690571152\n",
      "Stochastic Gradient Descent(147/399): loss=0.0815625206168519\n",
      "Stochastic Gradient Descent(148/399): loss=3.813386736870764\n",
      "Stochastic Gradient Descent(149/399): loss=0.35032545445392493\n",
      "Stochastic Gradient Descent(150/399): loss=0.20798115871260553\n",
      "Stochastic Gradient Descent(151/399): loss=2.6201782603131685\n",
      "Stochastic Gradient Descent(152/399): loss=0.03672331983057192\n",
      "Stochastic Gradient Descent(153/399): loss=15.104336871876678\n",
      "Stochastic Gradient Descent(154/399): loss=0.48723514761293624\n",
      "Stochastic Gradient Descent(155/399): loss=15.034080882075335\n",
      "Stochastic Gradient Descent(156/399): loss=7.448238504074242\n",
      "Stochastic Gradient Descent(157/399): loss=7.391649070362852\n",
      "Stochastic Gradient Descent(158/399): loss=6.731668557204811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(159/399): loss=5.354717739726628\n",
      "Stochastic Gradient Descent(160/399): loss=0.02916200751453136\n",
      "Stochastic Gradient Descent(161/399): loss=1.7847867404632225\n",
      "Stochastic Gradient Descent(162/399): loss=1.6150945814665036\n",
      "Stochastic Gradient Descent(163/399): loss=13.289712452059948\n",
      "Stochastic Gradient Descent(164/399): loss=4.654205950117052\n",
      "Stochastic Gradient Descent(165/399): loss=0.08128325193473297\n",
      "Stochastic Gradient Descent(166/399): loss=0.7789881010101257\n",
      "Stochastic Gradient Descent(167/399): loss=0.136834165847788\n",
      "Stochastic Gradient Descent(168/399): loss=0.08913583656268877\n",
      "Stochastic Gradient Descent(169/399): loss=0.3705361697069894\n",
      "Stochastic Gradient Descent(170/399): loss=1.0743839439193759\n",
      "Stochastic Gradient Descent(171/399): loss=0.02602231859933285\n",
      "Stochastic Gradient Descent(172/399): loss=6.0501017984323\n",
      "Stochastic Gradient Descent(173/399): loss=0.16992034690224994\n",
      "Stochastic Gradient Descent(174/399): loss=3.99808284860992\n",
      "Stochastic Gradient Descent(175/399): loss=0.43832648406617797\n",
      "Stochastic Gradient Descent(176/399): loss=16.143145111195878\n",
      "Stochastic Gradient Descent(177/399): loss=8.799704253910825\n",
      "Stochastic Gradient Descent(178/399): loss=3.313270410776108\n",
      "Stochastic Gradient Descent(179/399): loss=4.663174761044574\n",
      "Stochastic Gradient Descent(180/399): loss=2.214807829517854\n",
      "Stochastic Gradient Descent(181/399): loss=6.67547328818344\n",
      "Stochastic Gradient Descent(182/399): loss=29.723186603585642\n",
      "Stochastic Gradient Descent(183/399): loss=2.049689105600634\n",
      "Stochastic Gradient Descent(184/399): loss=0.28416264461760243\n",
      "Stochastic Gradient Descent(185/399): loss=91.63842401751167\n",
      "Stochastic Gradient Descent(186/399): loss=5.866757502406893\n",
      "Stochastic Gradient Descent(187/399): loss=163.414265258978\n",
      "Stochastic Gradient Descent(188/399): loss=53.53877764038699\n",
      "Stochastic Gradient Descent(189/399): loss=2.5416124928326886\n",
      "Stochastic Gradient Descent(190/399): loss=67.76724087363255\n",
      "Stochastic Gradient Descent(191/399): loss=49.42594316656608\n",
      "Stochastic Gradient Descent(192/399): loss=0.27531484822023106\n",
      "Stochastic Gradient Descent(193/399): loss=21.346064067679162\n",
      "Stochastic Gradient Descent(194/399): loss=1.748596659688675\n",
      "Stochastic Gradient Descent(195/399): loss=23.411083502457522\n",
      "Stochastic Gradient Descent(196/399): loss=3.105300129768265\n",
      "Stochastic Gradient Descent(197/399): loss=4.30651788054572\n",
      "Stochastic Gradient Descent(198/399): loss=22.851349083503237\n",
      "Stochastic Gradient Descent(199/399): loss=12.709165749221789\n",
      "Stochastic Gradient Descent(200/399): loss=5.76202598505296\n",
      "Stochastic Gradient Descent(201/399): loss=1.0357673181984355\n",
      "Stochastic Gradient Descent(202/399): loss=13.295290378819614\n",
      "Stochastic Gradient Descent(203/399): loss=7.3467949136578055\n",
      "Stochastic Gradient Descent(204/399): loss=44.04215067501561\n",
      "Stochastic Gradient Descent(205/399): loss=25.0947528695776\n",
      "Stochastic Gradient Descent(206/399): loss=2.5753339008055773\n",
      "Stochastic Gradient Descent(207/399): loss=13.582408492364255\n",
      "Stochastic Gradient Descent(208/399): loss=207.2395021290026\n",
      "Stochastic Gradient Descent(209/399): loss=247.72582541644604\n",
      "Stochastic Gradient Descent(210/399): loss=2.3821356474391693\n",
      "Stochastic Gradient Descent(211/399): loss=85.70001475209928\n",
      "Stochastic Gradient Descent(212/399): loss=56.142760313226056\n",
      "Stochastic Gradient Descent(213/399): loss=67.10018576911693\n",
      "Stochastic Gradient Descent(214/399): loss=260.24399629076333\n",
      "Stochastic Gradient Descent(215/399): loss=116.80909244961452\n",
      "Stochastic Gradient Descent(216/399): loss=70.00433010620594\n",
      "Stochastic Gradient Descent(217/399): loss=55.611731841197376\n",
      "Stochastic Gradient Descent(218/399): loss=23.491275056200152\n",
      "Stochastic Gradient Descent(219/399): loss=173.82613173033295\n",
      "Stochastic Gradient Descent(220/399): loss=183.30000898314762\n",
      "Stochastic Gradient Descent(221/399): loss=48.842659112012704\n",
      "Stochastic Gradient Descent(222/399): loss=41.54961341099759\n",
      "Stochastic Gradient Descent(223/399): loss=0.42268412390207977\n",
      "Stochastic Gradient Descent(224/399): loss=4.0489824607261236\n",
      "Stochastic Gradient Descent(225/399): loss=0.5462030927069392\n",
      "Stochastic Gradient Descent(226/399): loss=0.2960784809831444\n",
      "Stochastic Gradient Descent(227/399): loss=9.106412804570999\n",
      "Stochastic Gradient Descent(228/399): loss=1.5261994057651282\n",
      "Stochastic Gradient Descent(229/399): loss=1.3008335311781793\n",
      "Stochastic Gradient Descent(230/399): loss=22.14898043271732\n",
      "Stochastic Gradient Descent(231/399): loss=18.821744957985914\n",
      "Stochastic Gradient Descent(232/399): loss=126.88876102262755\n",
      "Stochastic Gradient Descent(233/399): loss=29.036123883562176\n",
      "Stochastic Gradient Descent(234/399): loss=216.20116039283812\n",
      "Stochastic Gradient Descent(235/399): loss=23.94071581022789\n",
      "Stochastic Gradient Descent(236/399): loss=14.016338005096431\n",
      "Stochastic Gradient Descent(237/399): loss=27.205017196564096\n",
      "Stochastic Gradient Descent(238/399): loss=24.4893414815883\n",
      "Stochastic Gradient Descent(239/399): loss=6.517094320181714\n",
      "Stochastic Gradient Descent(240/399): loss=1.7818799457539247\n",
      "Stochastic Gradient Descent(241/399): loss=1.4669362207151415\n",
      "Stochastic Gradient Descent(242/399): loss=5.689558531071518\n",
      "Stochastic Gradient Descent(243/399): loss=1.6898150443787832\n",
      "Stochastic Gradient Descent(244/399): loss=0.3183149321997529\n",
      "Stochastic Gradient Descent(245/399): loss=0.005307874302018112\n",
      "Stochastic Gradient Descent(246/399): loss=1.5673813814889619\n",
      "Stochastic Gradient Descent(247/399): loss=21.543426675182523\n",
      "Stochastic Gradient Descent(248/399): loss=23.00669429519114\n",
      "Stochastic Gradient Descent(249/399): loss=9.369237453627486\n",
      "Stochastic Gradient Descent(250/399): loss=0.5581795986686973\n",
      "Stochastic Gradient Descent(251/399): loss=12.120864488436945\n",
      "Stochastic Gradient Descent(252/399): loss=4.907815641435821e-05\n",
      "Stochastic Gradient Descent(253/399): loss=2.3226547061236937\n",
      "Stochastic Gradient Descent(254/399): loss=5.420764095317308\n",
      "Stochastic Gradient Descent(255/399): loss=14.469682430401347\n",
      "Stochastic Gradient Descent(256/399): loss=15.99580971045725\n",
      "Stochastic Gradient Descent(257/399): loss=38.26496584151717\n",
      "Stochastic Gradient Descent(258/399): loss=4.711229701625956\n",
      "Stochastic Gradient Descent(259/399): loss=0.7797915368197325\n",
      "Stochastic Gradient Descent(260/399): loss=3.981021383896273e-05\n",
      "Stochastic Gradient Descent(261/399): loss=1.7918868182839465\n",
      "Stochastic Gradient Descent(262/399): loss=126.39882506464564\n",
      "Stochastic Gradient Descent(263/399): loss=12.75772132195289\n",
      "Stochastic Gradient Descent(264/399): loss=105.4847243269031\n",
      "Stochastic Gradient Descent(265/399): loss=172.20185077716167\n",
      "Stochastic Gradient Descent(266/399): loss=40.105334920197315\n",
      "Stochastic Gradient Descent(267/399): loss=11.060871816615508\n",
      "Stochastic Gradient Descent(268/399): loss=9.605996552445434\n",
      "Stochastic Gradient Descent(269/399): loss=16.102091504514497\n",
      "Stochastic Gradient Descent(270/399): loss=3.41347429325174\n",
      "Stochastic Gradient Descent(271/399): loss=3.123448359161541e-05\n",
      "Stochastic Gradient Descent(272/399): loss=0.7226933164737941\n",
      "Stochastic Gradient Descent(273/399): loss=18.562077879930445\n",
      "Stochastic Gradient Descent(274/399): loss=4.159854291963175\n",
      "Stochastic Gradient Descent(275/399): loss=0.39597438555689674\n",
      "Stochastic Gradient Descent(276/399): loss=9.452135079024337\n",
      "Stochastic Gradient Descent(277/399): loss=0.3310697940581318\n",
      "Stochastic Gradient Descent(278/399): loss=5.4752761208645016\n",
      "Stochastic Gradient Descent(279/399): loss=4.989907129873096\n",
      "Stochastic Gradient Descent(280/399): loss=0.24420914331192523\n",
      "Stochastic Gradient Descent(281/399): loss=80.04700055018536\n",
      "Stochastic Gradient Descent(282/399): loss=3.227650280202125\n",
      "Stochastic Gradient Descent(283/399): loss=1.071069756069084\n",
      "Stochastic Gradient Descent(284/399): loss=3.160694144768735\n",
      "Stochastic Gradient Descent(285/399): loss=18.329961467994238\n",
      "Stochastic Gradient Descent(286/399): loss=30.01144921372015\n",
      "Stochastic Gradient Descent(287/399): loss=11.496755619304393\n",
      "Stochastic Gradient Descent(288/399): loss=3.2711893093141073\n",
      "Stochastic Gradient Descent(289/399): loss=0.3910063758558872\n",
      "Stochastic Gradient Descent(290/399): loss=4.453687106983485\n",
      "Stochastic Gradient Descent(291/399): loss=2.6272742616609235\n",
      "Stochastic Gradient Descent(292/399): loss=4.9462440987675\n",
      "Stochastic Gradient Descent(293/399): loss=1.7332400725793113\n",
      "Stochastic Gradient Descent(294/399): loss=25.69599595624518\n",
      "Stochastic Gradient Descent(295/399): loss=0.03708558022065051\n",
      "Stochastic Gradient Descent(296/399): loss=148.49040631648396\n",
      "Stochastic Gradient Descent(297/399): loss=106.31246951462138\n",
      "Stochastic Gradient Descent(298/399): loss=274.4822559643023\n",
      "Stochastic Gradient Descent(299/399): loss=19.48217259535503\n",
      "Stochastic Gradient Descent(300/399): loss=2.95819787649949\n",
      "Stochastic Gradient Descent(301/399): loss=95.04333237241663\n",
      "Stochastic Gradient Descent(302/399): loss=7.0997101775062\n",
      "Stochastic Gradient Descent(303/399): loss=321.37156655539326\n",
      "Stochastic Gradient Descent(304/399): loss=174.6042737097429\n",
      "Stochastic Gradient Descent(305/399): loss=73.98277525702116\n",
      "Stochastic Gradient Descent(306/399): loss=4.055247232261321\n",
      "Stochastic Gradient Descent(307/399): loss=0.45946090604275075\n",
      "Stochastic Gradient Descent(308/399): loss=67.42545331572131\n",
      "Stochastic Gradient Descent(309/399): loss=4.276282477281362\n",
      "Stochastic Gradient Descent(310/399): loss=36.51624159645859\n",
      "Stochastic Gradient Descent(311/399): loss=107.75703407609734\n",
      "Stochastic Gradient Descent(312/399): loss=3.659204595763636\n",
      "Stochastic Gradient Descent(313/399): loss=4.978547765392091\n",
      "Stochastic Gradient Descent(314/399): loss=34.53047233629391\n",
      "Stochastic Gradient Descent(315/399): loss=143.60979469831656\n",
      "Stochastic Gradient Descent(316/399): loss=146.44423796430416\n",
      "Stochastic Gradient Descent(317/399): loss=26.23763946674213\n",
      "Stochastic Gradient Descent(318/399): loss=10.955009834061569\n",
      "Stochastic Gradient Descent(319/399): loss=5.854633798400777\n",
      "Stochastic Gradient Descent(320/399): loss=271.46016473934856\n",
      "Stochastic Gradient Descent(321/399): loss=40.462644632373426\n",
      "Stochastic Gradient Descent(322/399): loss=32.612670564141524\n",
      "Stochastic Gradient Descent(323/399): loss=1.318286154971804\n",
      "Stochastic Gradient Descent(324/399): loss=10.695380545259832\n",
      "Stochastic Gradient Descent(325/399): loss=11.382615408976031\n",
      "Stochastic Gradient Descent(326/399): loss=23.908260130311383\n",
      "Stochastic Gradient Descent(327/399): loss=4.602233618962059\n",
      "Stochastic Gradient Descent(328/399): loss=48.0844005004109\n",
      "Stochastic Gradient Descent(329/399): loss=39.42172308077001\n",
      "Stochastic Gradient Descent(330/399): loss=21.560569549442917\n",
      "Stochastic Gradient Descent(331/399): loss=132.24110954657974\n",
      "Stochastic Gradient Descent(332/399): loss=179.8964829827934\n",
      "Stochastic Gradient Descent(333/399): loss=38.390440737013606\n",
      "Stochastic Gradient Descent(334/399): loss=35.54625773673504\n",
      "Stochastic Gradient Descent(335/399): loss=0.02478872730911806\n",
      "Stochastic Gradient Descent(336/399): loss=17.258561322699023\n",
      "Stochastic Gradient Descent(337/399): loss=28.214443037682404\n",
      "Stochastic Gradient Descent(338/399): loss=8.16218654382136\n",
      "Stochastic Gradient Descent(339/399): loss=8.625325844709034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(340/399): loss=10.16389087005669\n",
      "Stochastic Gradient Descent(341/399): loss=46.8962567330982\n",
      "Stochastic Gradient Descent(342/399): loss=56.64604637384322\n",
      "Stochastic Gradient Descent(343/399): loss=80.83169135525243\n",
      "Stochastic Gradient Descent(344/399): loss=118.65366106404123\n",
      "Stochastic Gradient Descent(345/399): loss=0.4189046059209876\n",
      "Stochastic Gradient Descent(346/399): loss=48.757972219241076\n",
      "Stochastic Gradient Descent(347/399): loss=5.304670760497148\n",
      "Stochastic Gradient Descent(348/399): loss=0.813449302890877\n",
      "Stochastic Gradient Descent(349/399): loss=5.319629670176661\n",
      "Stochastic Gradient Descent(350/399): loss=102.59900243891984\n",
      "Stochastic Gradient Descent(351/399): loss=13.07797989364041\n",
      "Stochastic Gradient Descent(352/399): loss=8.077595937350631\n",
      "Stochastic Gradient Descent(353/399): loss=1.2949110397374046\n",
      "Stochastic Gradient Descent(354/399): loss=6.013065975056309\n",
      "Stochastic Gradient Descent(355/399): loss=78.46955203725378\n",
      "Stochastic Gradient Descent(356/399): loss=48.69125473788866\n",
      "Stochastic Gradient Descent(357/399): loss=6.9871405278098235\n",
      "Stochastic Gradient Descent(358/399): loss=6.060481099851593e-05\n",
      "Stochastic Gradient Descent(359/399): loss=0.7655063743346202\n",
      "Stochastic Gradient Descent(360/399): loss=20.375209379552444\n",
      "Stochastic Gradient Descent(361/399): loss=5.9153658190479135\n",
      "Stochastic Gradient Descent(362/399): loss=5.235994190047245\n",
      "Stochastic Gradient Descent(363/399): loss=0.2056035727436057\n",
      "Stochastic Gradient Descent(364/399): loss=100.00771459446857\n",
      "Stochastic Gradient Descent(365/399): loss=67.14428765868159\n",
      "Stochastic Gradient Descent(366/399): loss=10.593834320552599\n",
      "Stochastic Gradient Descent(367/399): loss=15.12232717970987\n",
      "Stochastic Gradient Descent(368/399): loss=0.0037856420369414994\n",
      "Stochastic Gradient Descent(369/399): loss=0.4872579642176356\n",
      "Stochastic Gradient Descent(370/399): loss=30.886263796151052\n",
      "Stochastic Gradient Descent(371/399): loss=0.08316356644400501\n",
      "Stochastic Gradient Descent(372/399): loss=2.8634531480898677\n",
      "Stochastic Gradient Descent(373/399): loss=14.84729635247284\n",
      "Stochastic Gradient Descent(374/399): loss=15.349705461668693\n",
      "Stochastic Gradient Descent(375/399): loss=2.7942820362839607\n",
      "Stochastic Gradient Descent(376/399): loss=5.922059239762051\n",
      "Stochastic Gradient Descent(377/399): loss=0.017335509152055662\n",
      "Stochastic Gradient Descent(378/399): loss=0.6308277170900559\n",
      "Stochastic Gradient Descent(379/399): loss=14.200867631575822\n",
      "Stochastic Gradient Descent(380/399): loss=222.2388022706206\n",
      "Stochastic Gradient Descent(381/399): loss=3.2518146749052916\n",
      "Stochastic Gradient Descent(382/399): loss=427.10935720195727\n",
      "Stochastic Gradient Descent(383/399): loss=216.3576886328627\n",
      "Stochastic Gradient Descent(384/399): loss=94.6999584603684\n",
      "Stochastic Gradient Descent(385/399): loss=342.0968488281204\n",
      "Stochastic Gradient Descent(386/399): loss=1.3678210922689398\n",
      "Stochastic Gradient Descent(387/399): loss=1.7736099286668237\n",
      "Stochastic Gradient Descent(388/399): loss=21.75999954329124\n",
      "Stochastic Gradient Descent(389/399): loss=57.946448316174134\n",
      "Stochastic Gradient Descent(390/399): loss=0.08756608521216541\n",
      "Stochastic Gradient Descent(391/399): loss=0.07964545338939537\n",
      "Stochastic Gradient Descent(392/399): loss=8.521240803072937\n",
      "Stochastic Gradient Descent(393/399): loss=67.33074654652721\n",
      "Stochastic Gradient Descent(394/399): loss=10.126333713562863\n",
      "Stochastic Gradient Descent(395/399): loss=34.35226381822296\n",
      "Stochastic Gradient Descent(396/399): loss=6.529185858670123\n",
      "Stochastic Gradient Descent(397/399): loss=3.015173232541766\n",
      "Stochastic Gradient Descent(398/399): loss=22.832129058385416\n",
      "Stochastic Gradient Descent(399/399): loss=2.83702731324716\n",
      "0.46129333333333333\n"
     ]
    }
   ],
   "source": [
    "'''SGD'''\n",
    "(w2,loss2) = least_squares_SGD(y, tX, init_w, max_iter, alpha)\n",
    "sgd_tr_pred = predict_labels(w2, tX_valid)\n",
    "print((sgd_tr_pred == y_valid).mean())\n",
    "sgd_pred = predict_labels(w2, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "(w3,loss3) = least_squares(y, tX)\n",
    "ls_tr_pred = predict_labels(w3, tX_valid)\n",
    "print((ls_tr_pred == y_valid).mean())\n",
    "ls_pred = predict_labels(w3, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "lambda_ = np.logspace(-1, -6, 30)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        min_loss = loss4\n",
    "        ind = i\n",
    "(w4,loss4) = ridge_regression(y, tX, lambda_[ind])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())\n",
    "rd_pred = predict_labels(w4, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Gradient Descent(0/399): loss=nan\n",
      "Logistic Regression Gradient Descent(1/399): loss=nan\n",
      "Logistic Regression Gradient Descent(2/399): loss=nan\n",
      "Logistic Regression Gradient Descent(3/399): loss=nan\n",
      "Logistic Regression Gradient Descent(4/399): loss=-0.8170236617247584\n",
      "Logistic Regression Gradient Descent(5/399): loss=-0.7597351941121293\n",
      "Logistic Regression Gradient Descent(6/399): loss=-0.7157554020934421\n",
      "Logistic Regression Gradient Descent(7/399): loss=-0.6820699227503821\n",
      "Logistic Regression Gradient Descent(8/399): loss=-0.6562217558855663\n",
      "Logistic Regression Gradient Descent(9/399): loss=-0.6362853211335157\n",
      "Logistic Regression Gradient Descent(10/399): loss=-0.6207973802950664\n",
      "Logistic Regression Gradient Descent(11/399): loss=-0.6086659034615315\n",
      "Logistic Regression Gradient Descent(12/399): loss=-0.5990754902443401\n",
      "Logistic Regression Gradient Descent(13/399): loss=-0.591413402969122\n",
      "Logistic Regression Gradient Descent(14/399): loss=-0.5852235744103532\n",
      "Logistic Regression Gradient Descent(15/399): loss=-0.5801671732562039\n",
      "Logistic Regression Gradient Descent(16/399): loss=-0.5759902490373269\n",
      "Logistic Regression Gradient Descent(17/399): loss=-0.5725007117198854\n",
      "Logistic Regression Gradient Descent(18/399): loss=-0.5695521540149934\n",
      "Logistic Regression Gradient Descent(19/399): loss=-0.5670322621102292\n",
      "Logistic Regression Gradient Descent(20/399): loss=-0.5648543802140182\n",
      "Logistic Regression Gradient Descent(21/399): loss=-0.5629512981236952\n",
      "Logistic Regression Gradient Descent(22/399): loss=-0.5612706325640527\n",
      "Logistic Regression Gradient Descent(23/399): loss=-0.5597713657100487\n",
      "Logistic Regression Gradient Descent(24/399): loss=-0.5584212327998257\n",
      "Logistic Regression Gradient Descent(25/399): loss=-0.5571947385405333\n",
      "Logistic Regression Gradient Descent(26/399): loss=-0.5560716429764107\n",
      "Logistic Regression Gradient Descent(27/399): loss=-0.5550358003849829\n",
      "Logistic Regression Gradient Descent(28/399): loss=-0.5540742653119604\n",
      "Logistic Regression Gradient Descent(29/399): loss=-0.5531766018508647\n",
      "Logistic Regression Gradient Descent(30/399): loss=-0.5523343482801677\n",
      "Logistic Regression Gradient Descent(31/399): loss=-0.5515406009329888\n",
      "Logistic Regression Gradient Descent(32/399): loss=-0.550789689893838\n",
      "Logistic Regression Gradient Descent(33/399): loss=-0.5500769256311774\n",
      "Logistic Regression Gradient Descent(34/399): loss=-0.5493984005745892\n",
      "Logistic Regression Gradient Descent(35/399): loss=-0.5487508333527643\n",
      "Logistic Regression Gradient Descent(36/399): loss=-0.5481314462277215\n",
      "Logistic Regression Gradient Descent(37/399): loss=-0.5475378684135803\n",
      "Logistic Regression Gradient Descent(38/399): loss=-0.5469680596182632\n",
      "Logistic Regression Gradient Descent(39/399): loss=-0.5464202494150406\n",
      "Logistic Regression Gradient Descent(40/399): loss=-0.5458928890286461\n",
      "Logistic Regression Gradient Descent(41/399): loss=-0.545384612876114\n",
      "Logistic Regression Gradient Descent(42/399): loss=-0.544894207787299\n",
      "Logistic Regression Gradient Descent(43/399): loss=-0.5444205882835687\n",
      "Logistic Regression Gradient Descent(44/399): loss=-0.5439627766454787\n",
      "Logistic Regression Gradient Descent(45/399): loss=-0.5435198867743773\n",
      "Logistic Regression Gradient Descent(46/399): loss=-0.543091111066506\n",
      "Logistic Regression Gradient Descent(47/399): loss=-0.5426757096848938\n",
      "Logistic Regression Gradient Descent(48/399): loss=-0.5422730017446791\n",
      "Logistic Regression Gradient Descent(49/399): loss=-0.5418823580295263\n",
      "Logistic Regression Gradient Descent(50/399): loss=-0.5415031949368209\n",
      "Logistic Regression Gradient Descent(51/399): loss=-0.5411349694121708\n",
      "Logistic Regression Gradient Descent(52/399): loss=-0.5407771746831745\n",
      "Logistic Regression Gradient Descent(53/399): loss=-0.5404293366413638\n",
      "Logistic Regression Gradient Descent(54/399): loss=-0.540091010751959\n",
      "Logistic Regression Gradient Descent(55/399): loss=-0.5397617793953554\n",
      "Logistic Regression Gradient Descent(56/399): loss=-0.5394412495634777\n",
      "Logistic Regression Gradient Descent(57/399): loss=-0.5391290508493549\n",
      "Logistic Regression Gradient Descent(58/399): loss=-0.5388248336803559\n",
      "Logistic Regression Gradient Descent(59/399): loss=-0.5385282677551153\n",
      "Logistic Regression Gradient Descent(60/399): loss=-0.5382390406518132\n",
      "Logistic Regression Gradient Descent(61/399): loss=-0.5379568565815602\n",
      "Logistic Regression Gradient Descent(62/399): loss=-0.5376814352654864\n",
      "Logistic Regression Gradient Descent(63/399): loss=-0.5374125109180137\n",
      "Logistic Regression Gradient Descent(64/399): loss=-0.5371498313219037\n",
      "Logistic Regression Gradient Descent(65/399): loss=-0.536893156983164\n",
      "Logistic Regression Gradient Descent(66/399): loss=-0.5366422603559043\n",
      "Logistic Regression Gradient Descent(67/399): loss=-0.5363969251288623\n",
      "Logistic Regression Gradient Descent(68/399): loss=-0.5361569455666222\n",
      "Logistic Regression Gradient Descent(69/399): loss=-0.5359221258996296\n",
      "Logistic Regression Gradient Descent(70/399): loss=-0.5356922797579683\n",
      "Logistic Regression Gradient Descent(71/399): loss=-0.5354672296445865\n",
      "Logistic Regression Gradient Descent(72/399): loss=-0.5352468064442496\n",
      "Logistic Regression Gradient Descent(73/399): loss=-0.5350308489649785\n",
      "Logistic Regression Gradient Descent(74/399): loss=-0.5348192035091517\n",
      "Logistic Regression Gradient Descent(75/399): loss=-0.5346117234717774\n",
      "Logistic Regression Gradient Descent(76/399): loss=-0.5344082689637404\n",
      "Logistic Regression Gradient Descent(77/399): loss=-0.534208706458064\n",
      "Logistic Regression Gradient Descent(78/399): loss=-0.5340129084574416\n",
      "Logistic Regression Gradient Descent(79/399): loss=-0.5338207531814682\n",
      "Logistic Regression Gradient Descent(80/399): loss=-0.5336321242721533\n",
      "Logistic Regression Gradient Descent(81/399): loss=-0.5334469105164391\n",
      "Logistic Regression Gradient Descent(82/399): loss=-0.5332650055845547\n",
      "Logistic Regression Gradient Descent(83/399): loss=-0.5330863077831474\n",
      "Logistic Regression Gradient Descent(84/399): loss=-0.5329107198222179\n",
      "Logistic Regression Gradient Descent(85/399): loss=-0.5327381485949718\n",
      "Logistic Regression Gradient Descent(86/399): loss=-0.5325685049697618\n",
      "Logistic Regression Gradient Descent(87/399): loss=-0.5324017035933736\n",
      "Logistic Regression Gradient Descent(88/399): loss=-0.5322376627049504\n",
      "Logistic Regression Gradient Descent(89/399): loss=-0.5320763039599152\n",
      "Logistic Regression Gradient Descent(90/399): loss=-0.531917552263289\n",
      "Logistic Regression Gradient Descent(91/399): loss=-0.5317613356118497\n",
      "Logistic Regression Gradient Descent(92/399): loss=-0.5316075849446149\n",
      "Logistic Regression Gradient Descent(93/399): loss=-0.5314562340011638\n",
      "Logistic Regression Gradient Descent(94/399): loss=-0.531307219187353\n",
      "Logistic Regression Gradient Descent(95/399): loss=-0.5311604794479997\n",
      "Logistic Regression Gradient Descent(96/399): loss=-0.5310159561461463\n",
      "Logistic Regression Gradient Descent(97/399): loss=-0.5308735929485336\n",
      "Logistic Regression Gradient Descent(98/399): loss=-0.5307333357169413\n",
      "Logistic Regression Gradient Descent(99/399): loss=-0.5305951324050704\n",
      "Logistic Regression Gradient Descent(100/399): loss=-0.5304589329606673\n",
      "Logistic Regression Gradient Descent(101/399): loss=-0.5303246892326015\n",
      "Logistic Regression Gradient Descent(102/399): loss=-0.5301923548826319\n",
      "Logistic Regression Gradient Descent(103/399): loss=-0.53006188530161\n",
      "Logistic Regression Gradient Descent(104/399): loss=-0.5299332375298798\n",
      "Logistic Regression Gradient Descent(105/399): loss=-0.5298063701816568\n",
      "Logistic Regression Gradient Descent(106/399): loss=-0.5296812433731702\n",
      "Logistic Regression Gradient Descent(107/399): loss=-0.5295578186543755\n",
      "Logistic Regression Gradient Descent(108/399): loss=-0.5294360589440444\n",
      "Logistic Regression Gradient Descent(109/399): loss=-0.5293159284680632\n",
      "Logistic Regression Gradient Descent(110/399): loss=-0.5291973927007655\n",
      "Logistic Regression Gradient Descent(111/399): loss=-0.5290804183091476\n",
      "Logistic Regression Gradient Descent(112/399): loss=-0.5289649730998154\n",
      "Logistic Regression Gradient Descent(113/399): loss=-0.5288510259685216\n",
      "Logistic Regression Gradient Descent(114/399): loss=-0.5287385468521639\n",
      "Logistic Regression Gradient Descent(115/399): loss=-0.528627506683114\n",
      "Logistic Regression Gradient Descent(116/399): loss=-0.528517877345762\n",
      "Logistic Regression Gradient Descent(117/399): loss=-0.5284096316351624\n",
      "Logistic Regression Gradient Descent(118/399): loss=-0.5283027432176742\n",
      "Logistic Regression Gradient Descent(119/399): loss=-0.5281971865934972\n",
      "Logistic Regression Gradient Descent(120/399): loss=-0.5280929370610057\n",
      "Logistic Regression Gradient Descent(121/399): loss=-0.5279899706827904\n",
      "Logistic Regression Gradient Descent(122/399): loss=-0.5278882642533247\n",
      "Logistic Regression Gradient Descent(123/399): loss=-0.52778779526817\n",
      "Logistic Regression Gradient Descent(124/399): loss=-0.5276885418946466\n",
      "Logistic Regression Gradient Descent(125/399): loss=-0.5275904829438964\n",
      "Logistic Regression Gradient Descent(126/399): loss=-0.5274935978442659\n",
      "Logistic Regression Gradient Descent(127/399): loss=-0.5273978666159462\n",
      "Logistic Regression Gradient Descent(128/399): loss=-0.527303269846809\n",
      "Logistic Regression Gradient Descent(129/399): loss=-0.527209788669373\n",
      "Logistic Regression Gradient Descent(130/399): loss=-0.527117404738854\n",
      "Logistic Regression Gradient Descent(131/399): loss=-0.5270261002122375\n",
      "Logistic Regression Gradient Descent(132/399): loss=-0.526935857728326\n",
      "Logistic Regression Gradient Descent(133/399): loss=-0.5268466603887156\n",
      "Logistic Regression Gradient Descent(134/399): loss=-0.5267584917396526\n",
      "Logistic Regression Gradient Descent(135/399): loss=-0.5266713357547277\n",
      "Logistic Regression Gradient Descent(136/399): loss=-0.5265851768183692\n",
      "Logistic Regression Gradient Descent(137/399): loss=-0.5264999997100934\n",
      "Logistic Regression Gradient Descent(138/399): loss=-0.5264157895894758\n",
      "Logistic Regression Gradient Descent(139/399): loss=-0.5263325319818086\n",
      "Logistic Regression Gradient Descent(140/399): loss=-0.5262502127644126\n",
      "Logistic Regression Gradient Descent(141/399): loss=-0.5261688181535668\n",
      "Logistic Regression Gradient Descent(142/399): loss=-0.5260883346920314\n",
      "Logistic Regression Gradient Descent(143/399): loss=-0.5260087492371318\n",
      "Logistic Regression Gradient Descent(144/399): loss=-0.5259300489493778\n",
      "Logistic Regression Gradient Descent(145/399): loss=-0.5258522212815921\n",
      "Logistic Regression Gradient Descent(146/399): loss=-0.5257752539685226\n",
      "Logistic Regression Gradient Descent(147/399): loss=-0.5256991350169173\n",
      "Logistic Regression Gradient Descent(148/399): loss=-0.5256238526960358\n",
      "Logistic Regression Gradient Descent(149/399): loss=-0.5255493955285822\n",
      "Logistic Regression Gradient Descent(150/399): loss=-0.5254757522820317\n",
      "Logistic Regression Gradient Descent(151/399): loss=-0.5254029119603403\n",
      "Logistic Regression Gradient Descent(152/399): loss=-0.5253308637960117\n",
      "Logistic Regression Gradient Descent(153/399): loss=-0.525259597242509\n",
      "Logistic Regression Gradient Descent(154/399): loss=-0.5251891019669953\n",
      "Logistic Regression Gradient Descent(155/399): loss=-0.5251193678433819\n",
      "Logistic Regression Gradient Descent(156/399): loss=-0.5250503849456776\n",
      "Logistic Regression Gradient Descent(157/399): loss=-0.5249821435416178\n",
      "Logistic Regression Gradient Descent(158/399): loss=-0.5249146340865639\n",
      "Logistic Regression Gradient Descent(159/399): loss=-0.5248478472176596\n",
      "Logistic Regression Gradient Descent(160/399): loss=-0.5247817737482312\n",
      "Logistic Regression Gradient Descent(161/399): loss=-0.5247164046624216\n",
      "Logistic Regression Gradient Descent(162/399): loss=-0.5246517311100455\n",
      "Logistic Regression Gradient Descent(163/399): loss=-0.524587744401657\n",
      "Logistic Regression Gradient Descent(164/399): loss=-0.5245244360038193\n",
      "Logistic Regression Gradient Descent(165/399): loss=-0.5244617975345662\n",
      "Logistic Regression Gradient Descent(166/399): loss=-0.5243998207590461\n",
      "Logistic Regression Gradient Descent(167/399): loss=-0.5243384975853442\n",
      "Logistic Regression Gradient Descent(168/399): loss=-0.5242778200604672\n",
      "Logistic Regression Gradient Descent(169/399): loss=-0.5242177803664911\n",
      "Logistic Regression Gradient Descent(170/399): loss=-0.524158370816858\n",
      "Logistic Regression Gradient Descent(171/399): loss=-0.524099583852819\n",
      "Logistic Regression Gradient Descent(172/399): loss=-0.5240414120400156\n",
      "Logistic Regression Gradient Descent(173/399): loss=-0.5239838480651922\n",
      "Logistic Regression Gradient Descent(174/399): loss=-0.5239268847330346\n",
      "Logistic Regression Gradient Descent(175/399): loss=-0.5238705149631295\n",
      "Logistic Regression Gradient Descent(176/399): loss=-0.5238147317870371\n",
      "Logistic Regression Gradient Descent(177/399): loss=-0.5237595283454753\n",
      "Logistic Regression Gradient Descent(178/399): loss=-0.5237048978856069\n",
      "Logistic Regression Gradient Descent(179/399): loss=-0.5236508337584275\n",
      "Logistic Regression Gradient Descent(180/399): loss=-0.523597329416248\n",
      "Logistic Regression Gradient Descent(181/399): loss=-0.5235443784102708\n",
      "Logistic Regression Gradient Descent(182/399): loss=-0.5234919743882489\n",
      "Logistic Regression Gradient Descent(183/399): loss=-0.5234401110922329\n",
      "Logistic Regression Gradient Descent(184/399): loss=-0.5233887823563939\n",
      "Logistic Regression Gradient Descent(185/399): loss=-0.5233379821049253\n",
      "Logistic Regression Gradient Descent(186/399): loss=-0.5232877043500159\n",
      "Logistic Regression Gradient Descent(187/399): loss=-0.5232379431898926\n",
      "Logistic Regression Gradient Descent(188/399): loss=-0.5231886928069308\n",
      "Logistic Regression Gradient Descent(189/399): loss=-0.5231399474658279\n",
      "Logistic Regression Gradient Descent(190/399): loss=-0.5230917015118373\n",
      "Logistic Regression Gradient Descent(191/399): loss=-0.523043949369062\n",
      "Logistic Regression Gradient Descent(192/399): loss=-0.522996685538803\n",
      "Logistic Regression Gradient Descent(193/399): loss=-0.5229499045979621\n",
      "Logistic Regression Gradient Descent(194/399): loss=-0.5229036011974959\n",
      "Logistic Regression Gradient Descent(195/399): loss=-0.5228577700609177\n",
      "Logistic Regression Gradient Descent(196/399): loss=-0.5228124059828487\n",
      "Logistic Regression Gradient Descent(197/399): loss=-0.5227675038276127\n",
      "Logistic Regression Gradient Descent(198/399): loss=-0.5227230585278742\n",
      "Logistic Regression Gradient Descent(199/399): loss=-0.5226790650833189\n",
      "Logistic Regression Gradient Descent(200/399): loss=-0.522635518559373\n",
      "Logistic Regression Gradient Descent(201/399): loss=-0.5225924140859619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Gradient Descent(202/399): loss=-0.5225497468563041\n",
      "Logistic Regression Gradient Descent(203/399): loss=-0.522507512125741\n",
      "Logistic Regression Gradient Descent(204/399): loss=-0.5224657052106011\n",
      "Logistic Regression Gradient Descent(205/399): loss=-0.5224243214870957\n",
      "Logistic Regression Gradient Descent(206/399): loss=-0.5223833563902459\n",
      "Logistic Regression Gradient Descent(207/399): loss=-0.5223428054128404\n",
      "Logistic Regression Gradient Descent(208/399): loss=-0.5223026641044208\n",
      "Logistic Regression Gradient Descent(209/399): loss=-0.5222629280702956\n",
      "Logistic Regression Gradient Descent(210/399): loss=-0.5222235929705805\n",
      "Logistic Regression Gradient Descent(211/399): loss=-0.5221846545192643\n",
      "Logistic Regression Gradient Descent(212/399): loss=-0.5221461084832991\n",
      "Logistic Regression Gradient Descent(213/399): loss=-0.5221079506817147\n",
      "Logistic Regression Gradient Descent(214/399): loss=-0.522070176984756\n",
      "Logistic Regression Gradient Descent(215/399): loss=-0.522032783313042\n",
      "Logistic Regression Gradient Descent(216/399): loss=-0.5219957656367457\n",
      "Logistic Regression Gradient Descent(217/399): loss=-0.5219591199747948\n",
      "Logistic Regression Gradient Descent(218/399): loss=-0.5219228423940929\n",
      "Logistic Regression Gradient Descent(219/399): loss=-0.5218869290087578\n",
      "Logistic Regression Gradient Descent(220/399): loss=-0.5218513759793797\n",
      "Logistic Regression Gradient Descent(221/399): loss=-0.521816179512297\n",
      "Logistic Regression Gradient Descent(222/399): loss=-0.5217813358588874\n",
      "Logistic Regression Gradient Descent(223/399): loss=-0.521746841314878\n",
      "Logistic Regression Gradient Descent(224/399): loss=-0.5217126922196699\n",
      "Logistic Regression Gradient Descent(225/399): loss=-0.5216788849556775\n",
      "Logistic Regression Gradient Descent(226/399): loss=-0.5216454159476843\n",
      "Logistic Regression Gradient Descent(227/399): loss=-0.5216122816622116\n",
      "Logistic Regression Gradient Descent(228/399): loss=-0.5215794786069023\n",
      "Logistic Regression Gradient Descent(229/399): loss=-0.5215470033299173\n",
      "Logistic Regression Gradient Descent(230/399): loss=-0.5215148524193445\n",
      "Logistic Regression Gradient Descent(231/399): loss=-0.5214830225026215\n",
      "Logistic Regression Gradient Descent(232/399): loss=-0.5214515102459701\n",
      "Logistic Regression Gradient Descent(233/399): loss=-0.5214203123538425\n",
      "Logistic Regression Gradient Descent(234/399): loss=-0.5213894255683776\n",
      "Logistic Regression Gradient Descent(235/399): loss=-0.521358846668871\n",
      "Logistic Regression Gradient Descent(236/399): loss=-0.5213285724712541\n",
      "Logistic Regression Gradient Descent(237/399): loss=-0.5212985998275829\n",
      "Logistic Regression Gradient Descent(238/399): loss=-0.5212689256255391\n",
      "Logistic Regression Gradient Descent(239/399): loss=-0.5212395467879387\n",
      "Logistic Regression Gradient Descent(240/399): loss=-0.5212104602722517\n",
      "Logistic Regression Gradient Descent(241/399): loss=-0.5211816630701303\n",
      "Logistic Regression Gradient Descent(242/399): loss=-0.5211531522069456\n",
      "Logistic Regression Gradient Descent(243/399): loss=-0.521124924741335\n",
      "Logistic Regression Gradient Descent(244/399): loss=-0.5210969777647557\n",
      "Logistic Regression Gradient Descent(245/399): loss=-0.5210693084010473\n",
      "Logistic Regression Gradient Descent(246/399): loss=-0.5210419138060025\n",
      "Logistic Regression Gradient Descent(247/399): loss=-0.5210147911669468\n",
      "Logistic Regression Gradient Descent(248/399): loss=-0.520987937702323\n",
      "Logistic Regression Gradient Descent(249/399): loss=-0.5209613506612855\n",
      "Logistic Regression Gradient Descent(250/399): loss=-0.5209350273233007\n",
      "Logistic Regression Gradient Descent(251/399): loss=-0.5209089649977545\n",
      "Logistic Regression Gradient Descent(252/399): loss=-0.5208831610235669\n",
      "Logistic Regression Gradient Descent(253/399): loss=-0.5208576127688129\n",
      "Logistic Regression Gradient Descent(254/399): loss=-0.5208323176303504\n",
      "Logistic Regression Gradient Descent(255/399): loss=-0.5208072730334526\n",
      "Logistic Regression Gradient Descent(256/399): loss=-0.5207824764314505\n",
      "Logistic Regression Gradient Descent(257/399): loss=-0.5207579253053758\n",
      "Logistic Regression Gradient Descent(258/399): loss=-0.5207336171636153\n",
      "Logistic Regression Gradient Descent(259/399): loss=-0.5207095495415669\n",
      "Logistic Regression Gradient Descent(260/399): loss=-0.5206857200013029\n",
      "Logistic Regression Gradient Descent(261/399): loss=-0.520662126131239\n",
      "Logistic Regression Gradient Descent(262/399): loss=-0.5206387655458073\n",
      "Logistic Regression Gradient Descent(263/399): loss=-0.5206156358851366\n",
      "Logistic Regression Gradient Descent(264/399): loss=-0.5205927348147347\n",
      "Logistic Regression Gradient Descent(265/399): loss=-0.5205700600251789\n",
      "Logistic Regression Gradient Descent(266/399): loss=-0.5205476092318096\n",
      "Logistic Regression Gradient Descent(267/399): loss=-0.5205253801744285\n",
      "Logistic Regression Gradient Descent(268/399): loss=-0.5205033706170017\n",
      "Logistic Regression Gradient Descent(269/399): loss=-0.5204815783473684\n",
      "Logistic Regression Gradient Descent(270/399): loss=-0.5204600011769521\n",
      "Logistic Regression Gradient Descent(271/399): loss=-0.520438636940478\n",
      "Logistic Regression Gradient Descent(272/399): loss=-0.5204174834956933\n",
      "Logistic Regression Gradient Descent(273/399): loss=-0.5203965387230921\n",
      "Logistic Regression Gradient Descent(274/399): loss=-0.5203758005256449\n",
      "Logistic Regression Gradient Descent(275/399): loss=-0.5203552668285316\n",
      "Logistic Regression Gradient Descent(276/399): loss=-0.5203349355788778\n",
      "Logistic Regression Gradient Descent(277/399): loss=-0.5203148047454961\n",
      "Logistic Regression Gradient Descent(278/399): loss=-0.5202948723186306\n",
      "Logistic Regression Gradient Descent(279/399): loss=-0.5202751363097053\n",
      "Logistic Regression Gradient Descent(280/399): loss=-0.5202555947510754\n",
      "Logistic Regression Gradient Descent(281/399): loss=-0.5202362456957829\n",
      "Logistic Regression Gradient Descent(282/399): loss=-0.5202170872173154\n",
      "Logistic Regression Gradient Descent(283/399): loss=-0.5201981174093687\n",
      "Logistic Regression Gradient Descent(284/399): loss=-0.5201793343856116\n",
      "Logistic Regression Gradient Descent(285/399): loss=-0.5201607362794555\n",
      "Logistic Regression Gradient Descent(286/399): loss=-0.5201423212438258\n",
      "Logistic Regression Gradient Descent(287/399): loss=-0.5201240874509375\n",
      "Logistic Regression Gradient Descent(288/399): loss=-0.5201060330920735\n",
      "Logistic Regression Gradient Descent(289/399): loss=-0.5200881563773656\n",
      "Logistic Regression Gradient Descent(290/399): loss=-0.5200704555355795\n",
      "Logistic Regression Gradient Descent(291/399): loss=-0.5200529288139011\n",
      "Logistic Regression Gradient Descent(292/399): loss=-0.5200355744777272\n",
      "Logistic Regression Gradient Descent(293/399): loss=-0.5200183908104589\n",
      "Logistic Regression Gradient Descent(294/399): loss=-0.5200013761132959\n",
      "Logistic Regression Gradient Descent(295/399): loss=-0.5199845287050369\n",
      "Logistic Regression Gradient Descent(296/399): loss=-0.5199678469218791\n",
      "Logistic Regression Gradient Descent(297/399): loss=-0.5199513291172229\n",
      "Logistic Regression Gradient Descent(298/399): loss=-0.5199349736614776\n",
      "Logistic Regression Gradient Descent(299/399): loss=-0.5199187789418714\n",
      "Logistic Regression Gradient Descent(300/399): loss=-0.5199027433622614\n",
      "Logistic Regression Gradient Descent(301/399): loss=-0.5198868653429488\n",
      "Logistic Regression Gradient Descent(302/399): loss=-0.5198711433204943\n",
      "Logistic Regression Gradient Descent(303/399): loss=-0.5198555757475374\n",
      "Logistic Regression Gradient Descent(304/399): loss=-0.5198401610926169\n",
      "Logistic Regression Gradient Descent(305/399): loss=-0.5198248978399947\n",
      "Logistic Regression Gradient Descent(306/399): loss=-0.5198097844894812\n",
      "Logistic Regression Gradient Descent(307/399): loss=-0.5197948195562632\n",
      "Logistic Regression Gradient Descent(308/399): loss=-0.5197800015707336\n",
      "Logistic Regression Gradient Descent(309/399): loss=-0.5197653290783241\n",
      "Logistic Regression Gradient Descent(310/399): loss=-0.5197508006393395\n",
      "Logistic Regression Gradient Descent(311/399): loss=-0.5197364148287935\n",
      "Logistic Regression Gradient Descent(312/399): loss=-0.5197221702362478\n",
      "Logistic Regression Gradient Descent(313/399): loss=-0.5197080654656524\n",
      "Logistic Regression Gradient Descent(314/399): loss=-0.5196940991351876\n",
      "Logistic Regression Gradient Descent(315/399): loss=-0.5196802698771096\n",
      "Logistic Regression Gradient Descent(316/399): loss=-0.5196665763375956\n",
      "Logistic Regression Gradient Descent(317/399): loss=-0.5196530171765931\n",
      "Logistic Regression Gradient Descent(318/399): loss=-0.5196395910676694\n",
      "Logistic Regression Gradient Descent(319/399): loss=-0.5196262966978643\n",
      "Logistic Regression Gradient Descent(320/399): loss=-0.5196131327675426\n",
      "Logistic Regression Gradient Descent(321/399): loss=-0.5196000979902518\n",
      "Logistic Regression Gradient Descent(322/399): loss=-0.5195871910925774\n",
      "Logistic Regression Gradient Descent(323/399): loss=-0.5195744108140036\n",
      "Logistic Regression Gradient Descent(324/399): loss=-0.5195617559067733\n",
      "Logistic Regression Gradient Descent(325/399): loss=-0.5195492251357507\n",
      "Logistic Regression Gradient Descent(326/399): loss=-0.5195368172782863\n",
      "Logistic Regression Gradient Descent(327/399): loss=-0.5195245311240813\n",
      "Logistic Regression Gradient Descent(328/399): loss=-0.5195123654750563\n",
      "Logistic Regression Gradient Descent(329/399): loss=-0.51950031914522\n",
      "Logistic Regression Gradient Descent(330/399): loss=-0.5194883909605401\n",
      "Logistic Regression Gradient Descent(331/399): loss=-0.5194765797588146\n",
      "Logistic Regression Gradient Descent(332/399): loss=-0.5194648843895456\n",
      "Logistic Regression Gradient Descent(333/399): loss=-0.5194533037138164\n",
      "Logistic Regression Gradient Descent(334/399): loss=-0.5194418366041655\n",
      "Logistic Regression Gradient Descent(335/399): loss=-0.5194304819444664\n",
      "Logistic Regression Gradient Descent(336/399): loss=-0.5194192386298068\n",
      "Logistic Regression Gradient Descent(337/399): loss=-0.5194081055663695\n",
      "Logistic Regression Gradient Descent(338/399): loss=-0.5193970816713148\n",
      "Logistic Regression Gradient Descent(339/399): loss=-0.5193861658726647\n",
      "Logistic Regression Gradient Descent(340/399): loss=-0.5193753571091876\n",
      "Logistic Regression Gradient Descent(341/399): loss=-0.5193646543302843\n",
      "Logistic Regression Gradient Descent(342/399): loss=-0.5193540564958775\n",
      "Logistic Regression Gradient Descent(343/399): loss=-0.5193435625762993\n",
      "Logistic Regression Gradient Descent(344/399): loss=-0.519333171552182\n",
      "Logistic Regression Gradient Descent(345/399): loss=-0.5193228824143505\n",
      "Logistic Regression Gradient Descent(346/399): loss=-0.5193126941637145\n",
      "Logistic Regression Gradient Descent(347/399): loss=-0.5193026058111629\n",
      "Logistic Regression Gradient Descent(348/399): loss=-0.5192926163774593\n",
      "Logistic Regression Gradient Descent(349/399): loss=-0.5192827248931383\n",
      "Logistic Regression Gradient Descent(350/399): loss=-0.5192729303984038\n",
      "Logistic Regression Gradient Descent(351/399): loss=-0.5192632319430277\n",
      "Logistic Regression Gradient Descent(352/399): loss=-0.5192536285862497\n",
      "Logistic Regression Gradient Descent(353/399): loss=-0.5192441193966792\n",
      "Logistic Regression Gradient Descent(354/399): loss=-0.5192347034521975\n",
      "Logistic Regression Gradient Descent(355/399): loss=-0.5192253798398607\n",
      "Logistic Regression Gradient Descent(356/399): loss=-0.5192161476558053\n",
      "Logistic Regression Gradient Descent(357/399): loss=-0.5192070060051527\n",
      "Logistic Regression Gradient Descent(358/399): loss=-0.5191979540019176\n",
      "Logistic Regression Gradient Descent(359/399): loss=-0.519188990768914\n",
      "Logistic Regression Gradient Descent(360/399): loss=-0.5191801154376652\n",
      "Logistic Regression Gradient Descent(361/399): loss=-0.5191713271483135\n",
      "Logistic Regression Gradient Descent(362/399): loss=-0.5191626250495304\n",
      "Logistic Regression Gradient Descent(363/399): loss=-0.5191540082984298\n",
      "Logistic Regression Gradient Descent(364/399): loss=-0.5191454760604794\n",
      "Logistic Regression Gradient Descent(365/399): loss=-0.5191370275094157\n",
      "Logistic Regression Gradient Descent(366/399): loss=-0.5191286618271584\n",
      "Logistic Regression Gradient Descent(367/399): loss=-0.5191203782037257\n",
      "Logistic Regression Gradient Descent(368/399): loss=-0.519112175837152\n",
      "Logistic Regression Gradient Descent(369/399): loss=-0.5191040539334044\n",
      "Logistic Regression Gradient Descent(370/399): loss=-0.5190960117063025\n",
      "Logistic Regression Gradient Descent(371/399): loss=-0.5190880483774366\n",
      "Logistic Regression Gradient Descent(372/399): loss=-0.5190801631760888\n",
      "Logistic Regression Gradient Descent(373/399): loss=-0.5190723553391541\n",
      "Logistic Regression Gradient Descent(374/399): loss=-0.5190646241110625\n",
      "Logistic Regression Gradient Descent(375/399): loss=-0.5190569687437016\n",
      "Logistic Regression Gradient Descent(376/399): loss=-0.5190493884963409\n",
      "Logistic Regression Gradient Descent(377/399): loss=-0.5190418826355561\n",
      "Logistic Regression Gradient Descent(378/399): loss=-0.5190344504351547\n",
      "Logistic Regression Gradient Descent(379/399): loss=-0.5190270911761022\n",
      "Logistic Regression Gradient Descent(380/399): loss=-0.5190198041464494\n",
      "Logistic Regression Gradient Descent(381/399): loss=-0.51901258864126\n",
      "Logistic Regression Gradient Descent(382/399): loss=-0.5190054439625393\n",
      "Logistic Regression Gradient Descent(383/399): loss=-0.5189983694191638\n",
      "Logistic Regression Gradient Descent(384/399): loss=-0.5189913643268114\n",
      "Logistic Regression Gradient Descent(385/399): loss=-0.5189844280078921\n",
      "Logistic Regression Gradient Descent(386/399): loss=-0.5189775597914802\n",
      "Logistic Regression Gradient Descent(387/399): loss=-0.5189707590132465\n",
      "Logistic Regression Gradient Descent(388/399): loss=-0.5189640250153913\n",
      "Logistic Regression Gradient Descent(389/399): loss=-0.5189573571465783\n",
      "Logistic Regression Gradient Descent(390/399): loss=-0.5189507547618701\n",
      "Logistic Regression Gradient Descent(391/399): loss=-0.5189442172226623\n",
      "Logistic Regression Gradient Descent(392/399): loss=-0.5189377438966202\n",
      "Logistic Regression Gradient Descent(393/399): loss=-0.5189313341576154\n",
      "Logistic Regression Gradient Descent(394/399): loss=-0.5189249873856633\n",
      "Logistic Regression Gradient Descent(395/399): loss=-0.5189187029668609\n",
      "Logistic Regression Gradient Descent(396/399): loss=-0.5189124802933262\n",
      "Logistic Regression Gradient Descent(397/399): loss=-0.5189063187631362\n",
      "Logistic Regression Gradient Descent(398/399): loss=-0.5189002177802685\n",
      "Logistic Regression Gradient Descent(399/399): loss=-0.5188941767545411\n",
      "0.7063955555555556\n"
     ]
    }
   ],
   "source": [
    "'''LOGISTIC REGRESSION'''\n",
    "(w5,loss5) = logistic_regression(y, tX, init_w, max_iter, alpha)\n",
    "log_tr_pred = predict_labels(w5, tX_valid)\n",
    "print((log_tr_pred == y_valid).mean())\n",
    "log_pred = predict_labels(w5, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/results_least_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w1, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
