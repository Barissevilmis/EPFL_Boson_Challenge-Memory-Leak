{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '/Users/wifinaynay/downloads/data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "'''DATASET INTRINSICS AND SHAPE (TARGETS AND IDS INCLUDED)'''\n",
    "def DataSetInfo(y, tX_old, ids):\n",
    "    print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "    print(\"Targets: \", y)\n",
    "    print(\"Ids: \",ids)\n",
    "    print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)\n",
    "DataSetInfo(y, tX_old, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIALIZE WEIGHTS'''\n",
    "def InitWeights(feat):\n",
    "    ww = np.random.rand(feat)\n",
    "    init_w = np.array(ww, dtype=np.float64)\n",
    "    return init_w\n",
    "\n",
    "'''HYPER PARAMETERS'''\n",
    "def HyperParameters():\n",
    "    max_iter = 800\n",
    "    epochs = 10\n",
    "    gamma = 1e-1\n",
    "    lambda_ = 1e-2\n",
    "    return max_iter, epochs, gamma, lambda_\n",
    "\n",
    "'''TAKE LOG TRANSFORMATION OF FEATURES'''\n",
    "def LogTransformData(tX, features):  \n",
    "    data = tX[:, features]\n",
    "    indices = np.where(data > -999)\n",
    "    data[indices] = np.log(1 + data[indices])\n",
    "    tX = ManipulateFeatures(tX, data, features)    \n",
    "    return tX\n",
    "\n",
    "'''DELETE GIVEN FEATURE VECTOR FROM FEATURE AND CONCETENATE WITH NEW DATA '''\n",
    "def ManipulateFeatures(tX, data,features):\n",
    "    tX = np.delete(tX, features, 1)\n",
    "    return np.hstack((tX, data))\n",
    "\n",
    "'''IMPUTE DATA WITH MOST FREQUENT VALUES OR ZERO'''\n",
    "def ImputeData(tX, typ=\"ZERO\"):\n",
    "    for i in range(tX.shape[1]):\n",
    "        '''REPLACE ACCORDING TO NAN VALUES(-999)'''\n",
    "        nan_ind = np.nonzero(np.isnan(tX[:,i]))\n",
    "        if np.any(tX[:, i] == -999):\n",
    "            tX_nonzero = (tX[:, i] > -999)\n",
    "            val, count = np.unique(tX[tX_nonzero, i], return_counts=True)\n",
    "            if (len(val) >= 2) and typ == \"MF\":\n",
    "                '''MOST FREQUENT VALUE'''\n",
    "                tX[~tX_nonzero, i] = val[np.argmax(count)]\n",
    "                tX[nan_ind,i] = val[np.argmax(count)]\n",
    "            elif typ == \"ZERO\":\n",
    "                '''ZERO'''\n",
    "                tX[~tX_nonzero, i] = 0\n",
    "                tX[nan_ind,i] = 0\n",
    "    return tX\n",
    "\n",
    "'''STANDARDIZE'''\n",
    "def Standardize(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "'''PREPROCESS'''\n",
    "def PreProcess(tX):\n",
    "    '''FEATURES PICKED BY HAND'''\n",
    "    log_feature_vec = np.array(([0, 2, 5, 9, 13, 16, 19, 21, 23, 26, 29]))\n",
    "    '''LUCKY FEATURE OF THE WEEK: 30 :)'''\n",
    "    lucky_feature = np.array(([30]))\n",
    "    tX = LogTransformData(tX, log_feature_vec)\n",
    "    tX = ImputeData(tX, \"MF\")\n",
    "    tX = Standardize(tX)[0]\n",
    "    return tX\n",
    "\n",
    "'''DATASET SEPERATED IN TERMS OF CATEGORIES IN COLUMN 22'''\n",
    "def Categorize_Train(y, tX, ids):\n",
    "    '''CATEGORIES '''\n",
    "    ind = [[] for j in range(4)]\n",
    "    xx = [[] for j in range(4)]\n",
    "    yy = [[] for j in range(4)]\n",
    "    iids = [[] for j in range(4)]\n",
    "    \n",
    "    for i in range(4): \n",
    "        ind[i] = np.nonzero(tX[:, 0] == i)[0]\n",
    "        xx[i] = tX[ind[i]]\n",
    "        yy[i] = y[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return yy, xx, iids, ind\n",
    "\n",
    "def Categorize_Test(tX, ids):\n",
    "    '''CATEGORIES '''\n",
    "    ind = [[] for j in range(4)]\n",
    "    xx = [[] for j in range(4)]\n",
    "    iids = [[] for j in range(4)]\n",
    "    \n",
    "    for i in range(4): \n",
    "        ind[i] = np.nonzero(tX[:, 0] == i)[0]\n",
    "        xx[i] = tX[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return xx, iids, ind\n",
    "\n",
    "'''PREDICTIONS INTO COMPARABLE FORM'''\n",
    "def Decategorize(y_cat, ind):\n",
    "    size = y_cat[0].shape[0] + y_cat[1].shape[0] + y_cat[2].shape[0]+ y_cat[3].shape[0]\n",
    "    y = np.zeros((size,), dtype=np.float)\n",
    "    for i in range(len(y_cat)):\n",
    "        y[ind[i]] = y_cat[i]\n",
    "    return y\n",
    "\n",
    "'''CHECK VALIDATION SCORE'''\n",
    "def WeightedAverage(pred, target):\n",
    "    total_count = pred[0].shape[0] + pred[1].shape[0] + pred[2].shape[0]+ pred[3].shape[0]\n",
    "    true_count = 0\n",
    "    for i in range(4):\n",
    "        true_count +=  np.sum(pred[i] == target[i])\n",
    "    acc = true_count / total_count\n",
    "    return acc\n",
    "'''FEATURE CORRELATION MAP: ONLY FOR VISUALIZATION'''\n",
    "'''CORRELATED FEATURES: CORR > THRESHOLD : USE FOR SYNTHESIS'''\n",
    "def CorrMap(tX):\n",
    "    df = pd.DataFrame(tX)\n",
    "    corr = df.corr()\n",
    "    return corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "'''FEATURE ENGINEERING'''\n",
    "def FeatureSynthesis(tX_pp, tX_old):\n",
    "    '''CORRELATED FEATURES WILL BE USED FOR NEW FEATURE ADDITION'''\n",
    "    '''MIN PART'''\n",
    "    #tX = np.hstack((tX_pp, np.minimum((tX_old[:,15:16] - tX_old[:,18:19]),np.minimum((tX_old[:,15:16] - tX_old[:,20:21]),(tX_old[:,18:19] - tX_old[:,20:21])))))                                           \n",
    "    tX = np.hstack((tX_pp, np.minimum((tX_old[:,15:16] - tX_old[:,20:21]),(tX_old[:,18:19] - tX_old[:,20:21]))))\n",
    "    tX = np.hstack((tX, np.minimum((tX_old[:,15:16] - tX_old[:,18:19]),(tX_old[:,15:16] - tX_old[:,20:21]))))\n",
    "    tX = np.hstack((tX, (tX_old[:,18:19] - tX_old[:,20:21])))\n",
    "    '''LN PART'''\n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,13:14]*tX_old[:,14:15])))+(tX_old[:,13:14]*tX_old[:,14:15]))))                                                                                         \n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,16:17]*tX_old[:,17:18])))+(tX_old[:,16:17]*tX_old[:,16:17]))))                                                                                         \n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,23:24]*tX_old[:,24:25])))+(tX_old[:,23:24]*tX_old[:,24:25]))))                                                                                         \n",
    "    return tX\n",
    "\n",
    "'''RANDOM DATA SPLIT'''\n",
    "def RandomizedDataSplit(tX, y, ids, inds, split_size = 0.1, my_seed=1):\n",
    "    '''SET SEED FOR REMOVING RANDOMNESS'''\n",
    "    #np.random.seed(my_seed)\n",
    "    '''RANDOM INDEXES'''\n",
    "    size = y.shape[0]\n",
    "    ind = np.random.permutation(size)\n",
    "    split = int(np.floor(split_size * size))\n",
    "    \n",
    "    ind_train = ind[split:]\n",
    "    ind_valid = ind[:split]\n",
    "    \n",
    "    \n",
    "    '''SPLIT DATA ACCORDING TO RANDOM INDICES'''\n",
    "    tX_train = tX[ind_train]\n",
    "    tX_valid = tX[ind_valid]\n",
    "    y_train = y[ind_train]\n",
    "    y_valid = y[ind_valid]\n",
    "    ids_train = ids[ind_train]\n",
    "    ids_valid = ids[ind_valid]\n",
    "    inds_train = inds[ind_train]\n",
    "    inds_valid = inds[ind_valid]\n",
    "    \n",
    "    print(\"Shapes of tX, y, Ids & Indices for Training: \", tX_train.shape, y_train.shape, ids_train.shape, inds_train.shape)\n",
    "    print(\"Shapes of tX, y, Ids & Indices for Validation: \", tX_valid.shape, y_valid.shape, ids_valid.shape, inds_valid.shape)\n",
    "    return (tX_train, y_train, ids_train, inds_train),(tX_valid, y_valid, ids_valid, inds_valid)\n",
    "\n",
    "'''BACKWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def BackwardSelection(y, tX, tX_valid, y_valid):\n",
    "    selected_features = []\n",
    "    cur_best_acc = 0\n",
    "    improved = True     \n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        worst_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features:\n",
    "                \n",
    "                diff = set(list(range(tX.shape[1]))) - set(selected_features + [i])            \n",
    "                #calculate accuracy\n",
    "                #print(tX[:,list(diff)].shape,y.shape)\n",
    "                \n",
    "                cur_acc = CrossValidation(y, tX[:,list(diff)],10)\n",
    "                print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    print(\"best so far: \",cur_best_acc)\n",
    "                    improved = True\n",
    "                    cur_best_acc = cur_acc\n",
    "                    worst_ft = i                    \n",
    "        if improved:\n",
    "            selected_features.append(worst_ft)      \n",
    "            \n",
    "    return list(set(list(range(tX.shape[1]))) - set(selected_features )), cur_best_acc\n",
    "\n",
    "'''FORWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def ForwardSelection(y, tX, tX_valid, y_valid):    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0  \n",
    "    improved = True\n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        best_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features: \n",
    "                \n",
    "                #calculate accuracy\n",
    "                \n",
    "                cur_acc = CrossValidation(y, tX[:,selected_features+[i]],5)\n",
    "                print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    improved = True                   \n",
    "                    cur_best_acc = cur_acc\n",
    "                    best_ft = i\n",
    "                    \n",
    "                    \n",
    "        if improved:\n",
    "            selected_features.append(best_ft)\n",
    "            print(selected_features)\n",
    "         \n",
    "    return selected_features, cur_best_acc\n",
    "\n",
    "'''CROSS VALIDATION HELPER FUNCTION'''\n",
    "def SelectIndices(y, k_fold, seed):\n",
    "    row_count = y.shape[0]\n",
    "    window_size = int((row_count / k_fold))\n",
    "    remainder = row_count % k_fold\n",
    "    np.random.seed(seed)\n",
    "    rand_indices = np.random.permutation(row_count)\n",
    "    indices = [[] for i in range(k_fold)]\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        \n",
    "            indices[k] = [rand_indices[k*window_size:(k+1)*window_size]]\n",
    "            \n",
    "    return np.array(indices)\n",
    "\n",
    "''' adds polynomials of features '''\n",
    "def AddFeatures(tX):\n",
    "    prime_numbers = [2]\n",
    "    pm = 0\n",
    "    for pm in range(len(prime_numbers)):\n",
    "        for i in range(tX.shape[1]):\n",
    "            print(\"hahahaa \",i)\n",
    "            tX = np.hstack((tX, np.power(tX[:,i], prime_numbers[pm]).reshape(-1,1)))\n",
    "    return tX\n",
    "\n",
    "def RemoveFeatures(tX, ratio = 0.):\n",
    "    \n",
    "    \n",
    "    tX = np.hstack((tX[:,22].reshape(-1,1),np.delete(tX,22,1)))\n",
    "    selected_indices = []\n",
    "    print(\"hereee\")\n",
    "    \n",
    "    for i in range(tX.shape[1]):\n",
    "        res = np.count_nonzero(tX[:,i] == -999)\n",
    "        \n",
    "        if (res / tX.shape[0]) > ratio:\n",
    "            print(i)\n",
    "            selected_indices.append(i)\n",
    "    diff = list(set(list(range(tX.shape[1])))- set(selected_indices))\n",
    "    return tX[:,diff]\n",
    "'''CROSS VALIDATION'''\n",
    "def CrossValidation(y, tX, k):\n",
    "    print(\"here\")\n",
    "    seed = np.random.randint(10)\n",
    "    indices = SelectIndices(y,k,seed)\n",
    "    average_acc = 0\n",
    "    for i in range (k):        \n",
    "\n",
    "        xk_train = tX[~indices[i]]\n",
    "        xk_valid = tX[indices[i]]\n",
    "        yk_train = y[~indices[i]]\n",
    "        yk_valid = y[indices[i]]\n",
    "        #print(yk_train.shape,xk_train.shape)\n",
    "        #print(xk_valid.shape)\n",
    "        max_iter, epochs, gamma, lambda_ = HyperParameters()\n",
    "        init_w_gd = np.array((InitWeights(xk_train[0].shape[1])))\n",
    "        \n",
    "        (w3,loss3) = least_squares_GD(yk_train[0], xk_train[0], init_w_gd, max_iter, gamma)\n",
    "        ls_tr_pred = predict_labels(w3, xk_valid[0])\n",
    "        #print(\"heree\",(ls_tr_pred == yk_valid[0]).mean())\n",
    "        average_acc += (ls_tr_pred == yk_valid[0]).mean()/k\n",
    "\n",
    "    return average_acc\n",
    "    \n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Train(y, tX_old, ids):\n",
    "    tX_old = RemoveFeatures(tX_old)\n",
    "    tX_old = AddFeatures(tX_old)\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize_Train(y, tX_old, ids)\n",
    "\n",
    "    for i in range(len(tX_cat)):\n",
    "        tX_cat[i] = PreProcess(tX_cat[i])\n",
    "\n",
    "    '''TRAIN SET'''\n",
    "    tX_tr_cat = [[] for j in range(4)]\n",
    "    y_tr_cat = [[] for j in range(4)]\n",
    "    id_tr_cat = [[] for j in range(4)]\n",
    "    ind_tr_cat = [[] for j in range(4)]\n",
    "\n",
    "    '''VALID SET'''\n",
    "    tX_val_cat = [[] for j in range(4)]\n",
    "    y_val_cat = [[] for j in range(4)]\n",
    "    id_val_cat = [[] for j in range(4)]\n",
    "    ind_val_cat = [[] for j in range(4)]\n",
    "\n",
    "    for i in range(len(tX_cat)):\n",
    "        (tX_tr_cat[i], y_tr_cat[i],id_tr_cat[i],ind_tr_cat[i]), (tX_val_cat[i], y_val_cat[i],id_val_cat[i], ind_val_cat[i]) = RandomizedDataSplit(tX_cat[i], y_cat[i], id_cat[i], ind_cat[i])\n",
    "\n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_tr_cat = np.array((tX_tr_cat))\n",
    "    y_tr_cat = np.array((y_tr_cat))\n",
    "    id_tr_cat = np.array((id_tr_cat))\n",
    "    ind_tr_cat = np.array((ind_tr_cat))\n",
    "\n",
    "    tX_val_cat = np.array((tX_val_cat))\n",
    "    y_val_cat = np.array((y_val_cat))\n",
    "    id_val_cat = np.array((id_val_cat))\n",
    "    ind_val_cat = np.array((ind_val_cat))\n",
    "\n",
    "    return (y_tr_cat, tX_tr_cat, id_tr_cat, ind_tr_cat), (y_val_cat, tX_val_cat, id_val_cat, ind_val_cat)\n",
    "   \n",
    "\n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Test(tX_old, ids):\n",
    "    tX_cat, id_cat, ind_cat = Categorize_Test(tX_old, ids)\n",
    "    \n",
    "    for i in range(len(tX_cat)):\n",
    "        tX_cat[i] = PreProcess(tX_cat[i])\n",
    "       \n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_cat = np.array((tX_cat))\n",
    "    id_cat = np.array((id_cat))\n",
    "    ind_cat = np.array((ind_cat))\n",
    "    \n",
    "    return tX_cat, id_cat, ind_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hereee\n",
      "5\n",
      "6\n",
      "7\n",
      "13\n",
      "26\n",
      "27\n",
      "28\n",
      "hahahaa  0\n",
      "hahahaa  1\n",
      "hahahaa  2\n",
      "hahahaa  3\n",
      "hahahaa  4\n",
      "hahahaa  5\n",
      "hahahaa  6\n",
      "hahahaa  7\n",
      "hahahaa  8\n",
      "hahahaa  9\n",
      "hahahaa  10\n",
      "hahahaa  11\n",
      "hahahaa  12\n",
      "hahahaa  13\n",
      "hahahaa  14\n",
      "hahahaa  15\n",
      "hahahaa  16\n",
      "hahahaa  17\n",
      "hahahaa  18\n",
      "hahahaa  19\n",
      "hahahaa  20\n",
      "hahahaa  21\n",
      "hahahaa  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wifinaynay/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/wifinaynay/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in log\n",
      "/Users/wifinaynay/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of tX, y, Ids & Indices for Training:  (89922, 46) (89922,) (89922,) (89922,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (9991, 46) (9991,) (9991,) (9991,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (69790, 46) (69790,) (69790,) (69790,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (7754, 46) (7754,) (7754,) (7754,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (45342, 46) (45342,) (45342,) (45342,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (5037, 46) (5037,) (5037,) (5037,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (19948, 46) (19948,) (19948,) (19948,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (2216, 46) (2216,) (2216,) (2216,)\n",
      "Categorized y shape: (4,)\n",
      "Categorized tX shape: (4,)\n",
      "Categorized ids shape: (4,)\n",
      "Categorized ids shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "(y_cat, tX_cat, ids_cat, ind_cat), (y_val_cat, tX_val_cat, ids_val_cat, ind_val_cat) = BuildDataModel_Train(y,tX_old,ids)\n",
    "y_cat = np.array((y_cat))\n",
    "tX_cat = np.array((tX_cat))\n",
    "ids_cat = np.array((ids_cat))\n",
    "ind_cat = np.array((ind_cat))\n",
    "print(\"Categorized y shape:\", y_cat.shape)\n",
    "print(\"Categorized tX shape:\", tX_cat.shape)\n",
    "print(\"Categorized ids shape:\", ids_cat.shape)\n",
    "print(\"Categorized ids shape:\", ind_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tx@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(w.T@w))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    z = 1 / (1 + np.exp(-1*(tx@w)))\n",
    "    return z\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm)))\n",
    "    \n",
    "    return loss\n",
    "def compute_log_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION FOR LR'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * (tx.T@(z - y))\n",
    "    return grad\n",
    "\n",
    "def compute_reg_log_loss(y, tx, w, lambda_):\n",
    "    '''LOGISTIC LOSS WITH REGULARIZATION'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm))) + ((lambda_/2)*np.sum(w.T@w)))\n",
    "    \n",
    "    return loss\n",
    "def compute_reg_log_gradient(y, tx, w, lambda_):\n",
    "    '''GRADIENT COMPUTATION FOR LR WITH REGULARIZATION'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * ((tx.T@(z - y)) + (lambda_*w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.inv((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]))@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_log_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_log_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad = compute_log_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT WITH REGULARIZATION'''\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_reg_log_loss(y, tx, w, lambda_)\n",
    "        grad = compute_reg_log_gradient(y, tx, w, lambda_)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_reg_log_loss(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            grad = compute__reg_log_gradient(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/wifinaynay/downloads/data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_vhstack_dispatcher() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-30c544b0bee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0my_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_val_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_val_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_val_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_val_cat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildDataModel_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX_old\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-e00d20d2944c>\u001b[0m in \u001b[0;36mBuildDataModel_Train\u001b[0;34m(y, tX_old, ids)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBuildDataModel_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mtX_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRemoveFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0mtX_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAddFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0my_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorize_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-e00d20d2944c>\u001b[0m in \u001b[0;36mRemoveFeatures\u001b[0;34m(tX, ratio)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRemoveFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mtX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _vhstack_dispatcher() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "(y_cat, tX_cat, ids_cat, ind_cat), (y_val_cat, tX_val_cat, ids_val_cat, ind_val_cat) = BuildDataModel_Train(y,tX_old,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "[nan nan nan ... nan nan nan]\n",
      "Accuracy of Batch GD: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "init_w_gd = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1])))\n",
    "gd_tr_pred = np.copy((y_val_cat))\n",
    "w_gd = np.zeros((init_w_gd.shape[0], init_w_gd.shape[1]))\n",
    "max_iter, epochs, gamma, lambda_ = HyperParameters()\n",
    "cat_lst = [0, 1, 2, 3]\n",
    "for cat in cat_lst:\n",
    "    #res = ForwardSelection(y_cat[cat], tX_cat[cat], tX_val_cat[cat], y_val_cat[cat])\n",
    "    #print(\"after forward:\",res[0], res[1])\n",
    "    #print(\"debugg\", init_w_gd[cat][res[0]].shape)\n",
    "    (ww2,loss1) = ridge_regression(y_cat[cat], tX_cat[cat], 0.05)\n",
    "    #print(ww2.shape, tX_val_cat[cat][:,res[0]].shape )\n",
    "    \n",
    "    gd_tr_pred[cat] = predict_labels(ww2, tX_val_cat[cat])\n",
    "    print(gd_tr_pred[cat])\n",
    "#pred = Decategorize(gd_tr_pred, ind_val_cat)\n",
    "acc = WeightedAverage(gd_tr_pred, y_val_cat)\n",
    "print(\"Accuracy of Batch GD:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Batch GD: 0.5302824225938075\n"
     ]
    }
   ],
   "source": [
    "'''STOCHASTIC GD'''\n",
    "init_w_sgd = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1])))\n",
    "gd_tr_pred = np.copy((y_val_cat))\n",
    "w_sgd = np.zeros((init_w_sgd.shape[0], init_w_sgd.shape[1]))\n",
    "max_iter, epochs, gamma, lambda_ = HyperParameters()\n",
    "cat_lst = [0, 1, 2, 3]\n",
    "for cat in cat_lst:\n",
    "    (w_sgd[cat],loss2) = least_squares_SGD(y_cat[cat], tX_cat[cat], init_w_gd[cat], max_iter, gamma)\n",
    "    sgd_tr_pred[cat] = predict_labels(w_gd[cat], tX_val_cat[cat])\n",
    "#pred = Decategorize(gd_tr_pred, ind_val_cat)\n",
    "acc = WeightedAverage(sgd_tr_pred, y_val_cat)\n",
    "print(\"Accuracy of Stochastic GD:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-141760846786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mls_tr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw_lsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_w_gd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_w_gd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mw_lsn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mls_tr_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_lsn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_val_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeightedAverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_tr_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-1df950849d9e>\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mw_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_ls_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/hw3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DD->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'dd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/hw3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "ls_tr_pred = np.copy((y_val_cat))\n",
    "w_lsn = np.zeros((init_w_gd.shape[0], init_w_gd.shape[1]))\n",
    "(w_lsn[cat],loss3) = least_squares(y_cat[cat], tX_cat[cat])\n",
    "ls_tr_pred[cat] = predict_labels(w_lsn[cat], tX_val_cat[cat])\n",
    "acc = WeightedAverage(ls_tr_pred, y_val_cat)\n",
    "print(\"Accuracy of Least Squares with Normal Equations\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "count = 3\n",
    "lambda_ = np.logspace(-1, -10, 50)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        best_lambda[count] = lambda_[i]\n",
    "        min_loss = loss4\n",
    "(w4,loss4) = ridge_regression(y, tX, best_lambda[count])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.array((InitWeights(),InitWeights(),InitWeights(),InitWeights()))\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOGISTIC REGRESSION WITH (STOCHASTIC) GRADIENT DESCENT'''\n",
    "init_w_gd = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[0].shape[1])))\n",
    "gd_tr_pred = np.copy((y_val_cat))\n",
    "w_gd = np.zeros((init_w_gd.shape[0], init_w_gd.shape[1]))\n",
    "max_iter, epochs, gamma, lambda_ = HyperParameters()\n",
    "cat_lst = [0, 1, 2, 3]\n",
    "for cat in cat_lst:\n",
    "    (w_gd[cat],loss1) = least_squares_GD(y_cat[cat], tX_cat[cat], init_w_gd[cat], max_iter, gamma)\n",
    "    gd_tr_pred[cat] = predict_labels(w_gd[cat], tX_val_cat[cat])\n",
    "#pred = Decategorize(gd_tr_pred, ind_val_cat)\n",
    "acc = WeightedAverage(gd_tr_pred, y_val_cat)\n",
    "print(\"Accuracy of Batch GD:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.array((InitWeights(),InitWeights(),InitWeights(),InitWeights()))\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOGISTIC REGRESSION WITH (STOCHASTIC) GRADIENT DESCENT + REGULARIZATION'''\n",
    "'''FOR OPTIMAL PARAMETERS: TAKES SOME TIME TO TRAIN COMPLETELY'''\n",
    "'''TO ENABLE LAMBDA ITERATION: UNCOMMENT LAMBDA_2 LINES + CHANGE best_lambda[3] TO lambda_2[n_lambda]'''\n",
    "'''OTHERWISE, LAMBDA CHOSEN FOR RIDGE REGRESSION WILL BE USED'''\n",
    "count = 5\n",
    "ind2 = 0\n",
    "min_loss2 = 1000000\n",
    "#lambda_2 = np.logspace(-1, -6, 30)\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        #for n_lambda in range(lambda_2.shape[0]):\n",
    "        (w6,loss6) = reg_logistic_regression(y, tX, best_lambda[3], init_w, n_iter, n_gamma)\n",
    "        log_tr_pred = predict_labels(w6, tX_valid)\n",
    "        res = (log_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w6\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "            #best_lambda[count] = lambda_2[n_lambda]\n",
    "print(\"Parameters for best accuracy in LR with regularization: \", best_res[count], \" are gamma:= \",\\\n",
    "      best_gamma[count], \", iteration number:=\", best_iter[count], \" & lambda:\", best_lambda[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/wifinaynay/downloads/data_project1/lr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(best_grad[0], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
