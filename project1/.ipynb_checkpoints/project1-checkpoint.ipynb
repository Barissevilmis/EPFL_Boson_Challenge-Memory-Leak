{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "            \n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples: \", tX, \" & shape: \")\n",
    "print(\"Targets: \", y)\n",
    "print(\"Ids: \",ids)\n",
    "print(\"Shapes of tX, y & Ids: \", tX.shape, y.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size:  25000\n",
      "Shapes of tX, y & Ids for Training:  (25000, 30) (25000,) (25000,)\n",
      "Shapes of tX, y & Ids for Validation:  (225000, 30) (225000,) (225000,)\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data\n",
    "tX, tX_mean, tX_std = standardize(tX)\n",
    "\n",
    "train_valid_split = int(tX.shape[0] / 10)\n",
    "print(\"Validation data size: \", train_valid_split)\n",
    "tX_valid = tX[train_valid_split:,:]\n",
    "y_valid = y[train_valid_split:]\n",
    "id_valid = ids[train_valid_split:]\n",
    "\n",
    "tX = tX[:train_valid_split]\n",
    "y = y[:train_valid_split]\n",
    "ids = ids[:train_valid_split]\n",
    "\n",
    "print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, id_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''typ = <LOSS_TYPE>(CAPITAL LETTERS)'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(np.square(w)))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx):\n",
    "    return 1 / (1 + np.exp(-tx))\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0\n",
    "    sigm = sigmoid(tx)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(np.log(hx).T * y - (np.log(1 - hx).T * (1-y))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_loss(y, tx, w_star, \"MSE\")\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.solve((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]), tx.T@y)\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "#Pre-process\n",
    "tX_test, tX_test_mean, tX_test_std = standardize(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "[0.90005632 0.89003445 0.13229146 0.34351175 0.39691614 0.74837181\n",
      " 0.52214846 0.24522845 0.62121985 0.09057787 0.92747648 0.1966588\n",
      " 0.62592623 0.06981853 0.65645942 0.02013562 0.52883783 0.38868988\n",
      " 0.1995837  0.32664805 0.38360278 0.26608571 0.58172377 0.44113436\n",
      " 0.94304921 0.77617273 0.4856734  0.36996948 0.53185935 0.64121021]\n"
     ]
    }
   ],
   "source": [
    "ww = np.random.rand(tX.shape[1])\n",
    "init_w = np.array(ww, dtype=np.float64)\n",
    "#init_w = np.zeros(tX.shape[1])\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 500\n",
    "alpha = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=28.886864455035116\n",
      "Gradient Descent(1/499): loss=28.886232744128954\n",
      "Gradient Descent(2/499): loss=28.88560104852366\n",
      "Gradient Descent(3/499): loss=28.884969368218847\n",
      "Gradient Descent(4/499): loss=28.884337703214154\n",
      "Gradient Descent(5/499): loss=28.88370605350921\n",
      "Gradient Descent(6/499): loss=28.88307441910363\n",
      "Gradient Descent(7/499): loss=28.882442799997044\n",
      "Gradient Descent(8/499): loss=28.881811196189066\n",
      "Gradient Descent(9/499): loss=28.881179607679325\n",
      "Gradient Descent(10/499): loss=28.88054803446745\n",
      "Gradient Descent(11/499): loss=28.87991647655306\n",
      "Gradient Descent(12/499): loss=28.879284933935782\n",
      "Gradient Descent(13/499): loss=28.878653406615246\n",
      "Gradient Descent(14/499): loss=28.87802189459107\n",
      "Gradient Descent(15/499): loss=28.877390397862875\n",
      "Gradient Descent(16/499): loss=28.87675891643029\n",
      "Gradient Descent(17/499): loss=28.876127450292937\n",
      "Gradient Descent(18/499): loss=28.875495999450443\n",
      "Gradient Descent(19/499): loss=28.874864563902438\n",
      "Gradient Descent(20/499): loss=28.874233143648535\n",
      "Gradient Descent(21/499): loss=28.87360173868836\n",
      "Gradient Descent(22/499): loss=28.872970349021553\n",
      "Gradient Descent(23/499): loss=28.872338974647715\n",
      "Gradient Descent(24/499): loss=28.8717076155665\n",
      "Gradient Descent(25/499): loss=28.871076271777497\n",
      "Gradient Descent(26/499): loss=28.870444943280358\n",
      "Gradient Descent(27/499): loss=28.86981363007469\n",
      "Gradient Descent(28/499): loss=28.86918233216014\n",
      "Gradient Descent(29/499): loss=28.868551049536308\n",
      "Gradient Descent(30/499): loss=28.867919782202833\n",
      "Gradient Descent(31/499): loss=28.867288530159325\n",
      "Gradient Descent(32/499): loss=28.866657293405428\n",
      "Gradient Descent(33/499): loss=28.86602607194076\n",
      "Gradient Descent(34/499): loss=28.86539486576494\n",
      "Gradient Descent(35/499): loss=28.864763674877597\n",
      "Gradient Descent(36/499): loss=28.864132499278348\n",
      "Gradient Descent(37/499): loss=28.86350133896683\n",
      "Gradient Descent(38/499): loss=28.862870193942666\n",
      "Gradient Descent(39/499): loss=28.86223906420547\n",
      "Gradient Descent(40/499): loss=28.861607949754877\n",
      "Gradient Descent(41/499): loss=28.860976850590507\n",
      "Gradient Descent(42/499): loss=28.860345766711983\n",
      "Gradient Descent(43/499): loss=28.859714698118932\n",
      "Gradient Descent(44/499): loss=28.859083644810976\n",
      "Gradient Descent(45/499): loss=28.858452606787754\n",
      "Gradient Descent(46/499): loss=28.857821584048875\n",
      "Gradient Descent(47/499): loss=28.85719057659397\n",
      "Gradient Descent(48/499): loss=28.856559584422655\n",
      "Gradient Descent(49/499): loss=28.855928607534565\n",
      "Gradient Descent(50/499): loss=28.85529764592932\n",
      "Gradient Descent(51/499): loss=28.854666699606547\n",
      "Gradient Descent(52/499): loss=28.85403576856587\n",
      "Gradient Descent(53/499): loss=28.853404852806914\n",
      "Gradient Descent(54/499): loss=28.8527739523293\n",
      "Gradient Descent(55/499): loss=28.852143067132666\n",
      "Gradient Descent(56/499): loss=28.851512197216618\n",
      "Gradient Descent(57/499): loss=28.850881342580795\n",
      "Gradient Descent(58/499): loss=28.850250503224817\n",
      "Gradient Descent(59/499): loss=28.8496196791483\n",
      "Gradient Descent(60/499): loss=28.848988870350887\n",
      "Gradient Descent(61/499): loss=28.848358076832195\n",
      "Gradient Descent(62/499): loss=28.847727298591842\n",
      "Gradient Descent(63/499): loss=28.84709653562946\n",
      "Gradient Descent(64/499): loss=28.84646578794467\n",
      "Gradient Descent(65/499): loss=28.845835055537105\n",
      "Gradient Descent(66/499): loss=28.845204338406372\n",
      "Gradient Descent(67/499): loss=28.84457363655212\n",
      "Gradient Descent(68/499): loss=28.84394294997396\n",
      "Gradient Descent(69/499): loss=28.843312278671515\n",
      "Gradient Descent(70/499): loss=28.842681622644413\n",
      "Gradient Descent(71/499): loss=28.842050981892285\n",
      "Gradient Descent(72/499): loss=28.841420356414744\n",
      "Gradient Descent(73/499): loss=28.840789746211424\n",
      "Gradient Descent(74/499): loss=28.84015915128195\n",
      "Gradient Descent(75/499): loss=28.83952857162594\n",
      "Gradient Descent(76/499): loss=28.838898007243028\n",
      "Gradient Descent(77/499): loss=28.838267458132837\n",
      "Gradient Descent(78/499): loss=28.83763692429498\n",
      "Gradient Descent(79/499): loss=28.837006405729095\n",
      "Gradient Descent(80/499): loss=28.836375902434803\n",
      "Gradient Descent(81/499): loss=28.835745414411736\n",
      "Gradient Descent(82/499): loss=28.835114941659512\n",
      "Gradient Descent(83/499): loss=28.83448448417775\n",
      "Gradient Descent(84/499): loss=28.83385404196609\n",
      "Gradient Descent(85/499): loss=28.833223615024142\n",
      "Gradient Descent(86/499): loss=28.832593203351543\n",
      "Gradient Descent(87/499): loss=28.831962806947914\n",
      "Gradient Descent(88/499): loss=28.83133242581287\n",
      "Gradient Descent(89/499): loss=28.83070205994606\n",
      "Gradient Descent(90/499): loss=28.830071709347077\n",
      "Gradient Descent(91/499): loss=28.829441374015577\n",
      "Gradient Descent(92/499): loss=28.828811053951167\n",
      "Gradient Descent(93/499): loss=28.82818074915348\n",
      "Gradient Descent(94/499): loss=28.827550459622135\n",
      "Gradient Descent(95/499): loss=28.826920185356762\n",
      "Gradient Descent(96/499): loss=28.82628992635698\n",
      "Gradient Descent(97/499): loss=28.82565968262243\n",
      "Gradient Descent(98/499): loss=28.825029454152716\n",
      "Gradient Descent(99/499): loss=28.82439924094748\n",
      "Gradient Descent(100/499): loss=28.823769043006337\n",
      "Gradient Descent(101/499): loss=28.82370602439516\n",
      "Gradient Descent(102/499): loss=28.823643005936628\n",
      "Gradient Descent(103/499): loss=28.823579987630726\n",
      "Gradient Descent(104/499): loss=28.823516969477456\n",
      "Gradient Descent(105/499): loss=28.823453951476825\n",
      "Gradient Descent(106/499): loss=28.823390933628826\n",
      "Gradient Descent(107/499): loss=28.823327915933454\n",
      "Gradient Descent(108/499): loss=28.823264898390725\n",
      "Gradient Descent(109/499): loss=28.82320188100062\n",
      "Gradient Descent(110/499): loss=28.823138863763155\n",
      "Gradient Descent(111/499): loss=28.823075846678318\n",
      "Gradient Descent(112/499): loss=28.823012829746112\n",
      "Gradient Descent(113/499): loss=28.82294981296654\n",
      "Gradient Descent(114/499): loss=28.822886796339596\n",
      "Gradient Descent(115/499): loss=28.822823779865278\n",
      "Gradient Descent(116/499): loss=28.8227607635436\n",
      "Gradient Descent(117/499): loss=28.822697747374544\n",
      "Gradient Descent(118/499): loss=28.82263473135812\n",
      "Gradient Descent(119/499): loss=28.822571715494323\n",
      "Gradient Descent(120/499): loss=28.822508699783153\n",
      "Gradient Descent(121/499): loss=28.822445684224622\n",
      "Gradient Descent(122/499): loss=28.822382668818708\n",
      "Gradient Descent(123/499): loss=28.822319653565415\n",
      "Gradient Descent(124/499): loss=28.822256638464758\n",
      "Gradient Descent(125/499): loss=28.82219362351673\n",
      "Gradient Descent(126/499): loss=28.822130608721324\n",
      "Gradient Descent(127/499): loss=28.822067594078543\n",
      "Gradient Descent(128/499): loss=28.82200457958839\n",
      "Gradient Descent(129/499): loss=28.82194156525086\n",
      "Gradient Descent(130/499): loss=28.82187855106596\n",
      "Gradient Descent(131/499): loss=28.821815537033675\n",
      "Gradient Descent(132/499): loss=28.82175252315402\n",
      "Gradient Descent(133/499): loss=28.821689509426985\n",
      "Gradient Descent(134/499): loss=28.82162649585258\n",
      "Gradient Descent(135/499): loss=28.82156348243079\n",
      "Gradient Descent(136/499): loss=28.821500469161624\n",
      "Gradient Descent(137/499): loss=28.821437456045082\n",
      "Gradient Descent(138/499): loss=28.821374443081165\n",
      "Gradient Descent(139/499): loss=28.82131143026987\n",
      "Gradient Descent(140/499): loss=28.821248417611184\n",
      "Gradient Descent(141/499): loss=28.821185405105126\n",
      "Gradient Descent(142/499): loss=28.82112239275169\n",
      "Gradient Descent(143/499): loss=28.82105938055087\n",
      "Gradient Descent(144/499): loss=28.820996368502673\n",
      "Gradient Descent(145/499): loss=28.820933356607092\n",
      "Gradient Descent(146/499): loss=28.820870344864133\n",
      "Gradient Descent(147/499): loss=28.820807333273788\n",
      "Gradient Descent(148/499): loss=28.820744321836067\n",
      "Gradient Descent(149/499): loss=28.820681310550963\n",
      "Gradient Descent(150/499): loss=28.82061829941847\n",
      "Gradient Descent(151/499): loss=28.820555288438594\n",
      "Gradient Descent(152/499): loss=28.820492277611343\n",
      "Gradient Descent(153/499): loss=28.8204292669367\n",
      "Gradient Descent(154/499): loss=28.820366256414673\n",
      "Gradient Descent(155/499): loss=28.820303246045263\n",
      "Gradient Descent(156/499): loss=28.82024023582847\n",
      "Gradient Descent(157/499): loss=28.820177225764283\n",
      "Gradient Descent(158/499): loss=28.820114215852723\n",
      "Gradient Descent(159/499): loss=28.82005120609377\n",
      "Gradient Descent(160/499): loss=28.81998819648743\n",
      "Gradient Descent(161/499): loss=28.819925187033707\n",
      "Gradient Descent(162/499): loss=28.81986217773259\n",
      "Gradient Descent(163/499): loss=28.819799168584094\n",
      "Gradient Descent(164/499): loss=28.819736159588203\n",
      "Gradient Descent(165/499): loss=28.819673150744926\n",
      "Gradient Descent(166/499): loss=28.819610142054263\n",
      "Gradient Descent(167/499): loss=28.819547133516203\n",
      "Gradient Descent(168/499): loss=28.819484125130757\n",
      "Gradient Descent(169/499): loss=28.819421116897928\n",
      "Gradient Descent(170/499): loss=28.819358108817696\n",
      "Gradient Descent(171/499): loss=28.819295100890084\n",
      "Gradient Descent(172/499): loss=28.819232093115073\n",
      "Gradient Descent(173/499): loss=28.819169085492682\n",
      "Gradient Descent(174/499): loss=28.819106078022887\n",
      "Gradient Descent(175/499): loss=28.819043070705707\n",
      "Gradient Descent(176/499): loss=28.818980063541133\n",
      "Gradient Descent(177/499): loss=28.818917056529166\n",
      "Gradient Descent(178/499): loss=28.818854049669806\n",
      "Gradient Descent(179/499): loss=28.81879104296305\n",
      "Gradient Descent(180/499): loss=28.818728036408896\n",
      "Gradient Descent(181/499): loss=28.818665030007356\n",
      "Gradient Descent(182/499): loss=28.81860202375842\n",
      "Gradient Descent(183/499): loss=28.818539017662083\n",
      "Gradient Descent(184/499): loss=28.818476011718356\n",
      "Gradient Descent(185/499): loss=28.818413005927237\n",
      "Gradient Descent(186/499): loss=28.818350000288714\n",
      "Gradient Descent(187/499): loss=28.818286994802797\n",
      "Gradient Descent(188/499): loss=28.81822398946948\n",
      "Gradient Descent(189/499): loss=28.818160984288763\n",
      "Gradient Descent(190/499): loss=28.818097979260656\n",
      "Gradient Descent(191/499): loss=28.81803497438515\n",
      "Gradient Descent(192/499): loss=28.817971969662235\n",
      "Gradient Descent(193/499): loss=28.817908965091934\n",
      "Gradient Descent(194/499): loss=28.817845960674234\n",
      "Gradient Descent(195/499): loss=28.817782956409125\n",
      "Gradient Descent(196/499): loss=28.817719952296617\n",
      "Gradient Descent(197/499): loss=28.817656948336715\n",
      "Gradient Descent(198/499): loss=28.817593944529403\n",
      "Gradient Descent(199/499): loss=28.8175309408747\n",
      "Gradient Descent(200/499): loss=28.81746793737259\n",
      "Gradient Descent(201/499): loss=28.817461637034203\n",
      "Gradient Descent(202/499): loss=28.81745533669735\n",
      "Gradient Descent(203/499): loss=28.817449036362014\n",
      "Gradient Descent(204/499): loss=28.817442736028205\n",
      "Gradient Descent(205/499): loss=28.817436435695925\n",
      "Gradient Descent(206/499): loss=28.817430135365168\n",
      "Gradient Descent(207/499): loss=28.817423835035942\n",
      "Gradient Descent(208/499): loss=28.817417534708245\n",
      "Gradient Descent(209/499): loss=28.817411234382067\n",
      "Gradient Descent(210/499): loss=28.817404934057418\n",
      "Gradient Descent(211/499): loss=28.817398633734292\n",
      "Gradient Descent(212/499): loss=28.81739233341269\n",
      "Gradient Descent(213/499): loss=28.817386033092614\n",
      "Gradient Descent(214/499): loss=28.81737973277407\n",
      "Gradient Descent(215/499): loss=28.81737343245705\n",
      "Gradient Descent(216/499): loss=28.817367132141552\n",
      "Gradient Descent(217/499): loss=28.817360831827585\n",
      "Gradient Descent(218/499): loss=28.81735453151514\n",
      "Gradient Descent(219/499): loss=28.817348231204225\n",
      "Gradient Descent(220/499): loss=28.817341930894834\n",
      "Gradient Descent(221/499): loss=28.817335630586967\n",
      "Gradient Descent(222/499): loss=28.817329330280632\n",
      "Gradient Descent(223/499): loss=28.817323029975814\n",
      "Gradient Descent(224/499): loss=28.81731672967253\n",
      "Gradient Descent(225/499): loss=28.817310429370767\n",
      "Gradient Descent(226/499): loss=28.817304129070536\n",
      "Gradient Descent(227/499): loss=28.817297828771817\n",
      "Gradient Descent(228/499): loss=28.81729152847464\n",
      "Gradient Descent(229/499): loss=28.81728522817898\n",
      "Gradient Descent(230/499): loss=28.81727892788485\n",
      "Gradient Descent(231/499): loss=28.817272627592242\n",
      "Gradient Descent(232/499): loss=28.817266327301162\n",
      "Gradient Descent(233/499): loss=28.817260027011617\n",
      "Gradient Descent(234/499): loss=28.81725372672358\n",
      "Gradient Descent(235/499): loss=28.817247426437078\n",
      "Gradient Descent(236/499): loss=28.81724112615211\n",
      "Gradient Descent(237/499): loss=28.81723482586866\n",
      "Gradient Descent(238/499): loss=28.817228525586735\n",
      "Gradient Descent(239/499): loss=28.817222225306338\n",
      "Gradient Descent(240/499): loss=28.817215925027465\n",
      "Gradient Descent(241/499): loss=28.817209624750124\n",
      "Gradient Descent(242/499): loss=28.817203324474303\n",
      "Gradient Descent(243/499): loss=28.817197024200006\n",
      "Gradient Descent(244/499): loss=28.817190723927236\n",
      "Gradient Descent(245/499): loss=28.817184423655995\n",
      "Gradient Descent(246/499): loss=28.81717812338628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(247/499): loss=28.817171823118088\n",
      "Gradient Descent(248/499): loss=28.81716552285143\n",
      "Gradient Descent(249/499): loss=28.817159222586287\n",
      "Gradient Descent(250/499): loss=28.817152922322673\n",
      "Gradient Descent(251/499): loss=28.81714662206059\n",
      "Gradient Descent(252/499): loss=28.817140321800032\n",
      "Gradient Descent(253/499): loss=28.817134021540994\n",
      "Gradient Descent(254/499): loss=28.81712772128349\n",
      "Gradient Descent(255/499): loss=28.817121421027508\n",
      "Gradient Descent(256/499): loss=28.817115120773046\n",
      "Gradient Descent(257/499): loss=28.817108820520126\n",
      "Gradient Descent(258/499): loss=28.817102520268712\n",
      "Gradient Descent(259/499): loss=28.81709622001884\n",
      "Gradient Descent(260/499): loss=28.817089919770485\n",
      "Gradient Descent(261/499): loss=28.81708361952366\n",
      "Gradient Descent(262/499): loss=28.817077319278358\n",
      "Gradient Descent(263/499): loss=28.817071019034582\n",
      "Gradient Descent(264/499): loss=28.817064718792334\n",
      "Gradient Descent(265/499): loss=28.817058418551607\n",
      "Gradient Descent(266/499): loss=28.817052118312414\n",
      "Gradient Descent(267/499): loss=28.817045818074742\n",
      "Gradient Descent(268/499): loss=28.8170395178386\n",
      "Gradient Descent(269/499): loss=28.81703321760398\n",
      "Gradient Descent(270/499): loss=28.817026917370885\n",
      "Gradient Descent(271/499): loss=28.817020617139324\n",
      "Gradient Descent(272/499): loss=28.81701431690928\n",
      "Gradient Descent(273/499): loss=28.81700801668076\n",
      "Gradient Descent(274/499): loss=28.817001716453774\n",
      "Gradient Descent(275/499): loss=28.816995416228313\n",
      "Gradient Descent(276/499): loss=28.816989116004372\n",
      "Gradient Descent(277/499): loss=28.81698281578196\n",
      "Gradient Descent(278/499): loss=28.816976515561073\n",
      "Gradient Descent(279/499): loss=28.816970215341716\n",
      "Gradient Descent(280/499): loss=28.816963915123885\n",
      "Gradient Descent(281/499): loss=28.816957614907572\n",
      "Gradient Descent(282/499): loss=28.816951314692798\n",
      "Gradient Descent(283/499): loss=28.816945014479543\n",
      "Gradient Descent(284/499): loss=28.816938714267813\n",
      "Gradient Descent(285/499): loss=28.816932414057607\n",
      "Gradient Descent(286/499): loss=28.81692611384893\n",
      "Gradient Descent(287/499): loss=28.816919813641782\n",
      "Gradient Descent(288/499): loss=28.816913513436152\n",
      "Gradient Descent(289/499): loss=28.81690721323205\n",
      "Gradient Descent(290/499): loss=28.816900913029475\n",
      "Gradient Descent(291/499): loss=28.81689461282843\n",
      "Gradient Descent(292/499): loss=28.816888312628908\n",
      "Gradient Descent(293/499): loss=28.816882012430913\n",
      "Gradient Descent(294/499): loss=28.816875712234445\n",
      "Gradient Descent(295/499): loss=28.816869412039498\n",
      "Gradient Descent(296/499): loss=28.816863111846082\n",
      "Gradient Descent(297/499): loss=28.816856811654187\n",
      "Gradient Descent(298/499): loss=28.816850511463826\n",
      "Gradient Descent(299/499): loss=28.81684421127498\n",
      "Gradient Descent(300/499): loss=28.816837911087667\n",
      "Gradient Descent(301/499): loss=28.816837281069056\n",
      "Gradient Descent(302/499): loss=28.816836651050455\n",
      "Gradient Descent(303/499): loss=28.816836021031875\n",
      "Gradient Descent(304/499): loss=28.816835391013303\n",
      "Gradient Descent(305/499): loss=28.816834760994755\n",
      "Gradient Descent(306/499): loss=28.816834130976215\n",
      "Gradient Descent(307/499): loss=28.816833500957692\n",
      "Gradient Descent(308/499): loss=28.816832870939184\n",
      "Gradient Descent(309/499): loss=28.816832240920693\n",
      "Gradient Descent(310/499): loss=28.81683161090222\n",
      "Gradient Descent(311/499): loss=28.816830980883754\n",
      "Gradient Descent(312/499): loss=28.816830350865313\n",
      "Gradient Descent(313/499): loss=28.81682972084688\n",
      "Gradient Descent(314/499): loss=28.81682909082847\n",
      "Gradient Descent(315/499): loss=28.816828460810072\n",
      "Gradient Descent(316/499): loss=28.81682783079168\n",
      "Gradient Descent(317/499): loss=28.816827200773318\n",
      "Gradient Descent(318/499): loss=28.816826570754962\n",
      "Gradient Descent(319/499): loss=28.816825940736624\n",
      "Gradient Descent(320/499): loss=28.816825310718304\n",
      "Gradient Descent(321/499): loss=28.81682468069999\n",
      "Gradient Descent(322/499): loss=28.8168240506817\n",
      "Gradient Descent(323/499): loss=28.816823420663418\n",
      "Gradient Descent(324/499): loss=28.816822790645155\n",
      "Gradient Descent(325/499): loss=28.816822160626913\n",
      "Gradient Descent(326/499): loss=28.816821530608678\n",
      "Gradient Descent(327/499): loss=28.816820900590464\n",
      "Gradient Descent(328/499): loss=28.816820270572265\n",
      "Gradient Descent(329/499): loss=28.81681964055408\n",
      "Gradient Descent(330/499): loss=28.816819010535905\n",
      "Gradient Descent(331/499): loss=28.816818380517752\n",
      "Gradient Descent(332/499): loss=28.816817750499613\n",
      "Gradient Descent(333/499): loss=28.816817120481485\n",
      "Gradient Descent(334/499): loss=28.816816490463374\n",
      "Gradient Descent(335/499): loss=28.816815860445278\n",
      "Gradient Descent(336/499): loss=28.8168152304272\n",
      "Gradient Descent(337/499): loss=28.81681460040914\n",
      "Gradient Descent(338/499): loss=28.816813970391088\n",
      "Gradient Descent(339/499): loss=28.816813340373052\n",
      "Gradient Descent(340/499): loss=28.816812710355038\n",
      "Gradient Descent(341/499): loss=28.81681208033704\n",
      "Gradient Descent(342/499): loss=28.81681145031905\n",
      "Gradient Descent(343/499): loss=28.816810820301075\n",
      "Gradient Descent(344/499): loss=28.816810190283125\n",
      "Gradient Descent(345/499): loss=28.81680956026517\n",
      "Gradient Descent(346/499): loss=28.81680893024725\n",
      "Gradient Descent(347/499): loss=28.816808300229336\n",
      "Gradient Descent(348/499): loss=28.816807670211446\n",
      "Gradient Descent(349/499): loss=28.816807040193563\n",
      "Gradient Descent(350/499): loss=28.816806410175694\n",
      "Gradient Descent(351/499): loss=28.816805780157846\n",
      "Gradient Descent(352/499): loss=28.816805150140013\n",
      "Gradient Descent(353/499): loss=28.816804520122194\n",
      "Gradient Descent(354/499): loss=28.81680389010439\n",
      "Gradient Descent(355/499): loss=28.8168032600866\n",
      "Gradient Descent(356/499): loss=28.816802630068825\n",
      "Gradient Descent(357/499): loss=28.81680200005107\n",
      "Gradient Descent(358/499): loss=28.81680137003332\n",
      "Gradient Descent(359/499): loss=28.816800740015594\n",
      "Gradient Descent(360/499): loss=28.81680010999788\n",
      "Gradient Descent(361/499): loss=28.816799479980187\n",
      "Gradient Descent(362/499): loss=28.8167988499625\n",
      "Gradient Descent(363/499): loss=28.816798219944836\n",
      "Gradient Descent(364/499): loss=28.816797589927184\n",
      "Gradient Descent(365/499): loss=28.81679695990955\n",
      "Gradient Descent(366/499): loss=28.816796329891925\n",
      "Gradient Descent(367/499): loss=28.81679569987432\n",
      "Gradient Descent(368/499): loss=28.816795069856735\n",
      "Gradient Descent(369/499): loss=28.816794439839153\n",
      "Gradient Descent(370/499): loss=28.81679380982159\n",
      "Gradient Descent(371/499): loss=28.816793179804048\n",
      "Gradient Descent(372/499): loss=28.81679254978652\n",
      "Gradient Descent(373/499): loss=28.816791919769\n",
      "Gradient Descent(374/499): loss=28.816791289751503\n",
      "Gradient Descent(375/499): loss=28.816790659734018\n",
      "Gradient Descent(376/499): loss=28.816790029716554\n",
      "Gradient Descent(377/499): loss=28.816789399699097\n",
      "Gradient Descent(378/499): loss=28.81678876968166\n",
      "Gradient Descent(379/499): loss=28.816788139664236\n",
      "Gradient Descent(380/499): loss=28.816787509646833\n",
      "Gradient Descent(381/499): loss=28.816786879629433\n",
      "Gradient Descent(382/499): loss=28.81678624961206\n",
      "Gradient Descent(383/499): loss=28.8167856195947\n",
      "Gradient Descent(384/499): loss=28.816784989577346\n",
      "Gradient Descent(385/499): loss=28.816784359560025\n",
      "Gradient Descent(386/499): loss=28.816783729542696\n",
      "Gradient Descent(387/499): loss=28.816783099525402\n",
      "Gradient Descent(388/499): loss=28.816782469508116\n",
      "Gradient Descent(389/499): loss=28.816781839490847\n",
      "Gradient Descent(390/499): loss=28.816781209473593\n",
      "Gradient Descent(391/499): loss=28.81678057945635\n",
      "Gradient Descent(392/499): loss=28.816779949439127\n",
      "Gradient Descent(393/499): loss=28.816779319421915\n",
      "Gradient Descent(394/499): loss=28.816778689404725\n",
      "Gradient Descent(395/499): loss=28.81677805938754\n",
      "Gradient Descent(396/499): loss=28.816777429370376\n",
      "Gradient Descent(397/499): loss=28.81677679935323\n",
      "Gradient Descent(398/499): loss=28.816776169336094\n",
      "Gradient Descent(399/499): loss=28.816775539318982\n",
      "Gradient Descent(400/499): loss=28.816774909301877\n",
      "Gradient Descent(401/499): loss=28.81677484630017\n",
      "Gradient Descent(402/499): loss=28.81677478329846\n",
      "Gradient Descent(403/499): loss=28.816774720296745\n",
      "Gradient Descent(404/499): loss=28.816774657295035\n",
      "Gradient Descent(405/499): loss=28.81677459429333\n",
      "Gradient Descent(406/499): loss=28.816774531291617\n",
      "Gradient Descent(407/499): loss=28.816774468289914\n",
      "Gradient Descent(408/499): loss=28.81677440528821\n",
      "Gradient Descent(409/499): loss=28.816774342286497\n",
      "Gradient Descent(410/499): loss=28.816774279284793\n",
      "Gradient Descent(411/499): loss=28.816774216283086\n",
      "Gradient Descent(412/499): loss=28.81677415328138\n",
      "Gradient Descent(413/499): loss=28.816774090279672\n",
      "Gradient Descent(414/499): loss=28.816774027277965\n",
      "Gradient Descent(415/499): loss=28.816773964276262\n",
      "Gradient Descent(416/499): loss=28.81677390127455\n",
      "Gradient Descent(417/499): loss=28.816773838272848\n",
      "Gradient Descent(418/499): loss=28.81677377527114\n",
      "Gradient Descent(419/499): loss=28.816773712269427\n",
      "Gradient Descent(420/499): loss=28.81677364926773\n",
      "Gradient Descent(421/499): loss=28.81677358626602\n",
      "Gradient Descent(422/499): loss=28.816773523264317\n",
      "Gradient Descent(423/499): loss=28.81677346026261\n",
      "Gradient Descent(424/499): loss=28.816773397260896\n",
      "Gradient Descent(425/499): loss=28.816773334259196\n",
      "Gradient Descent(426/499): loss=28.816773271257492\n",
      "Gradient Descent(427/499): loss=28.81677320825579\n",
      "Gradient Descent(428/499): loss=28.816773145254086\n",
      "Gradient Descent(429/499): loss=28.81677308225238\n",
      "Gradient Descent(430/499): loss=28.816773019250675\n",
      "Gradient Descent(431/499): loss=28.81677295624897\n",
      "Gradient Descent(432/499): loss=28.81677289324727\n",
      "Gradient Descent(433/499): loss=28.816772830245558\n",
      "Gradient Descent(434/499): loss=28.816772767243858\n",
      "Gradient Descent(435/499): loss=28.816772704242148\n",
      "Gradient Descent(436/499): loss=28.816772641240444\n",
      "Gradient Descent(437/499): loss=28.816772578238744\n",
      "Gradient Descent(438/499): loss=28.816772515237037\n",
      "Gradient Descent(439/499): loss=28.816772452235327\n",
      "Gradient Descent(440/499): loss=28.81677238923363\n",
      "Gradient Descent(441/499): loss=28.816772326231927\n",
      "Gradient Descent(442/499): loss=28.816772263230227\n",
      "Gradient Descent(443/499): loss=28.816772200228524\n",
      "Gradient Descent(444/499): loss=28.816772137226817\n",
      "Gradient Descent(445/499): loss=28.816772074225113\n",
      "Gradient Descent(446/499): loss=28.81677201122341\n",
      "Gradient Descent(447/499): loss=28.816771948221707\n",
      "Gradient Descent(448/499): loss=28.816771885220007\n",
      "Gradient Descent(449/499): loss=28.8167718222183\n",
      "Gradient Descent(450/499): loss=28.8167717592166\n",
      "Gradient Descent(451/499): loss=28.816771696214897\n",
      "Gradient Descent(452/499): loss=28.816771633213193\n",
      "Gradient Descent(453/499): loss=28.816771570211497\n",
      "Gradient Descent(454/499): loss=28.816771507209797\n",
      "Gradient Descent(455/499): loss=28.816771444208094\n",
      "Gradient Descent(456/499): loss=28.816771381206394\n",
      "Gradient Descent(457/499): loss=28.816771318204694\n",
      "Gradient Descent(458/499): loss=28.81677125520299\n",
      "Gradient Descent(459/499): loss=28.81677119220129\n",
      "Gradient Descent(460/499): loss=28.81677112919959\n",
      "Gradient Descent(461/499): loss=28.816771066197887\n",
      "Gradient Descent(462/499): loss=28.81677100319619\n",
      "Gradient Descent(463/499): loss=28.816770940194488\n",
      "Gradient Descent(464/499): loss=28.81677087719279\n",
      "Gradient Descent(465/499): loss=28.81677081419109\n",
      "Gradient Descent(466/499): loss=28.81677075118939\n",
      "Gradient Descent(467/499): loss=28.81677068818769\n",
      "Gradient Descent(468/499): loss=28.816770625185992\n",
      "Gradient Descent(469/499): loss=28.816770562184292\n",
      "Gradient Descent(470/499): loss=28.816770499182596\n",
      "Gradient Descent(471/499): loss=28.816770436180896\n",
      "Gradient Descent(472/499): loss=28.8167703731792\n",
      "Gradient Descent(473/499): loss=28.816770310177503\n",
      "Gradient Descent(474/499): loss=28.816770247175803\n",
      "Gradient Descent(475/499): loss=28.816770184174104\n",
      "Gradient Descent(476/499): loss=28.816770121172414\n",
      "Gradient Descent(477/499): loss=28.816770058170714\n",
      "Gradient Descent(478/499): loss=28.816769995169018\n",
      "Gradient Descent(479/499): loss=28.81676993216732\n",
      "Gradient Descent(480/499): loss=28.816769869165622\n",
      "Gradient Descent(481/499): loss=28.81676980616393\n",
      "Gradient Descent(482/499): loss=28.816769743162222\n",
      "Gradient Descent(483/499): loss=28.816769680160533\n",
      "Gradient Descent(484/499): loss=28.816769617158833\n",
      "Gradient Descent(485/499): loss=28.816769554157137\n",
      "Gradient Descent(486/499): loss=28.816769491155448\n",
      "Gradient Descent(487/499): loss=28.81676942815375\n",
      "Gradient Descent(488/499): loss=28.81676936515206\n",
      "Gradient Descent(489/499): loss=28.816769302150362\n",
      "Gradient Descent(490/499): loss=28.816769239148666\n",
      "Gradient Descent(491/499): loss=28.816769176146973\n",
      "Gradient Descent(492/499): loss=28.816769113145277\n",
      "Gradient Descent(493/499): loss=28.81676905014358\n",
      "Gradient Descent(494/499): loss=28.816768987141888\n",
      "Gradient Descent(495/499): loss=28.816768924140195\n",
      "Gradient Descent(496/499): loss=28.816768861138502\n",
      "Gradient Descent(497/499): loss=28.816768798136806\n",
      "Gradient Descent(498/499): loss=28.816768735135117\n",
      "Gradient Descent(499/499): loss=28.816768672133417\n",
      "0.5866311111111111\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "(w1,loss1) = least_squares_GD(y, tX, init_w, max_iter, alpha)\n",
    "gd_tr_pred = predict_labels(w1, tX_valid)\n",
    "print((gd_tr_pred == y_valid).mean())\n",
    "gd_pred = predict_labels(w1, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/399): loss=26124267.57961738\n",
      "Stochastic Gradient Descent(1/399): loss=236393.57089246958\n",
      "Stochastic Gradient Descent(2/399): loss=235428.37692316354\n",
      "Stochastic Gradient Descent(3/399): loss=306518.22554784344\n",
      "Stochastic Gradient Descent(4/399): loss=122955.01948023823\n",
      "Stochastic Gradient Descent(5/399): loss=78672.24840316524\n",
      "Stochastic Gradient Descent(6/399): loss=307494.4617813148\n",
      "Stochastic Gradient Descent(7/399): loss=393310.5708922117\n",
      "Stochastic Gradient Descent(8/399): loss=85935.82027349809\n",
      "Stochastic Gradient Descent(9/399): loss=250049.58807462105\n",
      "Stochastic Gradient Descent(10/399): loss=319.6998117776728\n",
      "Stochastic Gradient Descent(11/399): loss=619.5944011272667\n",
      "Stochastic Gradient Descent(12/399): loss=3465.500642948747\n",
      "Stochastic Gradient Descent(13/399): loss=9274.10150972572\n",
      "Stochastic Gradient Descent(14/399): loss=13097.921205002787\n",
      "Stochastic Gradient Descent(15/399): loss=2891.3434233741823\n",
      "Stochastic Gradient Descent(16/399): loss=3492.99045680143\n",
      "Stochastic Gradient Descent(17/399): loss=78828.03475649901\n",
      "Stochastic Gradient Descent(18/399): loss=381574.48268705985\n",
      "Stochastic Gradient Descent(19/399): loss=21652.56884347897\n",
      "Stochastic Gradient Descent(20/399): loss=41174.2495100286\n",
      "Stochastic Gradient Descent(21/399): loss=1577.2166146471955\n",
      "Stochastic Gradient Descent(22/399): loss=27952.05242180696\n",
      "Stochastic Gradient Descent(23/399): loss=120.75118181047418\n",
      "Stochastic Gradient Descent(24/399): loss=37287.269783352174\n",
      "Stochastic Gradient Descent(25/399): loss=4519.539273608541\n",
      "Stochastic Gradient Descent(26/399): loss=245563.01710223983\n",
      "Stochastic Gradient Descent(27/399): loss=52932.961089503566\n",
      "Stochastic Gradient Descent(28/399): loss=3158.9898414570803\n",
      "Stochastic Gradient Descent(29/399): loss=29456.954099639588\n",
      "Stochastic Gradient Descent(30/399): loss=2252.5642751032224\n",
      "Stochastic Gradient Descent(31/399): loss=5119.133509264924\n",
      "Stochastic Gradient Descent(32/399): loss=10810.788421241066\n",
      "Stochastic Gradient Descent(33/399): loss=84425.67564471977\n",
      "Stochastic Gradient Descent(34/399): loss=114497.96714107646\n",
      "Stochastic Gradient Descent(35/399): loss=410.09571842349123\n",
      "Stochastic Gradient Descent(36/399): loss=4670.325404738414\n",
      "Stochastic Gradient Descent(37/399): loss=15072.017269515949\n",
      "Stochastic Gradient Descent(38/399): loss=4678.359683422609\n",
      "Stochastic Gradient Descent(39/399): loss=371931.86347430124\n",
      "Stochastic Gradient Descent(40/399): loss=9029.562481768657\n",
      "Stochastic Gradient Descent(41/399): loss=7869.014541975317\n",
      "Stochastic Gradient Descent(42/399): loss=25701.56335421835\n",
      "Stochastic Gradient Descent(43/399): loss=34181.93049483681\n",
      "Stochastic Gradient Descent(44/399): loss=9411.073022264767\n",
      "Stochastic Gradient Descent(45/399): loss=0.0017650330018008798\n",
      "Stochastic Gradient Descent(46/399): loss=87.97246256171726\n",
      "Stochastic Gradient Descent(47/399): loss=2.4153096495050232\n",
      "Stochastic Gradient Descent(48/399): loss=19049.679499011945\n",
      "Stochastic Gradient Descent(49/399): loss=1.163993644024036\n",
      "Stochastic Gradient Descent(50/399): loss=4454.9268047119585\n",
      "Stochastic Gradient Descent(51/399): loss=5676.184791716826\n",
      "Stochastic Gradient Descent(52/399): loss=1309.2383880628174\n",
      "Stochastic Gradient Descent(53/399): loss=1488.268804669196\n",
      "Stochastic Gradient Descent(54/399): loss=19028.428999458825\n",
      "Stochastic Gradient Descent(55/399): loss=27984.366728280467\n",
      "Stochastic Gradient Descent(56/399): loss=39418.22649918679\n",
      "Stochastic Gradient Descent(57/399): loss=196524.73926945333\n",
      "Stochastic Gradient Descent(58/399): loss=103019.69619379968\n",
      "Stochastic Gradient Descent(59/399): loss=8237.657742449106\n",
      "Stochastic Gradient Descent(60/399): loss=8065.372947098662\n",
      "Stochastic Gradient Descent(61/399): loss=256.48952825566477\n",
      "Stochastic Gradient Descent(62/399): loss=1940.3476027035856\n",
      "Stochastic Gradient Descent(63/399): loss=688.4078046465535\n",
      "Stochastic Gradient Descent(64/399): loss=43655.41354362447\n",
      "Stochastic Gradient Descent(65/399): loss=26233.897167206782\n",
      "Stochastic Gradient Descent(66/399): loss=1127.2952182545407\n",
      "Stochastic Gradient Descent(67/399): loss=7.928469113577603\n",
      "Stochastic Gradient Descent(68/399): loss=467.4270279720155\n",
      "Stochastic Gradient Descent(69/399): loss=3848.3205811171483\n",
      "Stochastic Gradient Descent(70/399): loss=1859.4296818283558\n",
      "Stochastic Gradient Descent(71/399): loss=3640.7434767180357\n",
      "Stochastic Gradient Descent(72/399): loss=43115.922957634146\n",
      "Stochastic Gradient Descent(73/399): loss=13938.380162905996\n",
      "Stochastic Gradient Descent(74/399): loss=6040.476187821569\n",
      "Stochastic Gradient Descent(75/399): loss=27077.71131667441\n",
      "Stochastic Gradient Descent(76/399): loss=64799.13635308775\n",
      "Stochastic Gradient Descent(77/399): loss=7282.0045069268945\n",
      "Stochastic Gradient Descent(78/399): loss=939.949975659414\n",
      "Stochastic Gradient Descent(79/399): loss=8488.826822094039\n",
      "Stochastic Gradient Descent(80/399): loss=5378.443606855701\n",
      "Stochastic Gradient Descent(81/399): loss=5497.95963473011\n",
      "Stochastic Gradient Descent(82/399): loss=4379.148777377998\n",
      "Stochastic Gradient Descent(83/399): loss=372.3950801609506\n",
      "Stochastic Gradient Descent(84/399): loss=9088.450076964387\n",
      "Stochastic Gradient Descent(85/399): loss=7244.518506685644\n",
      "Stochastic Gradient Descent(86/399): loss=860.3564920262327\n",
      "Stochastic Gradient Descent(87/399): loss=23002.9332553938\n",
      "Stochastic Gradient Descent(88/399): loss=65.48915372754941\n",
      "Stochastic Gradient Descent(89/399): loss=26833.261876852637\n",
      "Stochastic Gradient Descent(90/399): loss=75316.31125871761\n",
      "Stochastic Gradient Descent(91/399): loss=252.06640120233436\n",
      "Stochastic Gradient Descent(92/399): loss=19150.41308439193\n",
      "Stochastic Gradient Descent(93/399): loss=4017.261373531397\n",
      "Stochastic Gradient Descent(94/399): loss=12682.32818432808\n",
      "Stochastic Gradient Descent(95/399): loss=1286.1260894381337\n",
      "Stochastic Gradient Descent(96/399): loss=3081.9684916360734\n",
      "Stochastic Gradient Descent(97/399): loss=8986.636891539258\n",
      "Stochastic Gradient Descent(98/399): loss=3356.209513114775\n",
      "Stochastic Gradient Descent(99/399): loss=17.543815025178034\n",
      "Stochastic Gradient Descent(100/399): loss=198122.17833414068\n",
      "Stochastic Gradient Descent(101/399): loss=193.47568944518036\n",
      "Stochastic Gradient Descent(102/399): loss=4738.744235889855\n",
      "Stochastic Gradient Descent(103/399): loss=89.82671650662984\n",
      "Stochastic Gradient Descent(104/399): loss=18604.965652636307\n",
      "Stochastic Gradient Descent(105/399): loss=549.2997121295707\n",
      "Stochastic Gradient Descent(106/399): loss=1674.1847398466628\n",
      "Stochastic Gradient Descent(107/399): loss=25989.550038100828\n",
      "Stochastic Gradient Descent(108/399): loss=2322.6040471285164\n",
      "Stochastic Gradient Descent(109/399): loss=21449.645499711783\n",
      "Stochastic Gradient Descent(110/399): loss=67309.7572751626\n",
      "Stochastic Gradient Descent(111/399): loss=0.07212770357114964\n",
      "Stochastic Gradient Descent(112/399): loss=24174.498344729705\n",
      "Stochastic Gradient Descent(113/399): loss=0.3386959334516743\n",
      "Stochastic Gradient Descent(114/399): loss=7306.138867224137\n",
      "Stochastic Gradient Descent(115/399): loss=2758.3611410033222\n",
      "Stochastic Gradient Descent(116/399): loss=3367.973424863405\n",
      "Stochastic Gradient Descent(117/399): loss=248.37555070604162\n",
      "Stochastic Gradient Descent(118/399): loss=31683.384292666826\n",
      "Stochastic Gradient Descent(119/399): loss=1054.2859569772702\n",
      "Stochastic Gradient Descent(120/399): loss=2456.288138950575\n",
      "Stochastic Gradient Descent(121/399): loss=10786.05754076341\n",
      "Stochastic Gradient Descent(122/399): loss=57313.49059794536\n",
      "Stochastic Gradient Descent(123/399): loss=957.808788126664\n",
      "Stochastic Gradient Descent(124/399): loss=41628.98189683737\n",
      "Stochastic Gradient Descent(125/399): loss=644549.0400454414\n",
      "Stochastic Gradient Descent(126/399): loss=155882.9145443806\n",
      "Stochastic Gradient Descent(127/399): loss=63726.34383335316\n",
      "Stochastic Gradient Descent(128/399): loss=11301.663049785391\n",
      "Stochastic Gradient Descent(129/399): loss=45607.249839467215\n",
      "Stochastic Gradient Descent(130/399): loss=48232.956020122634\n",
      "Stochastic Gradient Descent(131/399): loss=11575.52939111147\n",
      "Stochastic Gradient Descent(132/399): loss=993.6061364276568\n",
      "Stochastic Gradient Descent(133/399): loss=9169.544056268578\n",
      "Stochastic Gradient Descent(134/399): loss=2210.499073166608\n",
      "Stochastic Gradient Descent(135/399): loss=9.563056523065741\n",
      "Stochastic Gradient Descent(136/399): loss=2177.851426600209\n",
      "Stochastic Gradient Descent(137/399): loss=60228.176885805406\n",
      "Stochastic Gradient Descent(138/399): loss=2131.9593975284315\n",
      "Stochastic Gradient Descent(139/399): loss=42180.882529346236\n",
      "Stochastic Gradient Descent(140/399): loss=53138.77933727854\n",
      "Stochastic Gradient Descent(141/399): loss=5232.606759192347\n",
      "Stochastic Gradient Descent(142/399): loss=477.65412653127896\n",
      "Stochastic Gradient Descent(143/399): loss=144.65726002218227\n",
      "Stochastic Gradient Descent(144/399): loss=21.128020639616285\n",
      "Stochastic Gradient Descent(145/399): loss=2253.5675383717453\n",
      "Stochastic Gradient Descent(146/399): loss=790.5762485245262\n",
      "Stochastic Gradient Descent(147/399): loss=5452.758885745053\n",
      "Stochastic Gradient Descent(148/399): loss=56.29609102186019\n",
      "Stochastic Gradient Descent(149/399): loss=298.64784225781455\n",
      "Stochastic Gradient Descent(150/399): loss=22236.28485245879\n",
      "Stochastic Gradient Descent(151/399): loss=9.569114112165542\n",
      "Stochastic Gradient Descent(152/399): loss=10242.776531695445\n",
      "Stochastic Gradient Descent(153/399): loss=21808.30829769891\n",
      "Stochastic Gradient Descent(154/399): loss=49254.14185781118\n",
      "Stochastic Gradient Descent(155/399): loss=1242.9548109429072\n",
      "Stochastic Gradient Descent(156/399): loss=21.34617194148828\n",
      "Stochastic Gradient Descent(157/399): loss=229.4522167710758\n",
      "Stochastic Gradient Descent(158/399): loss=1423.687066958266\n",
      "Stochastic Gradient Descent(159/399): loss=139.8226465623472\n",
      "Stochastic Gradient Descent(160/399): loss=192.75422160998113\n",
      "Stochastic Gradient Descent(161/399): loss=1338.916333621259\n",
      "Stochastic Gradient Descent(162/399): loss=55158.682113983894\n",
      "Stochastic Gradient Descent(163/399): loss=43.41345573257352\n",
      "Stochastic Gradient Descent(164/399): loss=35642.59449457719\n",
      "Stochastic Gradient Descent(165/399): loss=2541.925968208114\n",
      "Stochastic Gradient Descent(166/399): loss=2655.1029933445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(167/399): loss=1603.0741576340995\n",
      "Stochastic Gradient Descent(168/399): loss=1154.1783702557696\n",
      "Stochastic Gradient Descent(169/399): loss=20607.506024152062\n",
      "Stochastic Gradient Descent(170/399): loss=52611.14039924702\n",
      "Stochastic Gradient Descent(171/399): loss=6504.896403543011\n",
      "Stochastic Gradient Descent(172/399): loss=4153.678925378714\n",
      "Stochastic Gradient Descent(173/399): loss=2153.086520660052\n",
      "Stochastic Gradient Descent(174/399): loss=431230.52725592814\n",
      "Stochastic Gradient Descent(175/399): loss=4864.457885314963\n",
      "Stochastic Gradient Descent(176/399): loss=4728.023264150276\n",
      "Stochastic Gradient Descent(177/399): loss=1079.960978758179\n",
      "Stochastic Gradient Descent(178/399): loss=3895.970061389226\n",
      "Stochastic Gradient Descent(179/399): loss=2825.336709296781\n",
      "Stochastic Gradient Descent(180/399): loss=9055.373166258883\n",
      "Stochastic Gradient Descent(181/399): loss=31438.131763013163\n",
      "Stochastic Gradient Descent(182/399): loss=10358.96456481776\n",
      "Stochastic Gradient Descent(183/399): loss=3046.226367098603\n",
      "Stochastic Gradient Descent(184/399): loss=656.8305325112126\n",
      "Stochastic Gradient Descent(185/399): loss=11418.104093498223\n",
      "Stochastic Gradient Descent(186/399): loss=72596.71040930992\n",
      "Stochastic Gradient Descent(187/399): loss=10563.23433435436\n",
      "Stochastic Gradient Descent(188/399): loss=326.93917684090707\n",
      "Stochastic Gradient Descent(189/399): loss=279.6707415487514\n",
      "Stochastic Gradient Descent(190/399): loss=66682.03295656199\n",
      "Stochastic Gradient Descent(191/399): loss=178.70814006273312\n",
      "Stochastic Gradient Descent(192/399): loss=932.3354770565938\n",
      "Stochastic Gradient Descent(193/399): loss=4230.78105446512\n",
      "Stochastic Gradient Descent(194/399): loss=1130.0728910103803\n",
      "Stochastic Gradient Descent(195/399): loss=566.5677546619295\n",
      "Stochastic Gradient Descent(196/399): loss=19017.315293043783\n",
      "Stochastic Gradient Descent(197/399): loss=9115.488665777704\n",
      "Stochastic Gradient Descent(198/399): loss=73261.54348249025\n",
      "Stochastic Gradient Descent(199/399): loss=405.6751259137187\n",
      "Stochastic Gradient Descent(200/399): loss=860.3920762363559\n",
      "Stochastic Gradient Descent(201/399): loss=158.92942767131447\n",
      "Stochastic Gradient Descent(202/399): loss=4644.682699028004\n",
      "Stochastic Gradient Descent(203/399): loss=4123.004248655796\n",
      "Stochastic Gradient Descent(204/399): loss=875.9502669905359\n",
      "Stochastic Gradient Descent(205/399): loss=37.48654782129378\n",
      "Stochastic Gradient Descent(206/399): loss=662.2416595670344\n",
      "Stochastic Gradient Descent(207/399): loss=185.3466349625522\n",
      "Stochastic Gradient Descent(208/399): loss=50749.625790757986\n",
      "Stochastic Gradient Descent(209/399): loss=313.80161804879265\n",
      "Stochastic Gradient Descent(210/399): loss=6320.236973472837\n",
      "Stochastic Gradient Descent(211/399): loss=596.8964315665132\n",
      "Stochastic Gradient Descent(212/399): loss=913.7903590200638\n",
      "Stochastic Gradient Descent(213/399): loss=8187.318827659387\n",
      "Stochastic Gradient Descent(214/399): loss=2856.5409710542735\n",
      "Stochastic Gradient Descent(215/399): loss=390.4244724789991\n",
      "Stochastic Gradient Descent(216/399): loss=9785.899357038244\n",
      "Stochastic Gradient Descent(217/399): loss=3615.177625843467\n",
      "Stochastic Gradient Descent(218/399): loss=912.6645920569157\n",
      "Stochastic Gradient Descent(219/399): loss=2695.377956829171\n",
      "Stochastic Gradient Descent(220/399): loss=898.87956020155\n",
      "Stochastic Gradient Descent(221/399): loss=207.78749456210903\n",
      "Stochastic Gradient Descent(222/399): loss=307.06367142176157\n",
      "Stochastic Gradient Descent(223/399): loss=17553.808173657242\n",
      "Stochastic Gradient Descent(224/399): loss=643.18609299621\n",
      "Stochastic Gradient Descent(225/399): loss=1274.8060281050946\n",
      "Stochastic Gradient Descent(226/399): loss=127597.21999362322\n",
      "Stochastic Gradient Descent(227/399): loss=25637.48871193578\n",
      "Stochastic Gradient Descent(228/399): loss=8291.636814010119\n",
      "Stochastic Gradient Descent(229/399): loss=8637.430592431823\n",
      "Stochastic Gradient Descent(230/399): loss=2768.993092217726\n",
      "Stochastic Gradient Descent(231/399): loss=1178.207143256345\n",
      "Stochastic Gradient Descent(232/399): loss=10.41073285021251\n",
      "Stochastic Gradient Descent(233/399): loss=106.9491315827071\n",
      "Stochastic Gradient Descent(234/399): loss=2869.26157071635\n",
      "Stochastic Gradient Descent(235/399): loss=94.02459975130681\n",
      "Stochastic Gradient Descent(236/399): loss=434.7524568745522\n",
      "Stochastic Gradient Descent(237/399): loss=3847.799828200091\n",
      "Stochastic Gradient Descent(238/399): loss=11333.433646227852\n",
      "Stochastic Gradient Descent(239/399): loss=821.9645044607561\n",
      "Stochastic Gradient Descent(240/399): loss=11294.171834407556\n",
      "Stochastic Gradient Descent(241/399): loss=460.19778265272794\n",
      "Stochastic Gradient Descent(242/399): loss=1125.0656983143995\n",
      "Stochastic Gradient Descent(243/399): loss=1406.6343602919467\n",
      "Stochastic Gradient Descent(244/399): loss=622.756235208411\n",
      "Stochastic Gradient Descent(245/399): loss=12533.76805310909\n",
      "Stochastic Gradient Descent(246/399): loss=0.12609901830525397\n",
      "Stochastic Gradient Descent(247/399): loss=1131.4400244972692\n",
      "Stochastic Gradient Descent(248/399): loss=446.06635340457865\n",
      "Stochastic Gradient Descent(249/399): loss=0.37906800376194455\n",
      "Stochastic Gradient Descent(250/399): loss=1999.1053160179026\n",
      "Stochastic Gradient Descent(251/399): loss=13850.171512391755\n",
      "Stochastic Gradient Descent(252/399): loss=9264.685570177584\n",
      "Stochastic Gradient Descent(253/399): loss=43.91443514679603\n",
      "Stochastic Gradient Descent(254/399): loss=90.15492547540482\n",
      "Stochastic Gradient Descent(255/399): loss=468.5467687026786\n",
      "Stochastic Gradient Descent(256/399): loss=973.4568770139554\n",
      "Stochastic Gradient Descent(257/399): loss=12838.892980254976\n",
      "Stochastic Gradient Descent(258/399): loss=72.18838313240005\n",
      "Stochastic Gradient Descent(259/399): loss=4.919285749992425\n",
      "Stochastic Gradient Descent(260/399): loss=57.805490330457424\n",
      "Stochastic Gradient Descent(261/399): loss=211.4242866332173\n",
      "Stochastic Gradient Descent(262/399): loss=168.79882499955698\n",
      "Stochastic Gradient Descent(263/399): loss=8851.091651175771\n",
      "Stochastic Gradient Descent(264/399): loss=8706.101435527818\n",
      "Stochastic Gradient Descent(265/399): loss=354.68880686254613\n",
      "Stochastic Gradient Descent(266/399): loss=209.726840136242\n",
      "Stochastic Gradient Descent(267/399): loss=15237.570927831284\n",
      "Stochastic Gradient Descent(268/399): loss=1131.8492363761577\n",
      "Stochastic Gradient Descent(269/399): loss=11212.91372616647\n",
      "Stochastic Gradient Descent(270/399): loss=1689.6573392918692\n",
      "Stochastic Gradient Descent(271/399): loss=37.52104614906016\n",
      "Stochastic Gradient Descent(272/399): loss=611.3684957066963\n",
      "Stochastic Gradient Descent(273/399): loss=12044.737525363873\n",
      "Stochastic Gradient Descent(274/399): loss=48.62751816232719\n",
      "Stochastic Gradient Descent(275/399): loss=604.9248205941288\n",
      "Stochastic Gradient Descent(276/399): loss=1065.536185246667\n",
      "Stochastic Gradient Descent(277/399): loss=2197.7701780619655\n",
      "Stochastic Gradient Descent(278/399): loss=28.148461313923328\n",
      "Stochastic Gradient Descent(279/399): loss=2647.425611750509\n",
      "Stochastic Gradient Descent(280/399): loss=3253.5151190720717\n",
      "Stochastic Gradient Descent(281/399): loss=12342.52210435595\n",
      "Stochastic Gradient Descent(282/399): loss=3058.411921012706\n",
      "Stochastic Gradient Descent(283/399): loss=5.935311100874991\n",
      "Stochastic Gradient Descent(284/399): loss=60.53357965351073\n",
      "Stochastic Gradient Descent(285/399): loss=1312.3018321454408\n",
      "Stochastic Gradient Descent(286/399): loss=3952.3813969981757\n",
      "Stochastic Gradient Descent(287/399): loss=637.5190686401754\n",
      "Stochastic Gradient Descent(288/399): loss=16.122188631292545\n",
      "Stochastic Gradient Descent(289/399): loss=348.79749771949895\n",
      "Stochastic Gradient Descent(290/399): loss=2247.7641446155217\n",
      "Stochastic Gradient Descent(291/399): loss=431.43503327346247\n",
      "Stochastic Gradient Descent(292/399): loss=868.0030355342503\n",
      "Stochastic Gradient Descent(293/399): loss=1307.3178494099832\n",
      "Stochastic Gradient Descent(294/399): loss=548.9291259352701\n",
      "Stochastic Gradient Descent(295/399): loss=1070.1552878865002\n",
      "Stochastic Gradient Descent(296/399): loss=1625.3177327983076\n",
      "Stochastic Gradient Descent(297/399): loss=252.2575663968851\n",
      "Stochastic Gradient Descent(298/399): loss=49.63385711620656\n",
      "Stochastic Gradient Descent(299/399): loss=370.58780370359955\n",
      "Stochastic Gradient Descent(300/399): loss=17.55215540199325\n",
      "Stochastic Gradient Descent(301/399): loss=328.8024473615856\n",
      "Stochastic Gradient Descent(302/399): loss=23534.52722106891\n",
      "Stochastic Gradient Descent(303/399): loss=1015.2376166088358\n",
      "Stochastic Gradient Descent(304/399): loss=355.52489509690093\n",
      "Stochastic Gradient Descent(305/399): loss=446.99376162548384\n",
      "Stochastic Gradient Descent(306/399): loss=28.953247649574823\n",
      "Stochastic Gradient Descent(307/399): loss=150.84711763008053\n",
      "Stochastic Gradient Descent(308/399): loss=681.4851375744912\n",
      "Stochastic Gradient Descent(309/399): loss=42.39192598462544\n",
      "Stochastic Gradient Descent(310/399): loss=895.4762145138258\n",
      "Stochastic Gradient Descent(311/399): loss=2029.7184611450461\n",
      "Stochastic Gradient Descent(312/399): loss=595.3702023325861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(313/399): loss=5025.404072217542\n",
      "Stochastic Gradient Descent(314/399): loss=27929.420704331365\n",
      "Stochastic Gradient Descent(315/399): loss=1390.1865268094764\n",
      "Stochastic Gradient Descent(316/399): loss=19611.03805730012\n",
      "Stochastic Gradient Descent(317/399): loss=457.4532372349228\n",
      "Stochastic Gradient Descent(318/399): loss=26010.473132848925\n",
      "Stochastic Gradient Descent(319/399): loss=36.107388128437606\n",
      "Stochastic Gradient Descent(320/399): loss=19450.286980854024\n",
      "Stochastic Gradient Descent(321/399): loss=9984.041973335463\n",
      "Stochastic Gradient Descent(322/399): loss=41617.73711723526\n",
      "Stochastic Gradient Descent(323/399): loss=1074.853425932241\n",
      "Stochastic Gradient Descent(324/399): loss=2827.210440820649\n",
      "Stochastic Gradient Descent(325/399): loss=1.746214124732272\n",
      "Stochastic Gradient Descent(326/399): loss=913.6519793159086\n",
      "Stochastic Gradient Descent(327/399): loss=4059.844608849279\n",
      "Stochastic Gradient Descent(328/399): loss=3285.868664464841\n",
      "Stochastic Gradient Descent(329/399): loss=1173.2351523615384\n",
      "Stochastic Gradient Descent(330/399): loss=2254.442040592369\n",
      "Stochastic Gradient Descent(331/399): loss=2968.7536167396033\n",
      "Stochastic Gradient Descent(332/399): loss=19904.79174662418\n",
      "Stochastic Gradient Descent(333/399): loss=4556.7711633273475\n",
      "Stochastic Gradient Descent(334/399): loss=353.4258842091257\n",
      "Stochastic Gradient Descent(335/399): loss=1875.6027126994613\n",
      "Stochastic Gradient Descent(336/399): loss=3409.595062372385\n",
      "Stochastic Gradient Descent(337/399): loss=713.0086851754705\n",
      "Stochastic Gradient Descent(338/399): loss=477.82011055107023\n",
      "Stochastic Gradient Descent(339/399): loss=346.230945984694\n",
      "Stochastic Gradient Descent(340/399): loss=1.1473422187693845\n",
      "Stochastic Gradient Descent(341/399): loss=2237.313718847092\n",
      "Stochastic Gradient Descent(342/399): loss=7.921574660770513\n",
      "Stochastic Gradient Descent(343/399): loss=987.1359889819915\n",
      "Stochastic Gradient Descent(344/399): loss=134.78603453791487\n",
      "Stochastic Gradient Descent(345/399): loss=1000.2260143930785\n",
      "Stochastic Gradient Descent(346/399): loss=1761.4667488521234\n",
      "Stochastic Gradient Descent(347/399): loss=764.328606003237\n",
      "Stochastic Gradient Descent(348/399): loss=2880.2614330029887\n",
      "Stochastic Gradient Descent(349/399): loss=11.422733413344115\n",
      "Stochastic Gradient Descent(350/399): loss=46.75047661038228\n",
      "Stochastic Gradient Descent(351/399): loss=126.51438400146981\n",
      "Stochastic Gradient Descent(352/399): loss=451.6339380233417\n",
      "Stochastic Gradient Descent(353/399): loss=11582.576463157551\n",
      "Stochastic Gradient Descent(354/399): loss=4707.382165524984\n",
      "Stochastic Gradient Descent(355/399): loss=1398.5845789776056\n",
      "Stochastic Gradient Descent(356/399): loss=65867.37534150813\n",
      "Stochastic Gradient Descent(357/399): loss=1397.779678098681\n",
      "Stochastic Gradient Descent(358/399): loss=342.45195980640324\n",
      "Stochastic Gradient Descent(359/399): loss=12868.528553729187\n",
      "Stochastic Gradient Descent(360/399): loss=9888.24790206494\n",
      "Stochastic Gradient Descent(361/399): loss=98.86774359844615\n",
      "Stochastic Gradient Descent(362/399): loss=2200.7309506222014\n",
      "Stochastic Gradient Descent(363/399): loss=103034.83526459565\n",
      "Stochastic Gradient Descent(364/399): loss=2305.597345928259\n",
      "Stochastic Gradient Descent(365/399): loss=796.2684014967239\n",
      "Stochastic Gradient Descent(366/399): loss=1143.6822490439195\n",
      "Stochastic Gradient Descent(367/399): loss=38.041298399029884\n",
      "Stochastic Gradient Descent(368/399): loss=885.5277591317855\n",
      "Stochastic Gradient Descent(369/399): loss=3684.2171811209205\n",
      "Stochastic Gradient Descent(370/399): loss=2139.1814599211093\n",
      "Stochastic Gradient Descent(371/399): loss=0.0015957613743953689\n",
      "Stochastic Gradient Descent(372/399): loss=1599.8914420181923\n",
      "Stochastic Gradient Descent(373/399): loss=289.7646845152657\n",
      "Stochastic Gradient Descent(374/399): loss=381.693175225086\n",
      "Stochastic Gradient Descent(375/399): loss=96.85530400788734\n",
      "Stochastic Gradient Descent(376/399): loss=6148.637210565995\n",
      "Stochastic Gradient Descent(377/399): loss=282.7782628951897\n",
      "Stochastic Gradient Descent(378/399): loss=1513.0793490419328\n",
      "Stochastic Gradient Descent(379/399): loss=948.0371889649061\n",
      "Stochastic Gradient Descent(380/399): loss=2736.0855141809334\n",
      "Stochastic Gradient Descent(381/399): loss=2101.9406493300203\n",
      "Stochastic Gradient Descent(382/399): loss=26830.92205090155\n",
      "Stochastic Gradient Descent(383/399): loss=0.017792058497781288\n",
      "Stochastic Gradient Descent(384/399): loss=1995.428734424986\n",
      "Stochastic Gradient Descent(385/399): loss=3151.986587728106\n",
      "Stochastic Gradient Descent(386/399): loss=1.1985509634862417\n",
      "Stochastic Gradient Descent(387/399): loss=2363.188772600009\n",
      "Stochastic Gradient Descent(388/399): loss=32.09124909824599\n",
      "Stochastic Gradient Descent(389/399): loss=489.9624043532493\n",
      "Stochastic Gradient Descent(390/399): loss=108.18504853647724\n",
      "Stochastic Gradient Descent(391/399): loss=1979.4214215468105\n",
      "Stochastic Gradient Descent(392/399): loss=2806.1713716632444\n",
      "Stochastic Gradient Descent(393/399): loss=160.65505610968114\n",
      "Stochastic Gradient Descent(394/399): loss=69.95124396133536\n",
      "Stochastic Gradient Descent(395/399): loss=48.3608788703659\n",
      "Stochastic Gradient Descent(396/399): loss=316.7932304778626\n",
      "Stochastic Gradient Descent(397/399): loss=12.352062187903249\n",
      "Stochastic Gradient Descent(398/399): loss=2350.886083027295\n",
      "Stochastic Gradient Descent(399/399): loss=234.77377055109517\n",
      "0.3740222222222222\n"
     ]
    }
   ],
   "source": [
    "'''SGD'''\n",
    "(w2,loss2) = least_squares_SGD(y, tX, init_w, max_iter, alpha)\n",
    "sgd_tr_pred = predict_labels(w2, tX_valid)\n",
    "print((sgd_tr_pred == y_valid).mean())\n",
    "sgd_pred = predict_labels(w2, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7462\n"
     ]
    }
   ],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "(w3,loss3) = least_squares(y, tX)\n",
    "ls_tr_pred = predict_labels(w3, tX_valid)\n",
    "print((ls_tr_pred == y_valid).mean())\n",
    "ls_pred = predict_labels(w3, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7462\n"
     ]
    }
   ],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "lambda_ = np.logspace(-1, -6, 30)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        min_loss = loss4\n",
    "        ind = i\n",
    "(w4,loss4) = ridge_regression(y, tX, lambda_[ind])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())\n",
    "rd_pred = predict_labels(w4, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/results_least_sq.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w1, tX_test)\n",
    "create_csv_submission(ids_test, gd_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
