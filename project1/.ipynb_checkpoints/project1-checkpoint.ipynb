{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "            \n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "print(\"Targets: \", y)\n",
    "print(\"Ids: \",ids)\n",
    "print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE'''\n",
    "#Feature extraction\n",
    "tX = np.hstack((tX_old[:,1:4], tX_old[:,7:12]))\n",
    "tX = np.hstack((tX, tX_old[:,13:23]))\n",
    "#Standardize\n",
    "tX, tX_mean, tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size:  25000\n",
      "Shapes of tX, y & Ids for Training:  (25000, 18) (25000,) (25000,)\n",
      "Shapes of tX, y & Ids for Validation:  (225000, 18) (225000,) (225000,)\n"
     ]
    }
   ],
   "source": [
    "'''SPLIT INTO TRAIN AND VALIDATION'''\n",
    "\n",
    "train_valid_split = int(tX.shape[0] / 10)\n",
    "print(\"Validation data size: \", train_valid_split)\n",
    "tX_valid = tX[train_valid_split:,:]\n",
    "y_valid = y[train_valid_split:]\n",
    "id_valid = ids[train_valid_split:]\n",
    "\n",
    "tX = tX[:train_valid_split]\n",
    "y = y[:train_valid_split]\n",
    "ids = ids[:train_valid_split]\n",
    "\n",
    "print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, id_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7135288888888889\n"
     ]
    }
   ],
   "source": [
    "'''CHECK MODEL SUCCESS: DELETE BEFORE SUBMISSION'''\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "clf = linear_model.SGDClassifier(max_iter=500, tol=1e-3)\n",
    "clf.fit(tX, y)\n",
    "print(metrics.accuracy_score(clf.predict(tX_valid), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tX@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(np.square(w)))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    return 1 / (1 + np.exp(-1*(tx@w)))\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(np.log(sigm).T * -y - (np.log(1 - sigm).T * (1-y))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.inv((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]))@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    lr_scheduler = [(max_iters) / 4, (2* max_iters) / 4,(3* max_iters) / 4]\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter in lr_scheduler:\n",
    "            gamma = gamma / 10\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of tX & Ids for Testing:  (568238, 18) (25000,)\n"
     ]
    }
   ],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE FOR TEST'''\n",
    "#Feature extraction\n",
    "tX_test = np.hstack((tX_test_old[:,1:4], tX_test_old[:,7:12]))\n",
    "tX_test = np.hstack((tX_test, tX_test_old[:,13:23]))\n",
    "#Standardize\n",
    "tX_test, tX_test_mean, tX_test_std = standardize(tX_test)\n",
    "\n",
    "print(\"Shapes of tX & Ids for Testing: \", tX_test.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "[0.28878839 0.46716249 0.98066217 0.73794721 0.53839558 0.76739922\n",
      " 0.29195506 0.60629481 0.05643437 0.29796505 0.37559797 0.12523107\n",
      " 0.1678032  0.05460867 0.99508156 0.88094606 0.00761578 0.18877718]\n"
     ]
    }
   ],
   "source": [
    "ww = np.random.rand(tX.shape[1])\n",
    "init_w = np.array(ww, dtype=np.float64)\n",
    "#init_w = np.zeros(tX.shape[1])\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HYPER PARAMETERS FOR TUNING'''\n",
    "max_iter = 500\n",
    "alpha = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=5.994380081851646\n",
      "Gradient Descent(1/499): loss=5.952144158245616\n",
      "Gradient Descent(2/499): loss=5.910292430062107\n",
      "Gradient Descent(3/499): loss=5.868821182837534\n",
      "Gradient Descent(4/499): loss=5.827726738780492\n",
      "Gradient Descent(5/499): loss=5.7870054564068365\n",
      "Gradient Descent(6/499): loss=5.746653730178444\n",
      "Gradient Descent(7/499): loss=5.706667990145559\n",
      "Gradient Descent(8/499): loss=5.667044701592719\n",
      "Gradient Descent(9/499): loss=5.627780364688205\n",
      "Gradient Descent(10/499): loss=5.58887151413701\n",
      "Gradient Descent(11/499): loss=5.550314718837238\n",
      "Gradient Descent(12/499): loss=5.512106581539968\n",
      "Gradient Descent(13/499): loss=5.474243738512492\n",
      "Gradient Descent(14/499): loss=5.436722859204909\n",
      "Gradient Descent(15/499): loss=5.399540645920075\n",
      "Gradient Descent(16/499): loss=5.362693833486807\n",
      "Gradient Descent(17/499): loss=5.326179188936388\n",
      "Gradient Descent(18/499): loss=5.289993511182263\n",
      "Gradient Descent(19/499): loss=5.254133630702963\n",
      "Gradient Descent(20/499): loss=5.218596409228178\n",
      "Gradient Descent(21/499): loss=5.18337873942796\n",
      "Gradient Descent(22/499): loss=5.148477544605036\n",
      "Gradient Descent(23/499): loss=5.1138897783901855\n",
      "Gradient Descent(24/499): loss=5.079612424440663\n",
      "Gradient Descent(25/499): loss=5.045642496141623\n",
      "Gradient Descent(26/499): loss=5.011977036310532\n",
      "Gradient Descent(27/499): loss=4.978613116904522\n",
      "Gradient Descent(28/499): loss=4.945547838730669\n",
      "Gradient Descent(29/499): loss=4.912778331159166\n",
      "Gradient Descent(30/499): loss=4.880301751839348\n",
      "Gradient Descent(31/499): loss=4.8481152864185555\n",
      "Gradient Descent(32/499): loss=4.816216148263808\n",
      "Gradient Descent(33/499): loss=4.784601578186247\n",
      "Gradient Descent(34/499): loss=4.753268844168328\n",
      "Gradient Descent(35/499): loss=4.722215241093748\n",
      "Gradient Descent(36/499): loss=4.691438090480042\n",
      "Gradient Descent(37/499): loss=4.660934740213875\n",
      "Gradient Descent(38/499): loss=4.630702564288956\n",
      "Gradient Descent(39/499): loss=4.600738962546575\n",
      "Gradient Descent(40/499): loss=4.571041360418722\n",
      "Gradient Descent(41/499): loss=4.541607208673783\n",
      "Gradient Descent(42/499): loss=4.512433983164751\n",
      "Gradient Descent(43/499): loss=4.483519184579972\n",
      "Gradient Descent(44/499): loss=4.454860338196356\n",
      "Gradient Descent(45/499): loss=4.426454993635072\n",
      "Gradient Descent(46/499): loss=4.39830072461965\n",
      "Gradient Descent(47/499): loss=4.370395128736537\n",
      "Gradient Descent(48/499): loss=4.342735827198003\n",
      "Gradient Descent(49/499): loss=4.31532046460743\n",
      "Gradient Descent(50/499): loss=4.288146708726945\n",
      "Gradient Descent(51/499): loss=4.26121225024736\n",
      "Gradient Descent(52/499): loss=4.234514802560416\n",
      "Gradient Descent(53/499): loss=4.208052101533294\n",
      "Gradient Descent(54/499): loss=4.181821905285373\n",
      "Gradient Descent(55/499): loss=4.155821993967224\n",
      "Gradient Descent(56/499): loss=4.130050169541793\n",
      "Gradient Descent(57/499): loss=4.104504255567786\n",
      "Gradient Descent(58/499): loss=4.079182096985191\n",
      "Gradient Descent(59/499): loss=4.054081559902966\n",
      "Gradient Descent(60/499): loss=4.029200531388817\n",
      "Gradient Descent(61/499): loss=4.004536919261091\n",
      "Gradient Descent(62/499): loss=3.9800886518827396\n",
      "Gradient Descent(63/499): loss=3.9558536779573394\n",
      "Gradient Descent(64/499): loss=3.931829966327132\n",
      "Gradient Descent(65/499): loss=3.9080155057731045\n",
      "Gradient Descent(66/499): loss=3.884408304817038\n",
      "Gradient Descent(67/499): loss=3.8610063915255446\n",
      "Gradient Descent(68/499): loss=3.8378078133160574\n",
      "Gradient Descent(69/499): loss=3.8148106367647436\n",
      "Gradient Descent(70/499): loss=3.7920129474163535\n",
      "Gradient Descent(71/499): loss=3.769412849595944\n",
      "Gradient Descent(72/499): loss=3.74700846622249\n",
      "Gradient Descent(73/499): loss=3.7247979386243606\n",
      "Gradient Descent(74/499): loss=3.702779426356616\n",
      "Gradient Descent(75/499): loss=3.6809511070201495\n",
      "Gradient Descent(76/499): loss=3.6593111760826122\n",
      "Gradient Descent(77/499): loss=3.6378578467011327\n",
      "Gradient Descent(78/499): loss=3.6165893495468078\n",
      "Gradient Descent(79/499): loss=3.595503932630933\n",
      "Gradient Descent(80/499): loss=3.574599861132974\n",
      "Gradient Descent(81/499): loss=3.553875417230248\n",
      "Gradient Descent(82/499): loss=3.5333288999293027\n",
      "Gradient Descent(83/499): loss=3.512958624898982\n",
      "Gradient Descent(84/499): loss=3.492762924305154\n",
      "Gradient Descent(85/499): loss=3.472740146647073\n",
      "Gradient Descent(86/499): loss=3.4528886565953973\n",
      "Gradient Descent(87/499): loss=3.433206834831799\n",
      "Gradient Descent(88/499): loss=3.4136930778901875\n",
      "Gradient Descent(89/499): loss=3.3943457979995078\n",
      "Gradient Descent(90/499): loss=3.3751634229281047\n",
      "Gradient Descent(91/499): loss=3.356144395829646\n",
      "Gradient Descent(92/499): loss=3.3372871750905753\n",
      "Gradient Descent(93/499): loss=3.3185902341790836\n",
      "Gradient Descent(94/499): loss=3.300052061495594\n",
      "Gradient Descent(95/499): loss=3.2816711602247257\n",
      "Gradient Descent(96/499): loss=3.263446048188743\n",
      "Gradient Descent(97/499): loss=3.245375257702462\n",
      "Gradient Descent(98/499): loss=3.2274573354295937\n",
      "Gradient Descent(99/499): loss=3.20969084224054\n",
      "Gradient Descent(100/499): loss=3.192074353071582\n",
      "Gradient Descent(101/499): loss=3.1746064567854906\n",
      "Gradient Descent(102/499): loss=3.157285756033514\n",
      "Gradient Descent(103/499): loss=3.140110867118748\n",
      "Gradient Descent(104/499): loss=3.123080419860865\n",
      "Gradient Descent(105/499): loss=3.1061930574621885\n",
      "Gradient Descent(106/499): loss=3.089447436375104\n",
      "Gradient Descent(107/499): loss=3.0728422261707937\n",
      "Gradient Descent(108/499): loss=3.0563761094092743\n",
      "Gradient Descent(109/499): loss=3.0400477815107316\n",
      "Gradient Descent(110/499): loss=3.023855950628138\n",
      "Gradient Descent(111/499): loss=3.0077993375211367\n",
      "Gradient Descent(112/499): loss=2.9918766754311843\n",
      "Gradient Descent(113/499): loss=2.9760867099579373\n",
      "Gradient Descent(114/499): loss=2.960428198936868\n",
      "Gradient Descent(115/499): loss=2.9448999123181028\n",
      "Gradient Descent(116/499): loss=2.92950063204646\n",
      "Gradient Descent(117/499): loss=2.914229151942697\n",
      "Gradient Descent(118/499): loss=2.8990842775859216\n",
      "Gradient Descent(119/499): loss=2.884064826197192\n",
      "Gradient Descent(120/499): loss=2.8691696265242643\n",
      "Gradient Descent(121/499): loss=2.8543975187274926\n",
      "Gradient Descent(122/499): loss=2.8397473542668643\n",
      "Gradient Descent(123/499): loss=2.8252179957901657\n",
      "Gradient Descent(124/499): loss=2.8108083170222518\n",
      "Gradient Descent(125/499): loss=2.7965172026554206\n",
      "Gradient Descent(126/499): loss=2.782343548240896\n",
      "Gradient Descent(127/499): loss=2.7682862600813603\n",
      "Gradient Descent(128/499): loss=2.7543442551245914\n",
      "Gradient Descent(129/499): loss=2.7405164608581254\n",
      "Gradient Descent(130/499): loss=2.726801815205\n",
      "Gradient Descent(131/499): loss=2.71319926642051\n",
      "Gradient Descent(132/499): loss=2.699707772990007\n",
      "Gradient Descent(133/499): loss=2.6863263035277103\n",
      "Gradient Descent(134/499): loss=2.673053836676526\n",
      "Gradient Descent(135/499): loss=2.6598893610088647\n",
      "Gradient Descent(136/499): loss=2.646831874928442\n",
      "Gradient Descent(137/499): loss=2.633880386573069\n",
      "Gradient Descent(138/499): loss=2.621033913718393\n",
      "Gradient Descent(139/499): loss=2.608291483682611\n",
      "Gradient Descent(140/499): loss=2.5956521332321247\n",
      "Gradient Descent(141/499): loss=2.583114908488136\n",
      "Gradient Descent(142/499): loss=2.5706788648341727\n",
      "Gradient Descent(143/499): loss=2.5583430668245364\n",
      "Gradient Descent(144/499): loss=2.5461065880936578\n",
      "Gradient Descent(145/499): loss=2.5339685112663566\n",
      "Gradient Descent(146/499): loss=2.521927927868994\n",
      "Gradient Descent(147/499): loss=2.5099839382415134\n",
      "Gradient Descent(148/499): loss=2.4981356514503474\n",
      "Gradient Descent(149/499): loss=2.4863821852022014\n",
      "Gradient Descent(150/499): loss=2.4747226657586903\n",
      "Gradient Descent(151/499): loss=2.4631562278518206\n",
      "Gradient Descent(152/499): loss=2.4516820146003275\n",
      "Gradient Descent(153/499): loss=2.4402991774268274\n",
      "Gradient Descent(154/499): loss=2.4290068759758063\n",
      "Gradient Descent(155/499): loss=2.4178042780324214\n",
      "Gradient Descent(156/499): loss=2.406690559442106\n",
      "Gradient Descent(157/499): loss=2.3956649040309843\n",
      "Gradient Descent(158/499): loss=2.3847265035270673\n",
      "Gradient Descent(159/499): loss=2.3738745574822397\n",
      "Gradient Descent(160/499): loss=2.3631082731950204\n",
      "Gradient Descent(161/499): loss=2.352426865634091\n",
      "Gradient Descent(162/499): loss=2.341829557362589\n",
      "Gradient Descent(163/499): loss=2.3313155784631454\n",
      "Gradient Descent(164/499): loss=2.320884166463679\n",
      "Gradient Descent(165/499): loss=2.310534566263922\n",
      "Gradient Descent(166/499): loss=2.300266030062676\n",
      "Gradient Descent(167/499): loss=2.290077817285796\n",
      "Gradient Descent(168/499): loss=2.279969194514885\n",
      "Gradient Descent(169/499): loss=2.2699394354167066\n",
      "Gradient Descent(170/499): loss=2.259987820673288\n",
      "Gradient Descent(171/499): loss=2.250113637912728\n",
      "Gradient Descent(172/499): loss=2.2403161816406874\n",
      "Gradient Descent(173/499): loss=2.230594753172564\n",
      "Gradient Descent(174/499): loss=2.2209486605663415\n",
      "Gradient Descent(175/499): loss=2.211377218556102\n",
      "Gradient Descent(176/499): loss=2.2018797484862063\n",
      "Gradient Descent(177/499): loss=2.1924555782461232\n",
      "Gradient Descent(178/499): loss=2.1831040422059127\n",
      "Gradient Descent(179/499): loss=2.1738244811523413\n",
      "Gradient Descent(180/499): loss=2.164616242225643\n",
      "Gradient Descent(181/499): loss=2.1554786788569027\n",
      "Gradient Descent(182/499): loss=2.1464111507060615\n",
      "Gradient Descent(183/499): loss=2.1374130236005437\n",
      "Gradient Descent(184/499): loss=2.128483669474487\n",
      "Gradient Descent(185/499): loss=2.119622466308585\n",
      "Gradient Descent(186/499): loss=2.1108287980705187\n",
      "Gradient Descent(187/499): loss=2.1021020546559863\n",
      "Gradient Descent(188/499): loss=2.0934416318303173\n",
      "Gradient Descent(189/499): loss=2.084846931170666\n",
      "Gradient Descent(190/499): loss=2.076317360008782\n",
      "Gradient Descent(191/499): loss=2.0678523313743473\n",
      "Gradient Descent(192/499): loss=2.0594512639388745\n",
      "Gradient Descent(193/499): loss=2.051113581960169\n",
      "Gradient Descent(194/499): loss=2.042838715227338\n",
      "Gradient Descent(195/499): loss=2.034626099006344\n",
      "Gradient Descent(196/499): loss=2.0264751739861087\n",
      "Gradient Descent(197/499): loss=2.0183853862251384\n",
      "Gradient Descent(198/499): loss=2.010356187098694\n",
      "Gradient Descent(199/499): loss=2.002387033246477\n",
      "Gradient Descent(200/499): loss=1.994477386520836\n",
      "Gradient Descent(201/499): loss=1.986626713935491\n",
      "Gradient Descent(202/499): loss=1.9788344876147645\n",
      "Gradient Descent(203/499): loss=1.97110018474332\n",
      "Gradient Descent(204/499): loss=1.9634232875163944\n",
      "Gradient Descent(205/499): loss=1.9558032830905299\n",
      "Gradient Descent(206/499): loss=1.9482396635347918\n",
      "Gradient Descent(207/499): loss=1.9407319257824747\n",
      "Gradient Descent(208/499): loss=1.9332795715832831\n",
      "Gradient Descent(209/499): loss=1.925882107455991\n",
      "Gradient Descent(210/499): loss=1.9185390446415704\n",
      "Gradient Descent(211/499): loss=1.9112498990567854\n",
      "Gradient Descent(212/499): loss=1.9040141912482464\n",
      "Gradient Descent(213/499): loss=1.8968314463469207\n",
      "Gradient Descent(214/499): loss=1.8897011940230946\n",
      "Gradient Descent(215/499): loss=1.8826229684417861\n",
      "Gradient Descent(216/499): loss=1.8755963082185936\n",
      "Gradient Descent(217/499): loss=1.868620756375988\n",
      "Gradient Descent(218/499): loss=1.8616958603000384\n",
      "Gradient Descent(219/499): loss=1.8548211716975656\n",
      "Gradient Descent(220/499): loss=1.8479962465537205\n",
      "Gradient Descent(221/499): loss=1.8412206450899864\n",
      "Gradient Descent(222/499): loss=1.8344939317225955\n",
      "Gradient Descent(223/499): loss=1.8278156750213599\n",
      "Gradient Descent(224/499): loss=1.8211854476689087\n",
      "Gradient Descent(225/499): loss=1.814602826420333\n",
      "Gradient Descent(226/499): loss=1.8080673920632298\n",
      "Gradient Descent(227/499): loss=1.801578729378141\n",
      "Gradient Descent(228/499): loss=1.7951364270993861\n",
      "Gradient Descent(229/499): loss=1.7887400778762859\n",
      "Gradient Descent(230/499): loss=1.782389278234766\n",
      "Gradient Descent(231/499): loss=1.7760836285393453\n",
      "Gradient Descent(232/499): loss=1.7698227329555025\n",
      "Gradient Descent(233/499): loss=1.7636061994124075\n",
      "Gradient Descent(234/499): loss=1.757433639566036\n",
      "Gradient Descent(235/499): loss=1.7513046687626366\n",
      "Gradient Descent(236/499): loss=1.7452189060025716\n",
      "Gradient Descent(237/499): loss=1.739175973904507\n",
      "Gradient Descent(238/499): loss=1.7331754986699646\n",
      "Gradient Descent(239/499): loss=1.727217110048221\n",
      "Gradient Descent(240/499): loss=1.7213004413015596\n",
      "Gradient Descent(241/499): loss=1.715425129170859\n",
      "Gradient Descent(242/499): loss=1.7095908138415352\n",
      "Gradient Descent(243/499): loss=1.7037971389098063\n",
      "Gradient Descent(244/499): loss=1.6980437513493056\n",
      "Gradient Descent(245/499): loss=1.6923303014780169\n",
      "Gradient Descent(246/499): loss=1.6866564429255413\n",
      "Gradient Descent(247/499): loss=1.6810218326006865\n",
      "Gradient Descent(248/499): loss=1.6754261306593796\n",
      "Gradient Descent(249/499): loss=1.669869000472896\n",
      "Gradient Descent(250/499): loss=1.6643501085964056\n",
      "Gradient Descent(251/499): loss=1.6588691247378262\n",
      "Gradient Descent(252/499): loss=1.6534257217269919\n",
      "Gradient Descent(253/499): loss=1.648019575485122\n",
      "Gradient Descent(254/499): loss=1.642650364994594\n",
      "Gradient Descent(255/499): loss=1.6373177722690178\n",
      "Gradient Descent(256/499): loss=1.6320214823236028\n",
      "Gradient Descent(257/499): loss=1.6267611831458229\n",
      "Gradient Descent(258/499): loss=1.6215365656663687\n",
      "Gradient Descent(259/499): loss=1.6163473237303894\n",
      "Gradient Descent(260/499): loss=1.6111931540690183\n",
      "Gradient Descent(261/499): loss=1.6060737562711827\n",
      "Gradient Descent(262/499): loss=1.6009888327556885\n",
      "Gradient Descent(263/499): loss=1.5959380887435877\n",
      "Gradient Descent(264/499): loss=1.5909212322308124\n",
      "Gradient Descent(265/499): loss=1.585937973961088\n",
      "Gradient Descent(266/499): loss=1.5809880273991044\n",
      "Gradient Descent(267/499): loss=1.5760711087039645\n",
      "Gradient Descent(268/499): loss=1.5711869367028852\n",
      "Gradient Descent(269/499): loss=1.5663352328651665\n",
      "Gradient Descent(270/499): loss=1.5615157212764128\n",
      "Gradient Descent(271/499): loss=1.5567281286130137\n",
      "Gradient Descent(272/499): loss=1.5519721841168728\n",
      "Gradient Descent(273/499): loss=1.5472476195703935\n",
      "Gradient Descent(274/499): loss=1.5425541692717049\n",
      "Gradient Descent(275/499): loss=1.5378915700101368\n",
      "Gradient Descent(276/499): loss=1.5332595610419382\n",
      "Gradient Descent(277/499): loss=1.5286578840662337\n",
      "Gradient Descent(278/499): loss=1.5240862832012192\n",
      "Gradient Descent(279/499): loss=1.5195445049605933\n",
      "Gradient Descent(280/499): loss=1.515032298230222\n",
      "Gradient Descent(281/499): loss=1.5105494142450309\n",
      "Gradient Descent(282/499): loss=1.5060956065661333\n",
      "Gradient Descent(283/499): loss=1.501670631058177\n",
      "Gradient Descent(284/499): loss=1.4972742458669197\n",
      "Gradient Descent(285/499): loss=1.492906211397023\n",
      "Gradient Descent(286/499): loss=1.488566290290072\n",
      "Gradient Descent(287/499): loss=1.4842542474028035\n",
      "Gradient Descent(288/499): loss=1.479969849785558\n",
      "Gradient Descent(289/499): loss=1.475712866660938\n",
      "Gradient Descent(290/499): loss=1.4714830694026837\n",
      "Gradient Descent(291/499): loss=1.4672802315147502\n",
      "Gradient Descent(292/499): loss=1.4631041286105995\n",
      "Gradient Descent(293/499): loss=1.458954538392691\n",
      "Gradient Descent(294/499): loss=1.4548312406321782\n",
      "Gradient Descent(295/499): loss=1.450734017148803\n",
      "Gradient Descent(296/499): loss=1.4466626517909933\n",
      "Gradient Descent(297/499): loss=1.4426169304161542\n",
      "Gradient Descent(298/499): loss=1.4385966408711537\n",
      "Gradient Descent(299/499): loss=1.434601572973001\n",
      "Gradient Descent(300/499): loss=1.4306315184897191\n",
      "Gradient Descent(301/499): loss=1.4266862711214028\n",
      "Gradient Descent(302/499): loss=1.422765626481466\n",
      "Gradient Descent(303/499): loss=1.4188693820780724\n",
      "Gradient Descent(304/499): loss=1.4149973372957498\n",
      "Gradient Descent(305/499): loss=1.41114929337719\n",
      "Gradient Descent(306/499): loss=1.4073250534052222\n",
      "Gradient Descent(307/499): loss=1.4035244222849692\n",
      "Gradient Descent(308/499): loss=1.3997472067261776\n",
      "Gradient Descent(309/499): loss=1.3959932152257226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(310/499): loss=1.3922622580502886\n",
      "Gradient Descent(311/499): loss=1.3885541472192133\n",
      "Gradient Descent(312/499): loss=1.384868696487512\n",
      "Gradient Descent(313/499): loss=1.3812057213290587\n",
      "Gradient Descent(314/499): loss=1.3775650389199414\n",
      "Gradient Descent(315/499): loss=1.373946468121976\n",
      "Gradient Descent(316/499): loss=1.370349829466388\n",
      "Gradient Descent(317/499): loss=1.3667749451376496\n",
      "Gradient Descent(318/499): loss=1.3632216389574823\n",
      "Gradient Descent(319/499): loss=1.3596897363690112\n",
      "Gradient Descent(320/499): loss=1.356179064421082\n",
      "Gradient Descent(321/499): loss=1.3526894517527266\n",
      "Gradient Descent(322/499): loss=1.349220728577787\n",
      "Gradient Descent(323/499): loss=1.345772726669689\n",
      "Gradient Descent(324/499): loss=1.3423452793463628\n",
      "Gradient Descent(325/499): loss=1.3389382214553187\n",
      "Gradient Descent(326/499): loss=1.3355513893588649\n",
      "Gradient Descent(327/499): loss=1.3321846209194736\n",
      "Gradient Descent(328/499): loss=1.3288377554852906\n",
      "Gradient Descent(329/499): loss=1.3255106338757865\n",
      "Gradient Descent(330/499): loss=1.3222030983675526\n",
      "Gradient Descent(331/499): loss=1.3189149926802337\n",
      "Gradient Descent(332/499): loss=1.3156461619626003\n",
      "Gradient Descent(333/499): loss=1.3123964527787602\n",
      "Gradient Descent(334/499): loss=1.309165713094503\n",
      "Gradient Descent(335/499): loss=1.3059537922637798\n",
      "Gradient Descent(336/499): loss=1.3027605410153196\n",
      "Gradient Descent(337/499): loss=1.2995858114393712\n",
      "Gradient Descent(338/499): loss=1.2964294569745816\n",
      "Gradient Descent(339/499): loss=1.2932913323950013\n",
      "Gradient Descent(340/499): loss=1.2901712937972165\n",
      "Gradient Descent(341/499): loss=1.28706919858761\n",
      "Gradient Descent(342/499): loss=1.2839849054697468\n",
      "Gradient Descent(343/499): loss=1.2809182744318832\n",
      "Gradient Descent(344/499): loss=1.2778691667346\n",
      "Gradient Descent(345/499): loss=1.2748374448985578\n",
      "Gradient Descent(346/499): loss=1.2718229726923707\n",
      "Gradient Descent(347/499): loss=1.268825615120601\n",
      "Gradient Descent(348/499): loss=1.2658452384118728\n",
      "Gradient Descent(349/499): loss=1.2628817100071017\n",
      "Gradient Descent(350/499): loss=1.2599348985478378\n",
      "Gradient Descent(351/499): loss=1.2570046738647291\n",
      "Gradient Descent(352/499): loss=1.2540909069660935\n",
      "Gradient Descent(353/499): loss=1.2511934700266045\n",
      "Gradient Descent(354/499): loss=1.2483122363760897\n",
      "Gradient Descent(355/499): loss=1.2454470804884372\n",
      "Gradient Descent(356/499): loss=1.2425978779706142\n",
      "Gradient Descent(357/499): loss=1.2397645055517896\n",
      "Gradient Descent(358/499): loss=1.2369468410725672\n",
      "Gradient Descent(359/499): loss=1.2341447634743221\n",
      "Gradient Descent(360/499): loss=1.2313581527886441\n",
      "Gradient Descent(361/499): loss=1.2285868901268833\n",
      "Gradient Descent(362/499): loss=1.2258308576697983\n",
      "Gradient Descent(363/499): loss=1.2230899386573093\n",
      "Gradient Descent(364/499): loss=1.2203640173783463\n",
      "Gradient Descent(365/499): loss=1.2176529791608033\n",
      "Gradient Descent(366/499): loss=1.2149567103615848\n",
      "Gradient Descent(367/499): loss=1.2122750983567574\n",
      "Gradient Descent(368/499): loss=1.2096080315317905\n",
      "Gradient Descent(369/499): loss=1.206955399271899\n",
      "Gradient Descent(370/499): loss=1.204317091952478\n",
      "Gradient Descent(371/499): loss=1.2016930009296296\n",
      "Gradient Descent(372/499): loss=1.1990830185307888\n",
      "Gradient Descent(373/499): loss=1.1964870380454338\n",
      "Gradient Descent(374/499): loss=1.193904953715895\n",
      "Gradient Descent(375/499): loss=1.1913366607282472\n",
      "Gradient Descent(376/499): loss=1.1887820552032964\n",
      "Gradient Descent(377/499): loss=1.1862410341876526\n",
      "Gradient Descent(378/499): loss=1.1837134956448918\n",
      "Gradient Descent(379/499): loss=1.1811993384468014\n",
      "Gradient Descent(380/499): loss=1.1786984623647163\n",
      "Gradient Descent(381/499): loss=1.1762107680609364\n",
      "Gradient Descent(382/499): loss=1.1737361570802292\n",
      "Gradient Descent(383/499): loss=1.1712745318414166\n",
      "Gradient Descent(384/499): loss=1.1688257956290427\n",
      "Gradient Descent(385/499): loss=1.1663898525851242\n",
      "Gradient Descent(386/499): loss=1.1639666077009831\n",
      "Gradient Descent(387/499): loss=1.1615559668091564\n",
      "Gradient Descent(388/499): loss=1.1591578365753865\n",
      "Gradient Descent(389/499): loss=1.1567721244906919\n",
      "Gradient Descent(390/499): loss=1.1543987388635126\n",
      "Gradient Descent(391/499): loss=1.1520375888119345\n",
      "Gradient Descent(392/499): loss=1.1496885842559892\n",
      "Gradient Descent(393/499): loss=1.1473516359100289\n",
      "Gradient Descent(394/499): loss=1.1450266552751773\n",
      "Gradient Descent(395/499): loss=1.1427135546318536\n",
      "Gradient Descent(396/499): loss=1.140412247032369\n",
      "Gradient Descent(397/499): loss=1.138122646293596\n",
      "Gradient Descent(398/499): loss=1.135844666989712\n",
      "Gradient Descent(399/499): loss=1.13357822444501\n",
      "Gradient Descent(400/499): loss=1.1313232347267812\n",
      "Gradient Descent(401/499): loss=1.1290796146382691\n",
      "Gradient Descent(402/499): loss=1.1268472817116901\n",
      "Gradient Descent(403/499): loss=1.1246261542013232\n",
      "Gradient Descent(404/499): loss=1.1224161510766675\n",
      "Gradient Descent(405/499): loss=1.1202171920156663\n",
      "Gradient Descent(406/499): loss=1.1180291973979988\n",
      "Gradient Descent(407/499): loss=1.115852088298435\n",
      "Gradient Descent(408/499): loss=1.1136857864802565\n",
      "Gradient Descent(409/499): loss=1.1115302143887447\n",
      "Gradient Descent(410/499): loss=1.1093852951447263\n",
      "Gradient Descent(411/499): loss=1.1072509525381877\n",
      "Gradient Descent(412/499): loss=1.10512711102195\n",
      "Gradient Descent(413/499): loss=1.1030136957054038\n",
      "Gradient Descent(414/499): loss=1.100910632348308\n",
      "Gradient Descent(415/499): loss=1.0988178473546482\n",
      "Gradient Descent(416/499): loss=1.0967352677665536\n",
      "Gradient Descent(417/499): loss=1.0946628212582767\n",
      "Gradient Descent(418/499): loss=1.0926004361302277\n",
      "Gradient Descent(419/499): loss=1.0905480413030704\n",
      "Gradient Descent(420/499): loss=1.088505566311874\n",
      "Gradient Descent(421/499): loss=1.0864729413003225\n",
      "Gradient Descent(422/499): loss=1.0844500970149813\n",
      "Gradient Descent(423/499): loss=1.0824369647996182\n",
      "Gradient Descent(424/499): loss=1.0804334765895804\n",
      "Gradient Descent(425/499): loss=1.0784395649062284\n",
      "Gradient Descent(426/499): loss=1.07645516285142\n",
      "Gradient Descent(427/499): loss=1.0744802041020525\n",
      "Gradient Descent(428/499): loss=1.0725146229046547\n",
      "Gradient Descent(429/499): loss=1.0705583540700345\n",
      "Gradient Descent(430/499): loss=1.0686113329679752\n",
      "Gradient Descent(431/499): loss=1.0666734955219874\n",
      "Gradient Descent(432/499): loss=1.06474477820411\n",
      "Gradient Descent(433/499): loss=1.0628251180297603\n",
      "Gradient Descent(434/499): loss=1.060914452552638\n",
      "Gradient Descent(435/499): loss=1.0590127198596764\n",
      "Gradient Descent(436/499): loss=1.0571198585660413\n",
      "Gradient Descent(437/499): loss=1.0552358078101836\n",
      "Gradient Descent(438/499): loss=1.0533605072489347\n",
      "Gradient Descent(439/499): loss=1.0514938970526528\n",
      "Gradient Descent(440/499): loss=1.0496359179004153\n",
      "Gradient Descent(441/499): loss=1.0477865109752582\n",
      "Gradient Descent(442/499): loss=1.0459456179594628\n",
      "Gradient Descent(443/499): loss=1.044113181029886\n",
      "Gradient Descent(444/499): loss=1.0422891428533378\n",
      "Gradient Descent(445/499): loss=1.0404734465820038\n",
      "Gradient Descent(446/499): loss=1.0386660358489113\n",
      "Gradient Descent(447/499): loss=1.0368668547634403\n",
      "Gradient Descent(448/499): loss=1.0350758479068762\n",
      "Gradient Descent(449/499): loss=1.0332929603280103\n",
      "Gradient Descent(450/499): loss=1.0315181375387772\n",
      "Gradient Descent(451/499): loss=1.0297513255099382\n",
      "Gradient Descent(452/499): loss=1.0279924706668069\n",
      "Gradient Descent(453/499): loss=1.026241519885014\n",
      "Gradient Descent(454/499): loss=1.0244984204863155\n",
      "Gradient Descent(455/499): loss=1.0227631202344392\n",
      "Gradient Descent(456/499): loss=1.0210355673309743\n",
      "Gradient Descent(457/499): loss=1.019315710411299\n",
      "Gradient Descent(458/499): loss=1.0176034985405484\n",
      "Gradient Descent(459/499): loss=1.0158988812096201\n",
      "Gradient Descent(460/499): loss=1.014201808331222\n",
      "Gradient Descent(461/499): loss=1.0125122302359535\n",
      "Gradient Descent(462/499): loss=1.010830097668429\n",
      "Gradient Descent(463/499): loss=1.0091553617834375\n",
      "Gradient Descent(464/499): loss=1.0074879741421383\n",
      "Gradient Descent(465/499): loss=1.0058278867082948\n",
      "Gradient Descent(466/499): loss=1.0041750518445454\n",
      "Gradient Descent(467/499): loss=1.0025294223087085\n",
      "Gradient Descent(468/499): loss=1.000890951250125\n",
      "Gradient Descent(469/499): loss=0.9992595922060358\n",
      "Gradient Descent(470/499): loss=0.9976352990979941\n",
      "Gradient Descent(471/499): loss=0.996018026228313\n",
      "Gradient Descent(472/499): loss=0.9944077282765467\n",
      "Gradient Descent(473/499): loss=0.9928043602960056\n",
      "Gradient Descent(474/499): loss=0.9912078777103073\n",
      "Gradient Descent(475/499): loss=0.9896182363099579\n",
      "Gradient Descent(476/499): loss=0.9880353922489685\n",
      "Gradient Descent(477/499): loss=0.9864593020415037\n",
      "Gradient Descent(478/499): loss=0.9848899225585616\n",
      "Gradient Descent(479/499): loss=0.9833272110246891\n",
      "Gradient Descent(480/499): loss=0.9817711250147239\n",
      "Gradient Descent(481/499): loss=0.9802216224505731\n",
      "Gradient Descent(482/499): loss=0.9786786615980193\n",
      "Gradient Descent(483/499): loss=0.9771422010635598\n",
      "Gradient Descent(484/499): loss=0.9756121997912736\n",
      "Gradient Descent(485/499): loss=0.9740886170597229\n",
      "Gradient Descent(486/499): loss=0.97257141247888\n",
      "Gradient Descent(487/499): loss=0.9710605459870879\n",
      "Gradient Descent(488/499): loss=0.9695559778480455\n",
      "Gradient Descent(489/499): loss=0.9680576686478277\n",
      "Gradient Descent(490/499): loss=0.9665655792919287\n",
      "Gradient Descent(491/499): loss=0.9650796710023379\n",
      "Gradient Descent(492/499): loss=0.963599905314642\n",
      "Gradient Descent(493/499): loss=0.9621262440751545\n",
      "Gradient Descent(494/499): loss=0.960658649438075\n",
      "Gradient Descent(495/499): loss=0.9591970838626747\n",
      "Gradient Descent(496/499): loss=0.9577415101105076\n",
      "Gradient Descent(497/499): loss=0.9562918912426512\n",
      "Gradient Descent(498/499): loss=0.9548481906169727\n",
      "Gradient Descent(499/499): loss=0.9534103718854202\n",
      "0.5706133333333333\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "(w1,loss1) = least_squares_GD(y, tX, init_w, max_iter, alpha)\n",
    "gd_tr_pred = predict_labels(w1, tX_valid)\n",
    "print((gd_tr_pred == y_valid).mean())\n",
    "gd_pred = predict_labels(w1, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/499): loss=3.795878208163386\n",
      "Stochastic Gradient Descent(1/499): loss=2.6374790211620356\n",
      "Stochastic Gradient Descent(2/499): loss=6.509059828082071\n",
      "Stochastic Gradient Descent(3/499): loss=0.7826615399884105\n",
      "Stochastic Gradient Descent(4/499): loss=0.7433766745127527\n",
      "Stochastic Gradient Descent(5/499): loss=0.04340089063805986\n",
      "Stochastic Gradient Descent(6/499): loss=0.21905506343642356\n",
      "Stochastic Gradient Descent(7/499): loss=3.9465675434189564\n",
      "Stochastic Gradient Descent(8/499): loss=0.5855835216768063\n",
      "Stochastic Gradient Descent(9/499): loss=0.30637844933008407\n",
      "Stochastic Gradient Descent(10/499): loss=6.26191817025045\n",
      "Stochastic Gradient Descent(11/499): loss=0.000829709223557333\n",
      "Stochastic Gradient Descent(12/499): loss=1.197703560002073\n",
      "Stochastic Gradient Descent(13/499): loss=0.004441963699677796\n",
      "Stochastic Gradient Descent(14/499): loss=0.02187769933466961\n",
      "Stochastic Gradient Descent(15/499): loss=0.0345454927422117\n",
      "Stochastic Gradient Descent(16/499): loss=2.343030868139861\n",
      "Stochastic Gradient Descent(17/499): loss=1.1230045519689515\n",
      "Stochastic Gradient Descent(18/499): loss=2.2393032489900175\n",
      "Stochastic Gradient Descent(19/499): loss=0.11845736367331591\n",
      "Stochastic Gradient Descent(20/499): loss=4.839193515522935\n",
      "Stochastic Gradient Descent(21/499): loss=2.9197749988729047\n",
      "Stochastic Gradient Descent(22/499): loss=7.307743525004857\n",
      "Stochastic Gradient Descent(23/499): loss=2.476593206537268\n",
      "Stochastic Gradient Descent(24/499): loss=0.1999908099633299\n",
      "Stochastic Gradient Descent(25/499): loss=13.227153392306464\n",
      "Stochastic Gradient Descent(26/499): loss=5.610533353062594\n",
      "Stochastic Gradient Descent(27/499): loss=0.7218532078921056\n",
      "Stochastic Gradient Descent(28/499): loss=0.24522327432154198\n",
      "Stochastic Gradient Descent(29/499): loss=1.0566696946527514\n",
      "Stochastic Gradient Descent(30/499): loss=0.4974102468793787\n",
      "Stochastic Gradient Descent(31/499): loss=0.4906419579019315\n",
      "Stochastic Gradient Descent(32/499): loss=11.437467653748264\n",
      "Stochastic Gradient Descent(33/499): loss=4.862436697637479\n",
      "Stochastic Gradient Descent(34/499): loss=0.13590099633816644\n",
      "Stochastic Gradient Descent(35/499): loss=0.724976353119916\n",
      "Stochastic Gradient Descent(36/499): loss=0.44863711249988064\n",
      "Stochastic Gradient Descent(37/499): loss=3.3300592238376225\n",
      "Stochastic Gradient Descent(38/499): loss=11.250861609917902\n",
      "Stochastic Gradient Descent(39/499): loss=7.044743970540705\n",
      "Stochastic Gradient Descent(40/499): loss=0.7407354070367287\n",
      "Stochastic Gradient Descent(41/499): loss=0.4489651788186034\n",
      "Stochastic Gradient Descent(42/499): loss=13.781418503991436\n",
      "Stochastic Gradient Descent(43/499): loss=6.4963596167286175\n",
      "Stochastic Gradient Descent(44/499): loss=1.0216352453940405\n",
      "Stochastic Gradient Descent(45/499): loss=0.44951696587894907\n",
      "Stochastic Gradient Descent(46/499): loss=1.1530172869554747\n",
      "Stochastic Gradient Descent(47/499): loss=1.3331569788871638\n",
      "Stochastic Gradient Descent(48/499): loss=7.076737922188288\n",
      "Stochastic Gradient Descent(49/499): loss=0.24954315197354102\n",
      "Stochastic Gradient Descent(50/499): loss=2.375147381938691\n",
      "Stochastic Gradient Descent(51/499): loss=0.056360562889758166\n",
      "Stochastic Gradient Descent(52/499): loss=1.7099781487821548\n",
      "Stochastic Gradient Descent(53/499): loss=42.173696032934174\n",
      "Stochastic Gradient Descent(54/499): loss=0.6466767177139904\n",
      "Stochastic Gradient Descent(55/499): loss=11.517195177283552\n",
      "Stochastic Gradient Descent(56/499): loss=1.1676019910764444\n",
      "Stochastic Gradient Descent(57/499): loss=1.1701314690940172\n",
      "Stochastic Gradient Descent(58/499): loss=2.9656667859805306\n",
      "Stochastic Gradient Descent(59/499): loss=2.120892273567778\n",
      "Stochastic Gradient Descent(60/499): loss=2.1757913886733857\n",
      "Stochastic Gradient Descent(61/499): loss=9.044331339146193\n",
      "Stochastic Gradient Descent(62/499): loss=0.11084335762197879\n",
      "Stochastic Gradient Descent(63/499): loss=6.059921727919101\n",
      "Stochastic Gradient Descent(64/499): loss=0.8213786726501874\n",
      "Stochastic Gradient Descent(65/499): loss=0.7182893314932168\n",
      "Stochastic Gradient Descent(66/499): loss=0.6307790763749306\n",
      "Stochastic Gradient Descent(67/499): loss=6.684954173377212\n",
      "Stochastic Gradient Descent(68/499): loss=4.426329235538299\n",
      "Stochastic Gradient Descent(69/499): loss=4.2057820652462246\n",
      "Stochastic Gradient Descent(70/499): loss=3.45202703713743\n",
      "Stochastic Gradient Descent(71/499): loss=17.429994216760317\n",
      "Stochastic Gradient Descent(72/499): loss=11.670702712243312\n",
      "Stochastic Gradient Descent(73/499): loss=3.3825811291968284\n",
      "Stochastic Gradient Descent(74/499): loss=0.12141321893391852\n",
      "Stochastic Gradient Descent(75/499): loss=2.2017733771450505\n",
      "Stochastic Gradient Descent(76/499): loss=12.905221235834691\n",
      "Stochastic Gradient Descent(77/499): loss=11.146059535141449\n",
      "Stochastic Gradient Descent(78/499): loss=0.37510671415624525\n",
      "Stochastic Gradient Descent(79/499): loss=1.26972647413205\n",
      "Stochastic Gradient Descent(80/499): loss=7.1460608000437915\n",
      "Stochastic Gradient Descent(81/499): loss=0.004193509789008176\n",
      "Stochastic Gradient Descent(82/499): loss=1.9687120002677565\n",
      "Stochastic Gradient Descent(83/499): loss=1.4578688126922712\n",
      "Stochastic Gradient Descent(84/499): loss=4.133233780834144\n",
      "Stochastic Gradient Descent(85/499): loss=10.193240368667405\n",
      "Stochastic Gradient Descent(86/499): loss=0.15615097849070075\n",
      "Stochastic Gradient Descent(87/499): loss=0.6187094846016143\n",
      "Stochastic Gradient Descent(88/499): loss=1.255523458979451\n",
      "Stochastic Gradient Descent(89/499): loss=0.022938740047037637\n",
      "Stochastic Gradient Descent(90/499): loss=2.5364303431700246\n",
      "Stochastic Gradient Descent(91/499): loss=4.976807571668754\n",
      "Stochastic Gradient Descent(92/499): loss=4.4691029542971865\n",
      "Stochastic Gradient Descent(93/499): loss=9.202737989640536\n",
      "Stochastic Gradient Descent(94/499): loss=2.3096603340959985\n",
      "Stochastic Gradient Descent(95/499): loss=0.7669045882991671\n",
      "Stochastic Gradient Descent(96/499): loss=2.1011393294133165\n",
      "Stochastic Gradient Descent(97/499): loss=4.81300516007069\n",
      "Stochastic Gradient Descent(98/499): loss=1.4587999734798085\n",
      "Stochastic Gradient Descent(99/499): loss=0.018074152053318394\n",
      "Stochastic Gradient Descent(100/499): loss=0.17012415758935695\n",
      "Stochastic Gradient Descent(101/499): loss=0.048881517068347376\n",
      "Stochastic Gradient Descent(102/499): loss=0.009274132203103053\n",
      "Stochastic Gradient Descent(103/499): loss=0.2300172763049461\n",
      "Stochastic Gradient Descent(104/499): loss=11.59113104265633\n",
      "Stochastic Gradient Descent(105/499): loss=3.990783038362015\n",
      "Stochastic Gradient Descent(106/499): loss=0.00047071608763077957\n",
      "Stochastic Gradient Descent(107/499): loss=0.30407119239056535\n",
      "Stochastic Gradient Descent(108/499): loss=2.1094902884504267\n",
      "Stochastic Gradient Descent(109/499): loss=4.455603801184729\n",
      "Stochastic Gradient Descent(110/499): loss=8.178891580064008\n",
      "Stochastic Gradient Descent(111/499): loss=1.06923185652555\n",
      "Stochastic Gradient Descent(112/499): loss=1.6343884404665048\n",
      "Stochastic Gradient Descent(113/499): loss=1.5136612501806719\n",
      "Stochastic Gradient Descent(114/499): loss=5.481430355048967\n",
      "Stochastic Gradient Descent(115/499): loss=0.22483167181284566\n",
      "Stochastic Gradient Descent(116/499): loss=0.7371807912080732\n",
      "Stochastic Gradient Descent(117/499): loss=0.22277393053933922\n",
      "Stochastic Gradient Descent(118/499): loss=0.47569265600432487\n",
      "Stochastic Gradient Descent(119/499): loss=2.2009829870816713\n",
      "Stochastic Gradient Descent(120/499): loss=0.0065383437820028\n",
      "Stochastic Gradient Descent(121/499): loss=0.3618854311711145\n",
      "Stochastic Gradient Descent(122/499): loss=5.472225807477312\n",
      "Stochastic Gradient Descent(123/499): loss=0.6674267647452243\n",
      "Stochastic Gradient Descent(124/499): loss=0.21586925428752116\n",
      "Stochastic Gradient Descent(125/499): loss=2.1081144722511196\n",
      "Stochastic Gradient Descent(126/499): loss=0.14801614893239629\n",
      "Stochastic Gradient Descent(127/499): loss=0.7759125364686048\n",
      "Stochastic Gradient Descent(128/499): loss=0.06459629056380384\n",
      "Stochastic Gradient Descent(129/499): loss=6.081427916925563\n",
      "Stochastic Gradient Descent(130/499): loss=1.6721801965226006\n",
      "Stochastic Gradient Descent(131/499): loss=3.0642282603196414\n",
      "Stochastic Gradient Descent(132/499): loss=0.46119758003907674\n",
      "Stochastic Gradient Descent(133/499): loss=3.9624762823880415\n",
      "Stochastic Gradient Descent(134/499): loss=2.574523195714755\n",
      "Stochastic Gradient Descent(135/499): loss=0.03946481915829395\n",
      "Stochastic Gradient Descent(136/499): loss=0.04147789403002288\n",
      "Stochastic Gradient Descent(137/499): loss=0.03987211281116622\n",
      "Stochastic Gradient Descent(138/499): loss=0.24886978442385616\n",
      "Stochastic Gradient Descent(139/499): loss=1.1919523003807622\n",
      "Stochastic Gradient Descent(140/499): loss=1.0081122423568354\n",
      "Stochastic Gradient Descent(141/499): loss=0.6535183863260968\n",
      "Stochastic Gradient Descent(142/499): loss=3.0695245627559697\n",
      "Stochastic Gradient Descent(143/499): loss=2.1972423823670203e-05\n",
      "Stochastic Gradient Descent(144/499): loss=1.1935367302412971\n",
      "Stochastic Gradient Descent(145/499): loss=0.30599060393010424\n",
      "Stochastic Gradient Descent(146/499): loss=7.034546491739809\n",
      "Stochastic Gradient Descent(147/499): loss=1.0029434373346158\n",
      "Stochastic Gradient Descent(148/499): loss=0.9224933172202305\n",
      "Stochastic Gradient Descent(149/499): loss=0.18074724394056232\n",
      "Stochastic Gradient Descent(150/499): loss=1.5366219394376657\n",
      "Stochastic Gradient Descent(151/499): loss=0.8773962051532233\n",
      "Stochastic Gradient Descent(152/499): loss=0.09389433144392269\n",
      "Stochastic Gradient Descent(153/499): loss=0.014315884674372127\n",
      "Stochastic Gradient Descent(154/499): loss=1.7353022089192807\n",
      "Stochastic Gradient Descent(155/499): loss=0.05736528169703155\n",
      "Stochastic Gradient Descent(156/499): loss=0.3828959226031101\n",
      "Stochastic Gradient Descent(157/499): loss=16.059977708142394\n",
      "Stochastic Gradient Descent(158/499): loss=0.9289646777146432\n",
      "Stochastic Gradient Descent(159/499): loss=0.7253620454886734\n",
      "Stochastic Gradient Descent(160/499): loss=0.9948690592005822\n",
      "Stochastic Gradient Descent(161/499): loss=0.28357994675089276\n",
      "Stochastic Gradient Descent(162/499): loss=0.4684171617707327\n",
      "Stochastic Gradient Descent(163/499): loss=4.591948092889535\n",
      "Stochastic Gradient Descent(164/499): loss=1.2069590227989753\n",
      "Stochastic Gradient Descent(165/499): loss=0.05970943378276847\n",
      "Stochastic Gradient Descent(166/499): loss=0.26520559777212566\n",
      "Stochastic Gradient Descent(167/499): loss=2.2803148977827004\n",
      "Stochastic Gradient Descent(168/499): loss=1.5177491887002328\n",
      "Stochastic Gradient Descent(169/499): loss=2.0927557726684247\n",
      "Stochastic Gradient Descent(170/499): loss=0.427217186294864\n",
      "Stochastic Gradient Descent(171/499): loss=0.027222018256632185\n",
      "Stochastic Gradient Descent(172/499): loss=0.5927877024991203\n",
      "Stochastic Gradient Descent(173/499): loss=0.08089612181655381\n",
      "Stochastic Gradient Descent(174/499): loss=0.006507019761374779\n",
      "Stochastic Gradient Descent(175/499): loss=2.91979509084295\n",
      "Stochastic Gradient Descent(176/499): loss=0.12972389251769545\n",
      "Stochastic Gradient Descent(177/499): loss=0.9165492601397909\n",
      "Stochastic Gradient Descent(178/499): loss=2.07494834247033\n",
      "Stochastic Gradient Descent(179/499): loss=2.700693912696588\n",
      "Stochastic Gradient Descent(180/499): loss=2.6196395318655146\n",
      "Stochastic Gradient Descent(181/499): loss=0.007583535289226205\n",
      "Stochastic Gradient Descent(182/499): loss=0.001973308916250847\n",
      "Stochastic Gradient Descent(183/499): loss=7.013299480632389\n",
      "Stochastic Gradient Descent(184/499): loss=0.1345005771697269\n",
      "Stochastic Gradient Descent(185/499): loss=1.1644199701846736\n",
      "Stochastic Gradient Descent(186/499): loss=0.12108562191615316\n",
      "Stochastic Gradient Descent(187/499): loss=0.14041501480402277\n",
      "Stochastic Gradient Descent(188/499): loss=1.7379636418574322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(189/499): loss=0.09047586194203665\n",
      "Stochastic Gradient Descent(190/499): loss=0.30544297794936937\n",
      "Stochastic Gradient Descent(191/499): loss=1.562331584682478\n",
      "Stochastic Gradient Descent(192/499): loss=3.961263861079952\n",
      "Stochastic Gradient Descent(193/499): loss=0.5392692496039444\n",
      "Stochastic Gradient Descent(194/499): loss=5.474143581555345\n",
      "Stochastic Gradient Descent(195/499): loss=0.02681348343490159\n",
      "Stochastic Gradient Descent(196/499): loss=0.41918278761103617\n",
      "Stochastic Gradient Descent(197/499): loss=7.582057298118478\n",
      "Stochastic Gradient Descent(198/499): loss=1.9120897129663332\n",
      "Stochastic Gradient Descent(199/499): loss=0.7844112603459871\n",
      "Stochastic Gradient Descent(200/499): loss=0.3634589423505502\n",
      "Stochastic Gradient Descent(201/499): loss=0.009043146822448472\n",
      "Stochastic Gradient Descent(202/499): loss=2.82316406831756\n",
      "Stochastic Gradient Descent(203/499): loss=2.001715646437378\n",
      "Stochastic Gradient Descent(204/499): loss=0.03376160297809106\n",
      "Stochastic Gradient Descent(205/499): loss=1.2784648155351388\n",
      "Stochastic Gradient Descent(206/499): loss=0.7413376714889857\n",
      "Stochastic Gradient Descent(207/499): loss=5.096996448220005\n",
      "Stochastic Gradient Descent(208/499): loss=0.27458904041170173\n",
      "Stochastic Gradient Descent(209/499): loss=2.066210596499864\n",
      "Stochastic Gradient Descent(210/499): loss=0.5270509149010711\n",
      "Stochastic Gradient Descent(211/499): loss=3.535098228620534\n",
      "Stochastic Gradient Descent(212/499): loss=6.1026531107846615\n",
      "Stochastic Gradient Descent(213/499): loss=1.3963542523396992\n",
      "Stochastic Gradient Descent(214/499): loss=3.545756373000725\n",
      "Stochastic Gradient Descent(215/499): loss=14.168579739729726\n",
      "Stochastic Gradient Descent(216/499): loss=7.015737425719918\n",
      "Stochastic Gradient Descent(217/499): loss=1.2577233947331046\n",
      "Stochastic Gradient Descent(218/499): loss=0.837906182322011\n",
      "Stochastic Gradient Descent(219/499): loss=5.187459615732186\n",
      "Stochastic Gradient Descent(220/499): loss=0.12248846923519394\n",
      "Stochastic Gradient Descent(221/499): loss=24.12390422504075\n",
      "Stochastic Gradient Descent(222/499): loss=1.832226389594433\n",
      "Stochastic Gradient Descent(223/499): loss=0.2633382194691804\n",
      "Stochastic Gradient Descent(224/499): loss=0.30537176210749045\n",
      "Stochastic Gradient Descent(225/499): loss=0.4032529983641845\n",
      "Stochastic Gradient Descent(226/499): loss=3.7033274681715738\n",
      "Stochastic Gradient Descent(227/499): loss=1.184808505725282\n",
      "Stochastic Gradient Descent(228/499): loss=0.6078480749482249\n",
      "Stochastic Gradient Descent(229/499): loss=0.120274455834762\n",
      "Stochastic Gradient Descent(230/499): loss=16.60179448101652\n",
      "Stochastic Gradient Descent(231/499): loss=3.0272577842003074\n",
      "Stochastic Gradient Descent(232/499): loss=0.20903430313220195\n",
      "Stochastic Gradient Descent(233/499): loss=0.089434650018952\n",
      "Stochastic Gradient Descent(234/499): loss=2.132845412073916\n",
      "Stochastic Gradient Descent(235/499): loss=6.132407582950674\n",
      "Stochastic Gradient Descent(236/499): loss=0.04405128249326118\n",
      "Stochastic Gradient Descent(237/499): loss=3.8867106717752056\n",
      "Stochastic Gradient Descent(238/499): loss=0.12487342421092085\n",
      "Stochastic Gradient Descent(239/499): loss=0.9858432222951553\n",
      "Stochastic Gradient Descent(240/499): loss=4.779808410221111\n",
      "Stochastic Gradient Descent(241/499): loss=0.026887790090098736\n",
      "Stochastic Gradient Descent(242/499): loss=3.9399311997226043\n",
      "Stochastic Gradient Descent(243/499): loss=0.4605545112459866\n",
      "Stochastic Gradient Descent(244/499): loss=1.1102509871718373\n",
      "Stochastic Gradient Descent(245/499): loss=1.349200680475207\n",
      "Stochastic Gradient Descent(246/499): loss=0.0415768278840969\n",
      "Stochastic Gradient Descent(247/499): loss=0.9839526124904013\n",
      "Stochastic Gradient Descent(248/499): loss=0.08332663888282923\n",
      "Stochastic Gradient Descent(249/499): loss=0.5189472529098975\n",
      "Stochastic Gradient Descent(250/499): loss=0.26131103058771\n",
      "Stochastic Gradient Descent(251/499): loss=0.7729659678602574\n",
      "Stochastic Gradient Descent(252/499): loss=0.12403973008270838\n",
      "Stochastic Gradient Descent(253/499): loss=1.9100933415843708\n",
      "Stochastic Gradient Descent(254/499): loss=0.3209432287619443\n",
      "Stochastic Gradient Descent(255/499): loss=0.0003733874407564133\n",
      "Stochastic Gradient Descent(256/499): loss=0.3326832172549717\n",
      "Stochastic Gradient Descent(257/499): loss=0.00017902777250550784\n",
      "Stochastic Gradient Descent(258/499): loss=1.0688075214922172\n",
      "Stochastic Gradient Descent(259/499): loss=0.4191924119781439\n",
      "Stochastic Gradient Descent(260/499): loss=2.083477137078603\n",
      "Stochastic Gradient Descent(261/499): loss=2.5838147060058176\n",
      "Stochastic Gradient Descent(262/499): loss=12.255700391837404\n",
      "Stochastic Gradient Descent(263/499): loss=0.023625834805932635\n",
      "Stochastic Gradient Descent(264/499): loss=0.30077512109822646\n",
      "Stochastic Gradient Descent(265/499): loss=0.07476942977544587\n",
      "Stochastic Gradient Descent(266/499): loss=0.1681921637386968\n",
      "Stochastic Gradient Descent(267/499): loss=8.546118827020086\n",
      "Stochastic Gradient Descent(268/499): loss=4.548940135511771\n",
      "Stochastic Gradient Descent(269/499): loss=3.21168289189358\n",
      "Stochastic Gradient Descent(270/499): loss=2.357918003630567\n",
      "Stochastic Gradient Descent(271/499): loss=0.16363218740204333\n",
      "Stochastic Gradient Descent(272/499): loss=1.439232682482877\n",
      "Stochastic Gradient Descent(273/499): loss=0.7992261540184469\n",
      "Stochastic Gradient Descent(274/499): loss=1.3903844949601802\n",
      "Stochastic Gradient Descent(275/499): loss=2.4193282782368333\n",
      "Stochastic Gradient Descent(276/499): loss=2.6135462675060976\n",
      "Stochastic Gradient Descent(277/499): loss=2.146403083772291\n",
      "Stochastic Gradient Descent(278/499): loss=3.6097543939293626\n",
      "Stochastic Gradient Descent(279/499): loss=0.6143205726856946\n",
      "Stochastic Gradient Descent(280/499): loss=0.030257205266070487\n",
      "Stochastic Gradient Descent(281/499): loss=0.38313873089905526\n",
      "Stochastic Gradient Descent(282/499): loss=0.8726175651113296\n",
      "Stochastic Gradient Descent(283/499): loss=2.469757338793675\n",
      "Stochastic Gradient Descent(284/499): loss=0.003915283905994726\n",
      "Stochastic Gradient Descent(285/499): loss=0.3299623482045209\n",
      "Stochastic Gradient Descent(286/499): loss=0.3540511958246084\n",
      "Stochastic Gradient Descent(287/499): loss=0.6373952908401643\n",
      "Stochastic Gradient Descent(288/499): loss=0.492494677060029\n",
      "Stochastic Gradient Descent(289/499): loss=0.062389762846499046\n",
      "Stochastic Gradient Descent(290/499): loss=0.03281311961401656\n",
      "Stochastic Gradient Descent(291/499): loss=0.6292066467187234\n",
      "Stochastic Gradient Descent(292/499): loss=0.1350760045036576\n",
      "Stochastic Gradient Descent(293/499): loss=3.2883676719351786\n",
      "Stochastic Gradient Descent(294/499): loss=0.35252048779058637\n",
      "Stochastic Gradient Descent(295/499): loss=0.7544343842686406\n",
      "Stochastic Gradient Descent(296/499): loss=2.9618246428425805\n",
      "Stochastic Gradient Descent(297/499): loss=1.6199147409879184\n",
      "Stochastic Gradient Descent(298/499): loss=4.266792413179054\n",
      "Stochastic Gradient Descent(299/499): loss=5.14120856322734\n",
      "Stochastic Gradient Descent(300/499): loss=0.12263380996397513\n",
      "Stochastic Gradient Descent(301/499): loss=0.9763932137869996\n",
      "Stochastic Gradient Descent(302/499): loss=1.804872442465024\n",
      "Stochastic Gradient Descent(303/499): loss=0.2678884616665759\n",
      "Stochastic Gradient Descent(304/499): loss=0.1788185141069402\n",
      "Stochastic Gradient Descent(305/499): loss=0.0005706744044325622\n",
      "Stochastic Gradient Descent(306/499): loss=0.2737925575412518\n",
      "Stochastic Gradient Descent(307/499): loss=0.0043297704151623595\n",
      "Stochastic Gradient Descent(308/499): loss=2.3407903418675584\n",
      "Stochastic Gradient Descent(309/499): loss=0.47462847661495206\n",
      "Stochastic Gradient Descent(310/499): loss=0.119474278952037\n",
      "Stochastic Gradient Descent(311/499): loss=0.4118680352683658\n",
      "Stochastic Gradient Descent(312/499): loss=11.898808857890172\n",
      "Stochastic Gradient Descent(313/499): loss=2.2460642194665508\n",
      "Stochastic Gradient Descent(314/499): loss=1.16923535722553\n",
      "Stochastic Gradient Descent(315/499): loss=1.6762564053780493\n",
      "Stochastic Gradient Descent(316/499): loss=1.4859008518734895\n",
      "Stochastic Gradient Descent(317/499): loss=1.729256187165194\n",
      "Stochastic Gradient Descent(318/499): loss=1.5244880857508456\n",
      "Stochastic Gradient Descent(319/499): loss=3.0329120559113414\n",
      "Stochastic Gradient Descent(320/499): loss=0.7990992474603629\n",
      "Stochastic Gradient Descent(321/499): loss=3.7261595318909917\n",
      "Stochastic Gradient Descent(322/499): loss=4.715908250069094\n",
      "Stochastic Gradient Descent(323/499): loss=1.396462109639986\n",
      "Stochastic Gradient Descent(324/499): loss=0.7577810272298873\n",
      "Stochastic Gradient Descent(325/499): loss=5.936083084392885\n",
      "Stochastic Gradient Descent(326/499): loss=0.7873605492461546\n",
      "Stochastic Gradient Descent(327/499): loss=0.0009456777899951197\n",
      "Stochastic Gradient Descent(328/499): loss=0.11658643192297516\n",
      "Stochastic Gradient Descent(329/499): loss=0.2644748922342417\n",
      "Stochastic Gradient Descent(330/499): loss=0.016550561288205583\n",
      "Stochastic Gradient Descent(331/499): loss=0.0034922999230619593\n",
      "Stochastic Gradient Descent(332/499): loss=0.8418771500822697\n",
      "Stochastic Gradient Descent(333/499): loss=0.006214319944553202\n",
      "Stochastic Gradient Descent(334/499): loss=1.8330411680657517\n",
      "Stochastic Gradient Descent(335/499): loss=0.960634072206509\n",
      "Stochastic Gradient Descent(336/499): loss=0.042221043341313436\n",
      "Stochastic Gradient Descent(337/499): loss=0.00044187365907647424\n",
      "Stochastic Gradient Descent(338/499): loss=0.1907466619237176\n",
      "Stochastic Gradient Descent(339/499): loss=0.4641389053155975\n",
      "Stochastic Gradient Descent(340/499): loss=1.1654608352063665\n",
      "Stochastic Gradient Descent(341/499): loss=4.24035023962538\n",
      "Stochastic Gradient Descent(342/499): loss=0.9503061346827024\n",
      "Stochastic Gradient Descent(343/499): loss=0.23436084842598418\n",
      "Stochastic Gradient Descent(344/499): loss=1.3869832067161\n",
      "Stochastic Gradient Descent(345/499): loss=0.11769496679023643\n",
      "Stochastic Gradient Descent(346/499): loss=6.221717428828828\n",
      "Stochastic Gradient Descent(347/499): loss=0.0019311919205725696\n",
      "Stochastic Gradient Descent(348/499): loss=0.10175006806607381\n",
      "Stochastic Gradient Descent(349/499): loss=4.664662979093412\n",
      "Stochastic Gradient Descent(350/499): loss=0.9649068989829169\n",
      "Stochastic Gradient Descent(351/499): loss=0.2679664917022689\n",
      "Stochastic Gradient Descent(352/499): loss=1.3116984536685037\n",
      "Stochastic Gradient Descent(353/499): loss=0.45443872147953907\n",
      "Stochastic Gradient Descent(354/499): loss=0.22252431271728484\n",
      "Stochastic Gradient Descent(355/499): loss=0.659270747705488\n",
      "Stochastic Gradient Descent(356/499): loss=0.021592189788752554\n",
      "Stochastic Gradient Descent(357/499): loss=3.8537724175608052\n",
      "Stochastic Gradient Descent(358/499): loss=0.09146727866452875\n",
      "Stochastic Gradient Descent(359/499): loss=0.5239533489414473\n",
      "Stochastic Gradient Descent(360/499): loss=6.081033135391588\n",
      "Stochastic Gradient Descent(361/499): loss=0.6366790532111395\n",
      "Stochastic Gradient Descent(362/499): loss=0.2269189034983216\n",
      "Stochastic Gradient Descent(363/499): loss=0.6529463415233336\n",
      "Stochastic Gradient Descent(364/499): loss=2.2174740587674187\n",
      "Stochastic Gradient Descent(365/499): loss=0.03351968521506022\n",
      "Stochastic Gradient Descent(366/499): loss=3.663421813826595e-05\n",
      "Stochastic Gradient Descent(367/499): loss=0.41405511765598063\n",
      "Stochastic Gradient Descent(368/499): loss=5.99768763516133\n",
      "Stochastic Gradient Descent(369/499): loss=0.7888888297592938\n",
      "Stochastic Gradient Descent(370/499): loss=3.333169128484671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(371/499): loss=4.129998961337698\n",
      "Stochastic Gradient Descent(372/499): loss=0.21895542752228417\n",
      "Stochastic Gradient Descent(373/499): loss=0.5579365478321014\n",
      "Stochastic Gradient Descent(374/499): loss=1.4162338679046143\n",
      "Stochastic Gradient Descent(375/499): loss=1.3188224190441908\n",
      "Stochastic Gradient Descent(376/499): loss=0.06849064735425243\n",
      "Stochastic Gradient Descent(377/499): loss=1.012794227660733\n",
      "Stochastic Gradient Descent(378/499): loss=0.679831884552422\n",
      "Stochastic Gradient Descent(379/499): loss=0.023914076460110872\n",
      "Stochastic Gradient Descent(380/499): loss=1.6591413741560075\n",
      "Stochastic Gradient Descent(381/499): loss=0.001973054160258261\n",
      "Stochastic Gradient Descent(382/499): loss=0.29979837295731854\n",
      "Stochastic Gradient Descent(383/499): loss=0.5168830349194496\n",
      "Stochastic Gradient Descent(384/499): loss=1.7833584482705604\n",
      "Stochastic Gradient Descent(385/499): loss=3.955522364358633e-06\n",
      "Stochastic Gradient Descent(386/499): loss=0.09215338607120278\n",
      "Stochastic Gradient Descent(387/499): loss=0.3059955810052583\n",
      "Stochastic Gradient Descent(388/499): loss=0.0467913307800874\n",
      "Stochastic Gradient Descent(389/499): loss=0.04726597771870666\n",
      "Stochastic Gradient Descent(390/499): loss=2.161931188174104\n",
      "Stochastic Gradient Descent(391/499): loss=0.12182112030200032\n",
      "Stochastic Gradient Descent(392/499): loss=0.7033002800571276\n",
      "Stochastic Gradient Descent(393/499): loss=4.4383705458369365\n",
      "Stochastic Gradient Descent(394/499): loss=1.5971764772739232\n",
      "Stochastic Gradient Descent(395/499): loss=0.5602375792490117\n",
      "Stochastic Gradient Descent(396/499): loss=0.5549180374678347\n",
      "Stochastic Gradient Descent(397/499): loss=6.7093624405491115\n",
      "Stochastic Gradient Descent(398/499): loss=4.727050726223403\n",
      "Stochastic Gradient Descent(399/499): loss=0.011907969962159584\n",
      "Stochastic Gradient Descent(400/499): loss=0.14942918288903737\n",
      "Stochastic Gradient Descent(401/499): loss=0.06888933631122647\n",
      "Stochastic Gradient Descent(402/499): loss=0.049174326508073775\n",
      "Stochastic Gradient Descent(403/499): loss=0.956760673183069\n",
      "Stochastic Gradient Descent(404/499): loss=1.0496893491732675\n",
      "Stochastic Gradient Descent(405/499): loss=1.4939394064836273\n",
      "Stochastic Gradient Descent(406/499): loss=0.6206990562465495\n",
      "Stochastic Gradient Descent(407/499): loss=5.860626071546041\n",
      "Stochastic Gradient Descent(408/499): loss=1.524856180050894\n",
      "Stochastic Gradient Descent(409/499): loss=0.7288119858796145\n",
      "Stochastic Gradient Descent(410/499): loss=1.13627734344626\n",
      "Stochastic Gradient Descent(411/499): loss=1.0392284621078678\n",
      "Stochastic Gradient Descent(412/499): loss=5.74818073016815\n",
      "Stochastic Gradient Descent(413/499): loss=2.0123580922169224\n",
      "Stochastic Gradient Descent(414/499): loss=2.432406518646785\n",
      "Stochastic Gradient Descent(415/499): loss=0.11947848129076101\n",
      "Stochastic Gradient Descent(416/499): loss=2.901513963921673\n",
      "Stochastic Gradient Descent(417/499): loss=1.5601803751215515\n",
      "Stochastic Gradient Descent(418/499): loss=4.524545827136299\n",
      "Stochastic Gradient Descent(419/499): loss=0.7848581395675577\n",
      "Stochastic Gradient Descent(420/499): loss=1.068276624395839\n",
      "Stochastic Gradient Descent(421/499): loss=0.04788219806740459\n",
      "Stochastic Gradient Descent(422/499): loss=4.123360162055499\n",
      "Stochastic Gradient Descent(423/499): loss=0.877402127334903\n",
      "Stochastic Gradient Descent(424/499): loss=2.376701771101039\n",
      "Stochastic Gradient Descent(425/499): loss=2.3063779307497625\n",
      "Stochastic Gradient Descent(426/499): loss=0.03030544932711423\n",
      "Stochastic Gradient Descent(427/499): loss=1.5434482024325706\n",
      "Stochastic Gradient Descent(428/499): loss=3.8093852324684496\n",
      "Stochastic Gradient Descent(429/499): loss=0.08400215123492785\n",
      "Stochastic Gradient Descent(430/499): loss=0.23940412032773128\n",
      "Stochastic Gradient Descent(431/499): loss=0.47762067411297116\n",
      "Stochastic Gradient Descent(432/499): loss=2.6886668761792896\n",
      "Stochastic Gradient Descent(433/499): loss=0.17613168668064227\n",
      "Stochastic Gradient Descent(434/499): loss=0.022405433779896386\n",
      "Stochastic Gradient Descent(435/499): loss=0.908334476130858\n",
      "Stochastic Gradient Descent(436/499): loss=2.435354869794783\n",
      "Stochastic Gradient Descent(437/499): loss=0.5163440686120603\n",
      "Stochastic Gradient Descent(438/499): loss=0.016930694202121264\n",
      "Stochastic Gradient Descent(439/499): loss=0.027913641531659334\n",
      "Stochastic Gradient Descent(440/499): loss=0.3498249714503986\n",
      "Stochastic Gradient Descent(441/499): loss=0.09664179323477662\n",
      "Stochastic Gradient Descent(442/499): loss=1.715510193623888\n",
      "Stochastic Gradient Descent(443/499): loss=0.00451135988961137\n",
      "Stochastic Gradient Descent(444/499): loss=2.593649663852326\n",
      "Stochastic Gradient Descent(445/499): loss=1.4870071659333128\n",
      "Stochastic Gradient Descent(446/499): loss=9.47085981977328\n",
      "Stochastic Gradient Descent(447/499): loss=0.9561209495994055\n",
      "Stochastic Gradient Descent(448/499): loss=0.020393953272249892\n",
      "Stochastic Gradient Descent(449/499): loss=1.5637596188800926\n",
      "Stochastic Gradient Descent(450/499): loss=0.7093187876740914\n",
      "Stochastic Gradient Descent(451/499): loss=1.3155202519436222\n",
      "Stochastic Gradient Descent(452/499): loss=0.04131274096767904\n",
      "Stochastic Gradient Descent(453/499): loss=0.7981837792507961\n",
      "Stochastic Gradient Descent(454/499): loss=1.1153039370318518\n",
      "Stochastic Gradient Descent(455/499): loss=3.4102999015944695\n",
      "Stochastic Gradient Descent(456/499): loss=1.2067601992629367\n",
      "Stochastic Gradient Descent(457/499): loss=0.6519738954589817\n",
      "Stochastic Gradient Descent(458/499): loss=0.23570522485539055\n",
      "Stochastic Gradient Descent(459/499): loss=0.6409985337187927\n",
      "Stochastic Gradient Descent(460/499): loss=0.8154201230128612\n",
      "Stochastic Gradient Descent(461/499): loss=3.8624027312342406\n",
      "Stochastic Gradient Descent(462/499): loss=0.006734842814509496\n",
      "Stochastic Gradient Descent(463/499): loss=0.6719347220616969\n",
      "Stochastic Gradient Descent(464/499): loss=1.036302726705472\n",
      "Stochastic Gradient Descent(465/499): loss=5.314043729512111\n",
      "Stochastic Gradient Descent(466/499): loss=0.9757170984600736\n",
      "Stochastic Gradient Descent(467/499): loss=0.08321612606019019\n",
      "Stochastic Gradient Descent(468/499): loss=0.9958063437748725\n",
      "Stochastic Gradient Descent(469/499): loss=0.0914470402949005\n",
      "Stochastic Gradient Descent(470/499): loss=0.0018672253950900084\n",
      "Stochastic Gradient Descent(471/499): loss=0.11676154854289512\n",
      "Stochastic Gradient Descent(472/499): loss=7.363138316796389\n",
      "Stochastic Gradient Descent(473/499): loss=0.5822562183419442\n",
      "Stochastic Gradient Descent(474/499): loss=0.08839361476391129\n",
      "Stochastic Gradient Descent(475/499): loss=1.1616063495586118\n",
      "Stochastic Gradient Descent(476/499): loss=0.09859146976882731\n",
      "Stochastic Gradient Descent(477/499): loss=2.444997192900402\n",
      "Stochastic Gradient Descent(478/499): loss=0.02002994579324086\n",
      "Stochastic Gradient Descent(479/499): loss=8.388472324325873\n",
      "Stochastic Gradient Descent(480/499): loss=0.012820029649276398\n",
      "Stochastic Gradient Descent(481/499): loss=0.7544776884282983\n",
      "Stochastic Gradient Descent(482/499): loss=0.016805545160901634\n",
      "Stochastic Gradient Descent(483/499): loss=0.7885133846206209\n",
      "Stochastic Gradient Descent(484/499): loss=0.5343104338123235\n",
      "Stochastic Gradient Descent(485/499): loss=0.19746264540036415\n",
      "Stochastic Gradient Descent(486/499): loss=0.5010266925555292\n",
      "Stochastic Gradient Descent(487/499): loss=0.7245450599863604\n",
      "Stochastic Gradient Descent(488/499): loss=0.06924902316746658\n",
      "Stochastic Gradient Descent(489/499): loss=1.473509918998566\n",
      "Stochastic Gradient Descent(490/499): loss=0.17877247233486462\n",
      "Stochastic Gradient Descent(491/499): loss=0.02489127822478571\n",
      "Stochastic Gradient Descent(492/499): loss=0.012505571032017276\n",
      "Stochastic Gradient Descent(493/499): loss=0.39766522008272714\n",
      "Stochastic Gradient Descent(494/499): loss=4.883141122660772\n",
      "Stochastic Gradient Descent(495/499): loss=0.835647188886522\n",
      "Stochastic Gradient Descent(496/499): loss=0.00045395748203101244\n",
      "Stochastic Gradient Descent(497/499): loss=0.46131994254766207\n",
      "Stochastic Gradient Descent(498/499): loss=0.5588752073948399\n",
      "Stochastic Gradient Descent(499/499): loss=0.7266602188581616\n",
      "0.5594666666666667\n"
     ]
    }
   ],
   "source": [
    "'''SGD'''\n",
    "(w2,loss2) = least_squares_SGD(y, tX, init_w, max_iter, alpha)\n",
    "sgd_tr_pred = predict_labels(w2, tX_valid)\n",
    "print((sgd_tr_pred == y_valid).mean())\n",
    "sgd_pred = predict_labels(w2, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "(w3,loss3) = least_squares(y, tX)\n",
    "ls_tr_pred = predict_labels(w3, tX_valid)\n",
    "print((ls_tr_pred == y_valid).mean())\n",
    "ls_pred = predict_labels(w3, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "lambda_ = np.logspace(-1, -6, 30)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        min_loss = loss4\n",
    "        ind = i\n",
    "(w4,loss4) = ridge_regression(y, tX, lambda_[ind])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())\n",
    "rd_pred = predict_labels(w4, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Gradient Descent(0/499): loss=nan\n",
      "Logistic Regression Gradient Descent(1/499): loss=nan\n",
      "Logistic Regression Gradient Descent(2/499): loss=nan\n",
      "Logistic Regression Gradient Descent(3/499): loss=nan\n",
      "Logistic Regression Gradient Descent(4/499): loss=nan\n",
      "Logistic Regression Gradient Descent(5/499): loss=nan\n",
      "Logistic Regression Gradient Descent(6/499): loss=nan\n",
      "Logistic Regression Gradient Descent(7/499): loss=nan\n",
      "Logistic Regression Gradient Descent(8/499): loss=nan\n",
      "Logistic Regression Gradient Descent(9/499): loss=nan\n",
      "Logistic Regression Gradient Descent(10/499): loss=nan\n",
      "Logistic Regression Gradient Descent(11/499): loss=nan\n",
      "Logistic Regression Gradient Descent(12/499): loss=nan\n",
      "Logistic Regression Gradient Descent(13/499): loss=nan\n",
      "Logistic Regression Gradient Descent(14/499): loss=nan\n",
      "Logistic Regression Gradient Descent(15/499): loss=nan\n",
      "Logistic Regression Gradient Descent(16/499): loss=nan\n",
      "Logistic Regression Gradient Descent(17/499): loss=nan\n",
      "Logistic Regression Gradient Descent(18/499): loss=nan\n",
      "Logistic Regression Gradient Descent(19/499): loss=nan\n",
      "Logistic Regression Gradient Descent(20/499): loss=nan\n",
      "Logistic Regression Gradient Descent(21/499): loss=nan\n",
      "Logistic Regression Gradient Descent(22/499): loss=nan\n",
      "Logistic Regression Gradient Descent(23/499): loss=nan\n",
      "Logistic Regression Gradient Descent(24/499): loss=nan\n",
      "Logistic Regression Gradient Descent(25/499): loss=nan\n",
      "Logistic Regression Gradient Descent(26/499): loss=nan\n",
      "Logistic Regression Gradient Descent(27/499): loss=nan\n",
      "Logistic Regression Gradient Descent(28/499): loss=nan\n",
      "Logistic Regression Gradient Descent(29/499): loss=nan\n",
      "Logistic Regression Gradient Descent(30/499): loss=nan\n",
      "Logistic Regression Gradient Descent(31/499): loss=nan\n",
      "Logistic Regression Gradient Descent(32/499): loss=nan\n",
      "Logistic Regression Gradient Descent(33/499): loss=nan\n",
      "Logistic Regression Gradient Descent(34/499): loss=nan\n",
      "Logistic Regression Gradient Descent(35/499): loss=nan\n",
      "Logistic Regression Gradient Descent(36/499): loss=nan\n",
      "Logistic Regression Gradient Descent(37/499): loss=nan\n",
      "Logistic Regression Gradient Descent(38/499): loss=nan\n",
      "Logistic Regression Gradient Descent(39/499): loss=nan\n",
      "Logistic Regression Gradient Descent(40/499): loss=nan\n",
      "Logistic Regression Gradient Descent(41/499): loss=nan\n",
      "Logistic Regression Gradient Descent(42/499): loss=nan\n",
      "Logistic Regression Gradient Descent(43/499): loss=nan\n",
      "Logistic Regression Gradient Descent(44/499): loss=nan\n",
      "Logistic Regression Gradient Descent(45/499): loss=nan\n",
      "Logistic Regression Gradient Descent(46/499): loss=nan\n",
      "Logistic Regression Gradient Descent(47/499): loss=nan\n",
      "Logistic Regression Gradient Descent(48/499): loss=nan\n",
      "Logistic Regression Gradient Descent(49/499): loss=nan\n",
      "Logistic Regression Gradient Descent(50/499): loss=nan\n",
      "Logistic Regression Gradient Descent(51/499): loss=nan\n",
      "Logistic Regression Gradient Descent(52/499): loss=nan\n",
      "Logistic Regression Gradient Descent(53/499): loss=nan\n",
      "Logistic Regression Gradient Descent(54/499): loss=nan\n",
      "Logistic Regression Gradient Descent(55/499): loss=nan\n",
      "Logistic Regression Gradient Descent(56/499): loss=nan\n",
      "Logistic Regression Gradient Descent(57/499): loss=nan\n",
      "Logistic Regression Gradient Descent(58/499): loss=nan\n",
      "Logistic Regression Gradient Descent(59/499): loss=nan\n",
      "Logistic Regression Gradient Descent(60/499): loss=nan\n",
      "Logistic Regression Gradient Descent(61/499): loss=nan\n",
      "Logistic Regression Gradient Descent(62/499): loss=nan\n",
      "Logistic Regression Gradient Descent(63/499): loss=nan\n",
      "Logistic Regression Gradient Descent(64/499): loss=nan\n",
      "Logistic Regression Gradient Descent(65/499): loss=nan\n",
      "Logistic Regression Gradient Descent(66/499): loss=nan\n",
      "Logistic Regression Gradient Descent(67/499): loss=-0.6155258703803754\n",
      "Logistic Regression Gradient Descent(68/499): loss=-0.613809423783734\n",
      "Logistic Regression Gradient Descent(69/499): loss=-0.6121253703026767\n",
      "Logistic Regression Gradient Descent(70/499): loss=-0.6104732129199277\n",
      "Logistic Regression Gradient Descent(71/499): loss=-0.6088524683165466\n",
      "Logistic Regression Gradient Descent(72/499): loss=-0.6072626649609417\n",
      "Logistic Regression Gradient Descent(73/499): loss=-0.6057033414584811\n",
      "Logistic Regression Gradient Descent(74/499): loss=-0.6041740451331769\n",
      "Logistic Regression Gradient Descent(75/499): loss=-0.6026743308156307\n",
      "Logistic Regression Gradient Descent(76/499): loss=-0.6012037598139065\n",
      "Logistic Regression Gradient Descent(77/499): loss=-0.5997618990462578\n",
      "Logistic Regression Gradient Descent(78/499): loss=-0.598348320316706\n",
      "Logistic Regression Gradient Descent(79/499): loss=-0.5969625997163464\n",
      "Logistic Regression Gradient Descent(80/499): loss=-0.5956043171349837\n",
      "Logistic Regression Gradient Descent(81/499): loss=-0.5942730558692539\n",
      "Logistic Regression Gradient Descent(82/499): loss=-0.5929684023148171\n",
      "Logistic Regression Gradient Descent(83/499): loss=-0.5916899457314986\n",
      "Logistic Regression Gradient Descent(84/499): loss=-0.5904372780714223\n",
      "Logistic Regression Gradient Descent(85/499): loss=-0.5892099938612532\n",
      "Logistic Regression Gradient Descent(86/499): loss=-0.5880076901306202\n",
      "Logistic Regression Gradient Descent(87/499): loss=-0.5868299663796702\n",
      "Logistic Regression Gradient Descent(88/499): loss=-0.585676424579489\n",
      "Logistic Regression Gradient Descent(89/499): loss=-0.5845466691998398\n",
      "Logistic Regression Gradient Descent(90/499): loss=-0.583440307259312\n",
      "Logistic Regression Gradient Descent(91/499): loss=-0.5823569483935555\n",
      "Logistic Regression Gradient Descent(92/499): loss=-0.5812962049377975\n",
      "Logistic Regression Gradient Descent(93/499): loss=-0.5802576920203139\n",
      "Logistic Regression Gradient Descent(94/499): loss=-0.5792410276639429\n",
      "Logistic Regression Gradient Descent(95/499): loss=-0.5782458328931209\n",
      "Logistic Regression Gradient Descent(96/499): loss=-0.5772717318442522\n",
      "Logistic Regression Gradient Descent(97/499): loss=-0.5763183518775375\n",
      "Logistic Regression Gradient Descent(98/499): loss=-0.5753853236886552\n",
      "Logistic Regression Gradient Descent(99/499): loss=-0.5744722814189376\n",
      "Logistic Regression Gradient Descent(100/499): loss=-0.5735788627628996\n",
      "Logistic Regression Gradient Descent(101/499): loss=-0.5727047090721735\n",
      "Logistic Regression Gradient Descent(102/499): loss=-0.5718494654550718\n",
      "Logistic Regression Gradient Descent(103/499): loss=-0.5710127808711569\n",
      "Logistic Regression Gradient Descent(104/499): loss=-0.5701943082203238\n",
      "Logistic Regression Gradient Descent(105/499): loss=-0.5693937044260251\n",
      "Logistic Regression Gradient Descent(106/499): loss=-0.5686106305123663\n",
      "Logistic Regression Gradient Descent(107/499): loss=-0.5678447516748898\n",
      "Logistic Regression Gradient Descent(108/499): loss=-0.5670957373449456\n",
      "Logistic Regression Gradient Descent(109/499): loss=-0.5663632612476053\n",
      "Logistic Regression Gradient Descent(110/499): loss=-0.5656470014531452\n",
      "Logistic Regression Gradient Descent(111/499): loss=-0.5649466404221583\n",
      "Logistic Regression Gradient Descent(112/499): loss=-0.5642618650444069\n",
      "Logistic Regression Gradient Descent(113/499): loss=-0.5635923666715544\n",
      "Logistic Regression Gradient Descent(114/499): loss=-0.5629378411439443\n",
      "Logistic Regression Gradient Descent(115/499): loss=-0.5622979888116143\n",
      "Logistic Regression Gradient Descent(116/499): loss=-0.5616725145497593\n",
      "Logistic Regression Gradient Descent(117/499): loss=-0.5610611277688542\n",
      "Logistic Regression Gradient Descent(118/499): loss=-0.5604635424196761\n",
      "Logistic Regression Gradient Descent(119/499): loss=-0.559879476993456\n",
      "Logistic Regression Gradient Descent(120/499): loss=-0.5593086545174049\n",
      "Logistic Regression Gradient Descent(121/499): loss=-0.5587508025458546\n",
      "Logistic Regression Gradient Descent(122/499): loss=-0.5582056531472569\n",
      "Logistic Regression Gradient Descent(123/499): loss=-0.5576729428872801\n",
      "Logistic Regression Gradient Descent(124/499): loss=-0.5571524128082408\n",
      "Logistic Regression Gradient Descent(125/499): loss=-0.556643808405103\n",
      "Logistic Regression Gradient Descent(126/499): loss=-0.5561468795982728\n",
      "Logistic Regression Gradient Descent(127/499): loss=-0.5556613807034072\n",
      "Logistic Regression Gradient Descent(128/499): loss=-0.5551870703984545\n",
      "Logistic Regression Gradient Descent(129/499): loss=-0.5547237116881314\n",
      "Logistic Regression Gradient Descent(130/499): loss=-0.5542710718660373\n",
      "Logistic Regression Gradient Descent(131/499): loss=-0.5538289224745981\n",
      "Logistic Regression Gradient Descent(132/499): loss=-0.5533970392630218\n",
      "Logistic Regression Gradient Descent(133/499): loss=-0.5529752021434441\n",
      "Logistic Regression Gradient Descent(134/499): loss=-0.5525631951454328\n",
      "Logistic Regression Gradient Descent(135/499): loss=-0.5521608063690074\n",
      "Logistic Regression Gradient Descent(136/499): loss=-0.5517678279363305\n",
      "Logistic Regression Gradient Descent(137/499): loss=-0.5513840559422137\n",
      "Logistic Regression Gradient Descent(138/499): loss=-0.5510092904035758\n",
      "Logistic Regression Gradient Descent(139/499): loss=-0.550643335207982\n",
      "Logistic Regression Gradient Descent(140/499): loss=-0.5502859980613899\n",
      "Logistic Regression Gradient Descent(141/499): loss=-0.5499370904352129\n",
      "Logistic Regression Gradient Descent(142/499): loss=-0.5495964275128156\n",
      "Logistic Regression Gradient Descent(143/499): loss=-0.5492638281355398\n",
      "Logistic Regression Gradient Descent(144/499): loss=-0.548939114748358\n",
      "Logistic Regression Gradient Descent(145/499): loss=-0.5486221133452479\n",
      "Logistic Regression Gradient Descent(146/499): loss=-0.548312653414366\n",
      "Logistic Regression Gradient Descent(147/499): loss=-0.5480105678831061\n",
      "Logistic Regression Gradient Descent(148/499): loss=-0.5477156930631126\n",
      "Logistic Regression Gradient Descent(149/499): loss=-0.547427868595317\n",
      "Logistic Regression Gradient Descent(150/499): loss=-0.5471469373950625\n",
      "Logistic Regression Gradient Descent(151/499): loss=-0.5468727455973775\n",
      "Logistic Regression Gradient Descent(152/499): loss=-0.5466051425024491\n",
      "Logistic Regression Gradient Descent(153/499): loss=-0.5463439805213494\n",
      "Logistic Regression Gradient Descent(154/499): loss=-0.5460891151220636\n",
      "Logistic Regression Gradient Descent(155/499): loss=-0.5458404047758574\n",
      "Logistic Regression Gradient Descent(156/499): loss=-0.5455977109040293\n",
      "Logistic Regression Gradient Descent(157/499): loss=-0.5453608978250787\n",
      "Logistic Regression Gradient Descent(158/499): loss=-0.5451298327023272\n",
      "Logistic Regression Gradient Descent(159/499): loss=-0.5449043854920191\n",
      "Logistic Regression Gradient Descent(160/499): loss=-0.5446844288919318\n",
      "Logistic Regression Gradient Descent(161/499): loss=-0.5444698382905181\n",
      "Logistic Regression Gradient Descent(162/499): loss=-0.544260491716603\n",
      "Logistic Regression Gradient Descent(163/499): loss=-0.5440562697896563\n",
      "Logistic Regression Gradient Descent(164/499): loss=-0.5438570556706553\n",
      "Logistic Regression Gradient Descent(165/499): loss=-0.5436627350135557\n",
      "Logistic Regression Gradient Descent(166/499): loss=-0.5434731959173825\n",
      "Logistic Regression Gradient Descent(167/499): loss=-0.543288328878955\n",
      "Logistic Regression Gradient Descent(168/499): loss=-0.5431080267462501\n",
      "Logistic Regression Gradient Descent(169/499): loss=-0.5429321846724199\n",
      "Logistic Regression Gradient Descent(170/499): loss=-0.5427607000704634\n",
      "Logistic Regression Gradient Descent(171/499): loss=-0.5425934725685633\n",
      "Logistic Regression Gradient Descent(172/499): loss=-0.5424304039660872\n",
      "Logistic Regression Gradient Descent(173/499): loss=-0.5422713981902599\n",
      "Logistic Regression Gradient Descent(174/499): loss=-0.542116361253507\n",
      "Logistic Regression Gradient Descent(175/499): loss=-0.5419652012114698\n",
      "Logistic Regression Gradient Descent(176/499): loss=-0.5418178281216941\n",
      "Logistic Regression Gradient Descent(177/499): loss=-0.5416741540029884\n",
      "Logistic Regression Gradient Descent(178/499): loss=-0.5415340927954528\n",
      "Logistic Regression Gradient Descent(179/499): loss=-0.5413975603211745\n",
      "Logistic Regression Gradient Descent(180/499): loss=-0.5412644742455863\n",
      "Logistic Regression Gradient Descent(181/499): loss=-0.5411347540394861\n",
      "Logistic Regression Gradient Descent(182/499): loss=-0.5410083209417114\n",
      "Logistic Regression Gradient Descent(183/499): loss=-0.5408850979224643\n",
      "Logistic Regression Gradient Descent(184/499): loss=-0.5407650096472837\n",
      "Logistic Regression Gradient Descent(185/499): loss=-0.5406479824416548\n",
      "Logistic Regression Gradient Descent(186/499): loss=-0.5405339442562548\n",
      "Logistic Regression Gradient Descent(187/499): loss=-0.5404228246328244\n",
      "Logistic Regression Gradient Descent(188/499): loss=-0.5403145546706625\n",
      "Logistic Regression Gradient Descent(189/499): loss=-0.5402090669937314\n",
      "Logistic Regression Gradient Descent(190/499): loss=-0.540106295718374\n",
      "Logistic Regression Gradient Descent(191/499): loss=-0.5400061764216265\n",
      "Logistic Regression Gradient Descent(192/499): loss=-0.5399086461101262\n",
      "Logistic Regression Gradient Descent(193/499): loss=-0.5398136431896041\n",
      "Logistic Regression Gradient Descent(194/499): loss=-0.5397211074349553\n",
      "Logistic Regression Gradient Descent(195/499): loss=-0.5396309799608783\n",
      "Logistic Regression Gradient Descent(196/499): loss=-0.539543203193079\n",
      "Logistic Regression Gradient Descent(197/499): loss=-0.5394577208400276\n",
      "Logistic Regression Gradient Descent(198/499): loss=-0.539374477865264\n",
      "Logistic Regression Gradient Descent(199/499): loss=-0.5392934204602426\n",
      "Logistic Regression Gradient Descent(200/499): loss=-0.5392144960177084\n",
      "Logistic Regression Gradient Descent(201/499): loss=-0.5391376531055982\n",
      "Logistic Regression Gradient Descent(202/499): loss=-0.5390628414414597\n",
      "Logistic Regression Gradient Descent(203/499): loss=-0.5389900118673775\n",
      "Logistic Regression Gradient Descent(204/499): loss=-0.5389191163254036\n",
      "Logistic Regression Gradient Descent(205/499): loss=-0.5388501078334822\n",
      "Logistic Regression Gradient Descent(206/499): loss=-0.5387829404618607\n",
      "Logistic Regression Gradient Descent(207/499): loss=-0.5387175693099817\n",
      "Logistic Regression Gradient Descent(208/499): loss=-0.5386539504838491\n",
      "Logistic Regression Gradient Descent(209/499): loss=-0.5385920410738589\n",
      "Logistic Regression Gradient Descent(210/499): loss=-0.5385317991330888\n",
      "Logistic Regression Gradient Descent(211/499): loss=-0.5384731836560414\n",
      "Logistic Regression Gradient Descent(212/499): loss=-0.5384161545578321\n",
      "Logistic Regression Gradient Descent(213/499): loss=-0.538360672653814\n",
      "Logistic Regression Gradient Descent(214/499): loss=-0.5383066996396374\n",
      "Logistic Regression Gradient Descent(215/499): loss=-0.5382541980717308\n",
      "Logistic Regression Gradient Descent(216/499): loss=-0.5382031313482034\n",
      "Logistic Regression Gradient Descent(217/499): loss=-0.5381534636901574\n",
      "Logistic Regression Gradient Descent(218/499): loss=-0.5381051601234058\n",
      "Logistic Regression Gradient Descent(219/499): loss=-0.5380581864605888\n",
      "Logistic Regression Gradient Descent(220/499): loss=-0.5380125092836829\n",
      "Logistic Regression Gradient Descent(221/499): loss=-0.5379680959268941\n",
      "Logistic Regression Gradient Descent(222/499): loss=-0.5379249144599311\n",
      "Logistic Regression Gradient Descent(223/499): loss=-0.5378829336716502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Gradient Descent(224/499): loss=-0.5378421230540671\n",
      "Logistic Regression Gradient Descent(225/499): loss=-0.5378024527867273\n",
      "Logistic Regression Gradient Descent(226/499): loss=-0.5377638937214294\n",
      "Logistic Regression Gradient Descent(227/499): loss=-0.5377264173672947\n",
      "Logistic Regression Gradient Descent(228/499): loss=-0.5376899958761775\n",
      "Logistic Regression Gradient Descent(229/499): loss=-0.537654602028408\n",
      "Logistic Regression Gradient Descent(230/499): loss=-0.537620209218861\n",
      "Logistic Regression Gradient Descent(231/499): loss=-0.5375867914433473\n",
      "Logistic Regression Gradient Descent(232/499): loss=-0.5375543232853164\n",
      "Logistic Regression Gradient Descent(233/499): loss=-0.5375227799028675\n",
      "Logistic Regression Gradient Descent(234/499): loss=-0.5374921370160606\n",
      "Logistic Regression Gradient Descent(235/499): loss=-0.5374623708945225\n",
      "Logistic Regression Gradient Descent(236/499): loss=-0.5374334583453394\n",
      "Logistic Regression Gradient Descent(237/499): loss=-0.5374053767012312\n",
      "Logistic Regression Gradient Descent(238/499): loss=-0.5373781038090002\n",
      "Logistic Regression Gradient Descent(239/499): loss=-0.5373516180182484\n",
      "Logistic Regression Gradient Descent(240/499): loss=-0.5373258981703565\n",
      "Logistic Regression Gradient Descent(241/499): loss=-0.5373009235877184\n",
      "Logistic Regression Gradient Descent(242/499): loss=-0.5372766740632252\n",
      "Logistic Regression Gradient Descent(243/499): loss=-0.5372531298499928\n",
      "Logistic Regression Gradient Descent(244/499): loss=-0.5372302716513258\n",
      "Logistic Regression Gradient Descent(245/499): loss=-0.5372080806109126\n",
      "Logistic Regression Gradient Descent(246/499): loss=-0.5371865383032474\n",
      "Logistic Regression Gradient Descent(247/499): loss=-0.5371656267242684\n",
      "Logistic Regression Gradient Descent(248/499): loss=-0.5371453282822125\n",
      "Logistic Regression Gradient Descent(249/499): loss=-0.5371256257886766\n",
      "Logistic Regression Gradient Descent(250/499): loss=-0.5371065024498821\n",
      "Logistic Regression Gradient Descent(251/499): loss=-0.5370879418581359\n",
      "Logistic Regression Gradient Descent(252/499): loss=-0.537069927983484\n",
      "Logistic Regression Gradient Descent(253/499): loss=-0.5370524451655514\n",
      "Logistic Regression Gradient Descent(254/499): loss=-0.5370354781055646\n",
      "Logistic Regression Gradient Descent(255/499): loss=-0.5370190118585503\n",
      "Logistic Regression Gradient Descent(256/499): loss=-0.5370030318257073\n",
      "Logistic Regression Gradient Descent(257/499): loss=-0.5369875237469446\n",
      "Logistic Regression Gradient Descent(258/499): loss=-0.5369724736935851\n",
      "Logistic Regression Gradient Descent(259/499): loss=-0.5369578680612264\n",
      "Logistic Regression Gradient Descent(260/499): loss=-0.536943693562757\n",
      "Logistic Regression Gradient Descent(261/499): loss=-0.5369299372215237\n",
      "Logistic Regression Gradient Descent(262/499): loss=-0.5369165863646462\n",
      "Logistic Regression Gradient Descent(263/499): loss=-0.5369036286164734\n",
      "Logistic Regression Gradient Descent(264/499): loss=-0.5368910518921813\n",
      "Logistic Regression Gradient Descent(265/499): loss=-0.5368788443915056\n",
      "Logistic Regression Gradient Descent(266/499): loss=-0.5368669945926069\n",
      "Logistic Regression Gradient Descent(267/499): loss=-0.5368554912460671\n",
      "Logistic Regression Gradient Descent(268/499): loss=-0.53684432336901\n",
      "Logistic Regression Gradient Descent(269/499): loss=-0.536833480239346\n",
      "Logistic Regression Gradient Descent(270/499): loss=-0.5368229513901374\n",
      "Logistic Regression Gradient Descent(271/499): loss=-0.5368127266040817\n",
      "Logistic Regression Gradient Descent(272/499): loss=-0.5368027959081083\n",
      "Logistic Regression Gradient Descent(273/499): loss=-0.5367931495680885\n",
      "Logistic Regression Gradient Descent(274/499): loss=-0.5367837780836555\n",
      "Logistic Regression Gradient Descent(275/499): loss=-0.5367746721831302\n",
      "Logistic Regression Gradient Descent(276/499): loss=-0.5367658228185537\n",
      "Logistic Regression Gradient Descent(277/499): loss=-0.5367572211608207\n",
      "Logistic Regression Gradient Descent(278/499): loss=-0.5367488585949151\n",
      "Logistic Regression Gradient Descent(279/499): loss=-0.5367407267152419\n",
      "Logistic Regression Gradient Descent(280/499): loss=-0.5367328173210573\n",
      "Logistic Regression Gradient Descent(281/499): loss=-0.5367251224119913\n",
      "Logistic Regression Gradient Descent(282/499): loss=-0.5367176341836637\n",
      "Logistic Regression Gradient Descent(283/499): loss=-0.5367103450233903\n",
      "Logistic Regression Gradient Descent(284/499): loss=-0.5367032475059765\n",
      "Logistic Regression Gradient Descent(285/499): loss=-0.5366963343896002\n",
      "Logistic Regression Gradient Descent(286/499): loss=-0.5366895986117761\n",
      "Logistic Regression Gradient Descent(287/499): loss=-0.536683033285408\n",
      "Logistic Regression Gradient Descent(288/499): loss=-0.5366766316949168\n",
      "Logistic Regression Gradient Descent(289/499): loss=-0.5366703872924555\n",
      "Logistic Regression Gradient Descent(290/499): loss=-0.5366642936941969\n",
      "Logistic Regression Gradient Descent(291/499): loss=-0.5366583446767009\n",
      "Logistic Regression Gradient Descent(292/499): loss=-0.5366525341733577\n",
      "Logistic Regression Gradient Descent(293/499): loss=-0.5366468562709036\n",
      "Logistic Regression Gradient Descent(294/499): loss=-0.5366413052060106\n",
      "Logistic Regression Gradient Descent(295/499): loss=-0.5366358753619461\n",
      "Logistic Regression Gradient Descent(296/499): loss=-0.5366305612653035\n",
      "Logistic Regression Gradient Descent(297/499): loss=-0.5366253575828004\n",
      "Logistic Regression Gradient Descent(298/499): loss=-0.5366202591181446\n",
      "Logistic Regression Gradient Descent(299/499): loss=-0.5366152608089648\n",
      "Logistic Regression Gradient Descent(300/499): loss=-0.5366103577238083\n",
      "Logistic Regression Gradient Descent(301/499): loss=-0.5366055450591982\n",
      "Logistic Regression Gradient Descent(302/499): loss=-0.5366008181367569\n",
      "Logistic Regression Gradient Descent(303/499): loss=-0.5365961724003872\n",
      "Logistic Regression Gradient Descent(304/499): loss=-0.5365916034135152\n",
      "Logistic Regression Gradient Descent(305/499): loss=-0.5365871068563898\n",
      "Logistic Regression Gradient Descent(306/499): loss=-0.5365826785234419\n",
      "Logistic Regression Gradient Descent(307/499): loss=-0.5365783143206977\n",
      "Logistic Regression Gradient Descent(308/499): loss=-0.5365740102632486\n",
      "Logistic Regression Gradient Descent(309/499): loss=-0.536569762472774\n",
      "Logistic Regression Gradient Descent(310/499): loss=-0.5365655671751185\n",
      "Logistic Regression Gradient Descent(311/499): loss=-0.5365614206979196\n",
      "Logistic Regression Gradient Descent(312/499): loss=-0.5365573194682872\n",
      "Logistic Regression Gradient Descent(313/499): loss=-0.5365532600105333\n",
      "Logistic Regression Gradient Descent(314/499): loss=-0.5365492389439485\n",
      "Logistic Regression Gradient Descent(315/499): loss=-0.5365452529806296\n",
      "Logistic Regression Gradient Descent(316/499): loss=-0.5365412989233515\n",
      "Logistic Regression Gradient Descent(317/499): loss=-0.536537373663486\n",
      "Logistic Regression Gradient Descent(318/499): loss=-0.5365334741789664\n",
      "Logistic Regression Gradient Descent(319/499): loss=-0.536529597532295\n",
      "Logistic Regression Gradient Descent(320/499): loss=-0.5365257408685944\n",
      "Logistic Regression Gradient Descent(321/499): loss=-0.5365219014137013\n",
      "Logistic Regression Gradient Descent(322/499): loss=-0.536518076472301\n",
      "Logistic Regression Gradient Descent(323/499): loss=-0.5365142634261043\n",
      "Logistic Regression Gradient Descent(324/499): loss=-0.5365104597320624\n",
      "Logistic Regression Gradient Descent(325/499): loss=-0.5365066629206201\n",
      "Logistic Regression Gradient Descent(326/499): loss=-0.5365028705940109\n",
      "Logistic Regression Gradient Descent(327/499): loss=-0.536499080424585\n",
      "Logistic Regression Gradient Descent(328/499): loss=-0.5364952901531768\n",
      "Logistic Regression Gradient Descent(329/499): loss=-0.5364914975875067\n",
      "Logistic Regression Gradient Descent(330/499): loss=-0.5364877006006189\n",
      "Logistic Regression Gradient Descent(331/499): loss=-0.5364838971293531\n",
      "Logistic Regression Gradient Descent(332/499): loss=-0.5364800851728494\n",
      "Logistic Regression Gradient Descent(333/499): loss=-0.5364762627910874\n",
      "Logistic Regression Gradient Descent(334/499): loss=-0.5364724281034557\n",
      "Logistic Regression Gradient Descent(335/499): loss=-0.5364685792873543\n",
      "Logistic Regression Gradient Descent(336/499): loss=-0.536464714576827\n",
      "Logistic Regression Gradient Descent(337/499): loss=-0.536460832261225\n",
      "Logistic Regression Gradient Descent(338/499): loss=-0.5364569306838988\n",
      "Logistic Regression Gradient Descent(339/499): loss=-0.5364530082409191\n",
      "Logistic Regression Gradient Descent(340/499): loss=-0.5364490633798282\n",
      "Logistic Regression Gradient Descent(341/499): loss=-0.5364450945984152\n",
      "Logistic Regression Gradient Descent(342/499): loss=-0.536441100443522\n",
      "Logistic Regression Gradient Descent(343/499): loss=-0.5364370795098735\n",
      "Logistic Regression Gradient Descent(344/499): loss=-0.5364330304389348\n",
      "Logistic Regression Gradient Descent(345/499): loss=-0.5364289519177927\n",
      "Logistic Regression Gradient Descent(346/499): loss=-0.5364248426780638\n",
      "Logistic Regression Gradient Descent(347/499): loss=-0.5364207014948255\n",
      "Logistic Regression Gradient Descent(348/499): loss=-0.5364165271855703\n",
      "Logistic Regression Gradient Descent(349/499): loss=-0.5364123186091854\n",
      "Logistic Regression Gradient Descent(350/499): loss=-0.536408074664953\n",
      "Logistic Regression Gradient Descent(351/499): loss=-0.5364037942915736\n",
      "Logistic Regression Gradient Descent(352/499): loss=-0.5363994764662118\n",
      "Logistic Regression Gradient Descent(353/499): loss=-0.5363951202035618\n",
      "Logistic Regression Gradient Descent(354/499): loss=-0.5363907245549355\n",
      "Logistic Regression Gradient Descent(355/499): loss=-0.5363862886073691\n",
      "Logistic Regression Gradient Descent(356/499): loss=-0.5363818114827518\n",
      "Logistic Regression Gradient Descent(357/499): loss=-0.5363772923369717\n",
      "Logistic Regression Gradient Descent(358/499): loss=-0.5363727303590827\n",
      "Logistic Regression Gradient Descent(359/499): loss=-0.536368124770489\n",
      "Logistic Regression Gradient Descent(360/499): loss=-0.5363634748241479\n",
      "Logistic Regression Gradient Descent(361/499): loss=-0.5363587798037908\n",
      "Logistic Regression Gradient Descent(362/499): loss=-0.5363540390231613\n",
      "Logistic Regression Gradient Descent(363/499): loss=-0.5363492518252714\n",
      "Logistic Regression Gradient Descent(364/499): loss=-0.5363444175816722\n",
      "Logistic Regression Gradient Descent(365/499): loss=-0.5363395356917431\n",
      "Logistic Regression Gradient Descent(366/499): loss=-0.5363346055819965\n",
      "Logistic Regression Gradient Descent(367/499): loss=-0.536329626705397\n",
      "Logistic Regression Gradient Descent(368/499): loss=-0.5363245985406971\n",
      "Logistic Regression Gradient Descent(369/499): loss=-0.5363195205917874\n",
      "Logistic Regression Gradient Descent(370/499): loss=-0.5363143923870616\n",
      "Logistic Regression Gradient Descent(371/499): loss=-0.5363092134787951\n",
      "Logistic Regression Gradient Descent(372/499): loss=-0.5363039834425386\n",
      "Logistic Regression Gradient Descent(373/499): loss=-0.536298701876525\n",
      "Logistic Regression Gradient Descent(374/499): loss=-0.5362933684010889\n",
      "Logistic Regression Gradient Descent(375/499): loss=-0.5362879826581011\n",
      "Logistic Regression Gradient Descent(376/499): loss=-0.5362825443104127\n",
      "Logistic Regression Gradient Descent(377/499): loss=-0.5362770530413158\n",
      "Logistic Regression Gradient Descent(378/499): loss=-0.5362715085540127\n",
      "Logistic Regression Gradient Descent(379/499): loss=-0.5362659105710996\n",
      "Logistic Regression Gradient Descent(380/499): loss=-0.5362602588340608\n",
      "Logistic Regression Gradient Descent(381/499): loss=-0.5362545531027739\n",
      "Logistic Regression Gradient Descent(382/499): loss=-0.5362487931550287\n",
      "Logistic Regression Gradient Descent(383/499): loss=-0.5362429787860531\n",
      "Logistic Regression Gradient Descent(384/499): loss=-0.5362371098080533\n",
      "Logistic Regression Gradient Descent(385/499): loss=-0.5362311860497627\n",
      "Logistic Regression Gradient Descent(386/499): loss=-0.5362252073560014\n",
      "Logistic Regression Gradient Descent(387/499): loss=-0.5362191735872454\n",
      "Logistic Regression Gradient Descent(388/499): loss=-0.536213084619206\n",
      "Logistic Regression Gradient Descent(389/499): loss=-0.5362069403424193\n",
      "Logistic Regression Gradient Descent(390/499): loss=-0.5362007406618438\n",
      "Logistic Regression Gradient Descent(391/499): loss=-0.5361944854964671\n",
      "Logistic Regression Gradient Descent(392/499): loss=-0.5361881747789247\n",
      "Logistic Regression Gradient Descent(393/499): loss=-0.5361818084551221\n",
      "Logistic Regression Gradient Descent(394/499): loss=-0.5361753864838708\n",
      "Logistic Regression Gradient Descent(395/499): loss=-0.536168908836529\n",
      "Logistic Regression Gradient Descent(396/499): loss=-0.5361623754966529\n",
      "Logistic Regression Gradient Descent(397/499): loss=-0.5361557864596544\n",
      "Logistic Regression Gradient Descent(398/499): loss=-0.5361491417324673\n",
      "Logistic Regression Gradient Descent(399/499): loss=-0.5361424413332209\n",
      "Logistic Regression Gradient Descent(400/499): loss=-0.5361356852909221\n",
      "Logistic Regression Gradient Descent(401/499): loss=-0.5361288736451431\n",
      "Logistic Regression Gradient Descent(402/499): loss=-0.5361220064457177\n",
      "Logistic Regression Gradient Descent(403/499): loss=-0.5361150837524444\n",
      "Logistic Regression Gradient Descent(404/499): loss=-0.5361081056347953\n",
      "Logistic Regression Gradient Descent(405/499): loss=-0.536101072171633\n",
      "Logistic Regression Gradient Descent(406/499): loss=-0.5360939834509337\n",
      "Logistic Regression Gradient Descent(407/499): loss=-0.5360868395695164\n",
      "Logistic Regression Gradient Descent(408/499): loss=-0.5360796406327782\n",
      "Logistic Regression Gradient Descent(409/499): loss=-0.5360723867544365\n",
      "Logistic Regression Gradient Descent(410/499): loss=-0.5360650780562766\n",
      "Logistic Regression Gradient Descent(411/499): loss=-0.5360577146679056\n",
      "Logistic Regression Gradient Descent(412/499): loss=-0.5360502967265116\n",
      "Logistic Regression Gradient Descent(413/499): loss=-0.5360428243766285\n",
      "Logistic Regression Gradient Descent(414/499): loss=-0.5360352977699069\n",
      "Logistic Regression Gradient Descent(415/499): loss=-0.5360277170648894\n",
      "Logistic Regression Gradient Descent(416/499): loss=-0.5360200824267926\n",
      "Logistic Regression Gradient Descent(417/499): loss=-0.5360123940272924\n",
      "Logistic Regression Gradient Descent(418/499): loss=-0.5360046520443158\n",
      "Logistic Regression Gradient Descent(419/499): loss=-0.5359968566618369\n",
      "Logistic Regression Gradient Descent(420/499): loss=-0.5359890080696781\n",
      "Logistic Regression Gradient Descent(421/499): loss=-0.5359811064633155\n",
      "Logistic Regression Gradient Descent(422/499): loss=-0.5359731520436892\n",
      "Logistic Regression Gradient Descent(423/499): loss=-0.535965145017018\n",
      "Logistic Regression Gradient Descent(424/499): loss=-0.5359570855946191\n",
      "Logistic Regression Gradient Descent(425/499): loss=-0.5359489739927303\n",
      "Logistic Regression Gradient Descent(426/499): loss=-0.5359408104323397\n",
      "Logistic Regression Gradient Descent(427/499): loss=-0.5359325951390149\n",
      "Logistic Regression Gradient Descent(428/499): loss=-0.5359243283427406\n",
      "Logistic Regression Gradient Descent(429/499): loss=-0.5359160102777581\n",
      "Logistic Regression Gradient Descent(430/499): loss=-0.5359076411824079\n",
      "Logistic Regression Gradient Descent(431/499): loss=-0.5358992212989779\n",
      "Logistic Regression Gradient Descent(432/499): loss=-0.5358907508735539\n",
      "Logistic Regression Gradient Descent(433/499): loss=-0.5358822301558742\n",
      "Logistic Regression Gradient Descent(434/499): loss=-0.5358736593991883\n",
      "Logistic Regression Gradient Descent(435/499): loss=-0.535865038860117\n",
      "Logistic Regression Gradient Descent(436/499): loss=-0.5358563687985188\n",
      "Logistic Regression Gradient Descent(437/499): loss=-0.5358476494773571\n",
      "Logistic Regression Gradient Descent(438/499): loss=-0.5358388811625718\n",
      "Logistic Regression Gradient Descent(439/499): loss=-0.5358300641229538\n",
      "Logistic Regression Gradient Descent(440/499): loss=-0.5358211986300226\n",
      "Logistic Regression Gradient Descent(441/499): loss=-0.5358122849579071\n",
      "Logistic Regression Gradient Descent(442/499): loss=-0.5358033233832286\n",
      "Logistic Regression Gradient Descent(443/499): loss=-0.5357943141849871\n",
      "Logistic Regression Gradient Descent(444/499): loss=-0.5357852576444515\n",
      "Logistic Regression Gradient Descent(445/499): loss=-0.5357761540450505\n",
      "Logistic Regression Gradient Descent(446/499): loss=-0.5357670036722674\n",
      "Logistic Regression Gradient Descent(447/499): loss=-0.5357578068135377\n",
      "Logistic Regression Gradient Descent(448/499): loss=-0.5357485637581488\n",
      "Logistic Regression Gradient Descent(449/499): loss=-0.5357392747971422\n",
      "Logistic Regression Gradient Descent(450/499): loss=-0.5357299402232176\n",
      "Logistic Regression Gradient Descent(451/499): loss=-0.535720560330642\n",
      "Logistic Regression Gradient Descent(452/499): loss=-0.5357111354151567\n",
      "Logistic Regression Gradient Descent(453/499): loss=-0.535701665773891\n",
      "Logistic Regression Gradient Descent(454/499): loss=-0.5356921517052757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Gradient Descent(455/499): loss=-0.535682593508959\n",
      "Logistic Regression Gradient Descent(456/499): loss=-0.535672991485725\n",
      "Logistic Regression Gradient Descent(457/499): loss=-0.5356633459374147\n",
      "Logistic Regression Gradient Descent(458/499): loss=-0.5356536571668477\n",
      "Logistic Regression Gradient Descent(459/499): loss=-0.5356439254777474\n",
      "Logistic Regression Gradient Descent(460/499): loss=-0.5356341511746673\n",
      "Logistic Regression Gradient Descent(461/499): loss=-0.535624334562919\n",
      "Logistic Regression Gradient Descent(462/499): loss=-0.535614475948503\n",
      "Logistic Regression Gradient Descent(463/499): loss=-0.5356045756380408\n",
      "Logistic Regression Gradient Descent(464/499): loss=-0.5355946339387084\n",
      "Logistic Regression Gradient Descent(465/499): loss=-0.535584651158172\n",
      "Logistic Regression Gradient Descent(466/499): loss=-0.5355746276045255\n",
      "Logistic Regression Gradient Descent(467/499): loss=-0.53556456358623\n",
      "Logistic Regression Gradient Descent(468/499): loss=-0.535554459412053\n",
      "Logistic Regression Gradient Descent(469/499): loss=-0.5355443153910128\n",
      "Logistic Regression Gradient Descent(470/499): loss=-0.5355341318323198\n",
      "Logistic Regression Gradient Descent(471/499): loss=-0.535523909045324\n",
      "Logistic Regression Gradient Descent(472/499): loss=-0.5355136473394606\n",
      "Logistic Regression Gradient Descent(473/499): loss=-0.5355033470241989\n",
      "Logistic Regression Gradient Descent(474/499): loss=-0.5354930084089912\n",
      "Logistic Regression Gradient Descent(475/499): loss=-0.5354826318032249\n",
      "Logistic Regression Gradient Descent(476/499): loss=-0.5354722175161746\n",
      "Logistic Regression Gradient Descent(477/499): loss=-0.5354617658569553\n",
      "Logistic Regression Gradient Descent(478/499): loss=-0.5354512771344779\n",
      "Logistic Regression Gradient Descent(479/499): loss=-0.5354407516574063\n",
      "Logistic Regression Gradient Descent(480/499): loss=-0.5354301897341135\n",
      "Logistic Regression Gradient Descent(481/499): loss=-0.5354195916726419\n",
      "Logistic Regression Gradient Descent(482/499): loss=-0.5354089577806619\n",
      "Logistic Regression Gradient Descent(483/499): loss=-0.5353982883654346\n",
      "Logistic Regression Gradient Descent(484/499): loss=-0.5353875837337723\n",
      "Logistic Regression Gradient Descent(485/499): loss=-0.5353768441920037\n",
      "Logistic Regression Gradient Descent(486/499): loss=-0.535366070045937\n",
      "Logistic Regression Gradient Descent(487/499): loss=-0.5353552616008261\n",
      "Logistic Regression Gradient Descent(488/499): loss=-0.5353444191613372\n",
      "Logistic Regression Gradient Descent(489/499): loss=-0.5353335430315157\n",
      "Logistic Regression Gradient Descent(490/499): loss=-0.5353226335147556\n",
      "Logistic Regression Gradient Descent(491/499): loss=-0.5353116909137684\n",
      "Logistic Regression Gradient Descent(492/499): loss=-0.5353007155305542\n",
      "Logistic Regression Gradient Descent(493/499): loss=-0.5352897076663723\n",
      "Logistic Regression Gradient Descent(494/499): loss=-0.5352786676217137\n",
      "Logistic Regression Gradient Descent(495/499): loss=-0.5352675956962752\n",
      "Logistic Regression Gradient Descent(496/499): loss=-0.5352564921889316\n",
      "Logistic Regression Gradient Descent(497/499): loss=-0.5352453573977122\n",
      "Logistic Regression Gradient Descent(498/499): loss=-0.5352341916197756\n",
      "Logistic Regression Gradient Descent(499/499): loss=-0.5352229951513865\n",
      "0.6937644444444444\n"
     ]
    }
   ],
   "source": [
    "'''LOGISTIC REGRESSION'''\n",
    "(w5,loss5) = logistic_regression(y, tX, init_w, max_iter, alpha)\n",
    "log_tr_pred = predict_labels(w5, tX_valid)\n",
    "print((log_tr_pred == y_valid).mean())\n",
    "log_pred = predict_labels(w5, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/results_least_gd.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w1, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
