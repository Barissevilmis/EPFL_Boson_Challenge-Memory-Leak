{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y=='b')] = -1\n",
    "    \n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def predict_labels(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "            \n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "print(\"Targets: \", y)\n",
    "print(\"Ids: \",ids)\n",
    "print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE'''\n",
    "#Feature extraction\n",
    "tX = np.hstack((tX_old[:,1:4], tX_old[:,7:12]))\n",
    "tX = np.hstack((tX, tX_old[:,13:23]))\n",
    "'''\n",
    "#Fill out the empty values with mean of column\n",
    "fill_val = np.true_divide(tX.sum(axis=0),(tX!=-999).sum(axis=0))\n",
    "for fill in range(tX.shape[1]):\n",
    "    fill_ind = np.extract(tX[:,fill] == -999, tX)\n",
    "    print(fill_ind)\n",
    "    tX[fill_ind, fill] = fill_val[fill]\n",
    "'''\n",
    "#Standardize\n",
    "tX, tX_mean, tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06833197,  0.40768027, -0.46996624, ...,         nan,\n",
       "                nan, -3.13627044],\n",
       "       [ 0.55250482,  0.54013641, -0.15316749, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [ 3.19515553,  1.09655998, -0.34970965, ...,         nan,\n",
       "                nan,         nan],\n",
       "       ...,\n",
       "       [ 0.31931645, -0.13086367, -0.28495489, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [-0.84532397, -0.30297338, -0.69737766, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [ 0.66533608, -0.25352276, -0.79202769, ...,         nan,\n",
       "                nan,         nan]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''FEATURE ENGINEERING: TRANSFORMS LOG & MEAN FOR NOW && AGGREGATIONS OF REMAINING COLUMNS'''\n",
    "'''\n",
    "tX = np.hstack((tX, np.mean(tX)))\n",
    "tX = np.hstack((tX, np.log(tX_old)))\n",
    "tX.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data size:  25000\n",
      "Shapes of tX, y & Ids for Training:  (25000, 18) (25000,) (25000,)\n",
      "Shapes of tX, y & Ids for Validation:  (225000, 18) (225000,) (225000,)\n"
     ]
    }
   ],
   "source": [
    "'''SPLIT INTO TRAIN AND VALIDATION'''\n",
    "\n",
    "train_valid_split = int(tX.shape[0] / 10)\n",
    "print(\"Validation data size: \", train_valid_split)\n",
    "tX_valid = tX[train_valid_split:,:]\n",
    "y_valid = y[train_valid_split:]\n",
    "id_valid = ids[train_valid_split:]\n",
    "\n",
    "tX = tX[:train_valid_split]\n",
    "y = y[:train_valid_split]\n",
    "ids = ids[:train_valid_split]\n",
    "\n",
    "print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, id_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7400933333333334\n"
     ]
    }
   ],
   "source": [
    "'''CHECK MODEL SUCCESS: DELETE BEFORE SUBMISSION'''\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(tX, y)\n",
    "clf.fit(tX, y)\n",
    "print(metrics.accuracy_score(clf.predict(tX_valid), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tx@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(np.square(w)))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    z = 1 / (1 + np.exp(-1*(tx@w)))\n",
    "    return z\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm)))\n",
    "    \n",
    "    return loss\n",
    "def compute_log_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION FOR LR'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * (tx.T@(z - y))\n",
    "    return grad\n",
    "\n",
    "def compute_reg_log_loss(y, tx, w, lambda_):\n",
    "    '''LOGISTIC LOSS WITH REGULARIZATION'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm))) + ((lambda_/2)*np.sum(np.square(w))))\n",
    "    \n",
    "    return loss\n",
    "def compute_reg_log_gradient(y, tx, w, lambda_):\n",
    "    '''GRADIENT COMPUTATION FOR LR WITH REGULARIZATION'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * ((tx.T@(z - y)) + (lambda_*w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.inv((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]))@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_log_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_log_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad = compute_log_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT WITH REGULARIZATION'''\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_reg_log_loss(y, tx, w, lambda_)\n",
    "        grad = compute_reg_log_gradient(y, tx, w, lambda_)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_reg_log_loss(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            grad = compute__reg_log_gradient(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of tX & Ids for Testing:  (568238, 18) (25000,)\n"
     ]
    }
   ],
   "source": [
    "'''PREPROCESS: FEATURE EXTRACTION AND STANDARDIZE FOR TEST'''\n",
    "#Feature extraction\n",
    "tX_test = np.hstack((tX_test_old[:,1:4], tX_test_old[:,7:12]))\n",
    "tX_test = np.hstack((tX_test, tX_test_old[:,13:23]))\n",
    "#Standardize\n",
    "tX_test, tX_test_mean, tX_test_std = standardize(tX_test)\n",
    "\n",
    "print(\"Shapes of tX & Ids for Testing: \", tX_test.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "[0.37187665 0.17701228 0.94956115 0.52368943 0.9770155  0.59885257\n",
      " 0.6097812  0.86018647 0.48619013 0.9511333  0.98843491 0.60570815\n",
      " 0.99644431 0.28084673 0.47562736 0.01010419 0.23839104 0.92440486]\n"
     ]
    }
   ],
   "source": [
    "ww = np.random.rand(tX.shape[1])\n",
    "init_w = np.array(ww, dtype=np.float64)\n",
    "#init_w = np.zeros(tX.shape[1])\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HYPER PARAMETERS FOR TUNING'''\n",
    "max_iter = np.array([200, 300, 400, 500, 600, 700, 800, 1000])\n",
    "gamma = np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10])\n",
    "best_res = np.zeros((6,1))\n",
    "best_gamma = np.zeros((6,1))\n",
    "best_iter = np.zeros((6,1))\n",
    "best_lambda = np.zeros((6,1))\n",
    "best_grad = np.zeros((6,tX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for best accuracy in SGD:  [0.70837333]  are gamma:=  [0.01]  & iteration number:= [1000.]\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "count = 0\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        (w1,loss1) = least_squares_GD(y, tX, init_w, n_iter, n_gamma)\n",
    "        gd_tr_pred = predict_labels(w1, tX_valid)\n",
    "        res = (gd_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w1\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "print(\"Parameters for best accuracy in SGD: \", best_res[count], \" are gamma:= \",best_gamma[count], \" & iteration number:=\", best_iter[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for best accuracy in SGD:  [0.70259111]  are gamma:=  [0.01]  & iteration number:= [800.]\n"
     ]
    }
   ],
   "source": [
    "'''SGD'''\n",
    "count = 1\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        (w2,loss2) = least_squares_SGD(y, tX, init_w, n_iter, n_gamma)\n",
    "        sgd_tr_pred = predict_labels(w2, tX_valid)\n",
    "        res = (sgd_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w2\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "print(\"Parameters for best accuracy in SGD: \", best_res[count], \" are gamma:= \",best_gamma[count], \" & iteration number:=\", best_iter[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "count = 2\n",
    "(w3,loss3) = least_squares(y, tX)\n",
    "ls_tr_pred = predict_labels(w3, tX_valid)\n",
    "print((ls_tr_pred == y_valid).mean())\n",
    "ls_pred = predict_labels(w3, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066711111111111\n"
     ]
    }
   ],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "count = 3\n",
    "lambda_ = np.logspace(-1, -10, 50)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        best_lambda[count] = lambda_[i]\n",
    "        min_loss = loss4\n",
    "(w4,loss4) = ridge_regression(y, tX, best_lambda[count])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for best accuracy in LR with no regularization:  [0.71166222]  are gamma:=  [0.1]  & iteration number:= [1000.]\n"
     ]
    }
   ],
   "source": [
    "'''LOGISTIC REGRESSION WITH (STOCHASTIC) GRADIENT DESCENT'''\n",
    "count = 4\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        (w5,loss5) = logistic_regression(y, tX, init_w, n_iter, n_gamma)\n",
    "        log_tr_pred = predict_labels(w5, tX_valid)\n",
    "        res = (log_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w5\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "print(\"Parameters for best accuracy in LR with no regularization: \", best_res[count], \" are gamma:= \",\\\n",
    "      best_gamma[count], \" & iteration number:=\", best_iter[count])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for best accuracy in LR with regularization:  [0.71166222]  are gamma:=  [0.1] , iteration number:= [1000.]  & lambda: [0.00011721]\n"
     ]
    }
   ],
   "source": [
    "'''LOGISTIC REGRESSION WITH (STOCHASTIC) GRADIENT DESCENT + REGULARIZATION'''\n",
    "'''FOR OPTIMAL PARAMETERS: TAKES SOME TIME TO TRAIN COMPLETELY'''\n",
    "'''TO ENABLE LAMBDA ITERATION: UNCOMMENT LAMBDA_2 LINES + CHANGE best_lambda[3] TO lambda_2[n_lambda]'''\n",
    "'''OTHERWISE, LAMBDA CHOSEN FOR RIDGE REGRESSION WILL BE USED'''\n",
    "count = 5\n",
    "ind2 = 0\n",
    "min_loss2 = 1000000\n",
    "#lambda_2 = np.logspace(-1, -6, 30)\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        #for n_lambda in range(lambda_2.shape[0]):\n",
    "        (w6,loss6) = reg_logistic_regression(y, tX, best_lambda[3], init_w, n_iter, n_gamma)\n",
    "        log_tr_pred = predict_labels(w6, tX_valid)\n",
    "        res = (log_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w6\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "            #best_lambda[count] = lambda_2[n_lambda]\n",
    "print(\"Parameters for best accuracy in LR with regularization: \", best_res[count], \" are gamma:= \",\\\n",
    "      best_gamma[count], \", iteration number:=\", best_iter[count], \" & lambda:\", best_lambda[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/lr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(best_grad[4], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
