{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "'''DATASET INTRINSICS AND SHAPE (TARGETS AND IDS INCLUDED)'''\n",
    "def DataSetInfo(tX_old, y, ids):\n",
    "    print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "    print(\"Targets: \", y)\n",
    "    print(\"Ids: \",ids)\n",
    "    print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)\n",
    "DataSetInfo(tX_old, y, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIALIZE WEIGHTS'''\n",
    "def InitWeights():\n",
    "    ww = np.random.rand(tX.shape[1])\n",
    "    #init_w = np.zeros(tX.shape[1])\n",
    "    init_w = np.array(ww, dtype=np.float64)\n",
    "    return init_w\n",
    "\n",
    "'''HYPER PARAMETERS FOR TUNING'''\n",
    "def HyperParameters():\n",
    "    max_iter = np.array([200, 300, 400, 500, 600, 700, 800, 1000])\n",
    "    gamma = np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10])\n",
    "    best_res = np.zeros((6,1))\n",
    "    best_gamma = np.zeros((6,1))\n",
    "    best_iter = np.zeros((6,1))\n",
    "    best_lambda = np.zeros((6,1))\n",
    "    best_grad = np.zeros((6,1))\n",
    "    return max_iter, gamma, best_res, best_gamma, best_iter, best_lambda, best_grad\n",
    "\n",
    "'''TAKE LOG TRANSFORMATION OF FEATURES'''\n",
    "def LogTransformData(tX, features):  \n",
    "    data = tX[:, features]\n",
    "    indices = np.where(data > -999)\n",
    "    #min_nonzero = np.min(data[np.nonzero(data)])\n",
    "    #data[data == 0] = min_nonzero\n",
    "    data[indices] = np.log(1 + data[indices])\n",
    "    return data\n",
    "\n",
    "\n",
    "'''IMPUTE DATA WITH MOST FREQUENT VALUES OR ZERO'''\n",
    "def ImputeData(tX, typ=\"ZERO\"):\n",
    "    for i in range(tX.shape[1]):\n",
    "        '''REPLACE ACCORDING TO NAN VALUES(-999)'''\n",
    "        np.any(tX == -999)\n",
    "        if np.any(tX[:, i] == -999):\n",
    "            tX_nonzero = (tX[:, i] > -999)\n",
    "            val, count = np.unique(tX[tX_nonzero, i], return_counts=True)\n",
    "            if (len(val) >= 2) and typ == \"MF\":\n",
    "                '''MOST FREQUENT VALUE'''\n",
    "                tX[~tX_nonzero, i] = val[np.argmax(count)]\n",
    "            elif typ == \"ZERO\":\n",
    "                '''ZERO'''\n",
    "                tX[~tX_nonzero, i] = 0\n",
    "    return tX\n",
    "\n",
    "'''STANDARDIZE'''\n",
    "def Standardize(x):\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x[:, std_x > 0] = x[:, std_x > 0] / std_x[std_x > 0]\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "'''PREPROCESS'''\n",
    "def PreProcess(tX):\n",
    "    '''FEATURES PICKED BY HAND'''\n",
    "    feature_vec = np.array(([0, 2, 5, 9, 11, 13, 16, 19, 20, 21, 26, 29]))\n",
    "    '''LUCKY FEATURE OF THE WEEK: 30 :)'''\n",
    "    lucky_feature = np.array(([30]))\n",
    "    tX = LogTransformData(tX, feature_vec)\n",
    "    tX = ImputeData(tX, \"MF\")\n",
    "    tX = Standardize(tX)\n",
    "    return tX\n",
    "\n",
    "'''DATASET SEPERATED IN TERMS OF CATEGORIES IN COLUMN 22'''\n",
    "def Categorize(y, tX, ids):\n",
    "    '''CATEGORIES '''\n",
    "    ind = [[] for j in range(4)]\n",
    "    xx = [[] for j in range(4)]\n",
    "    yy = [[] for j in range(4)]\n",
    "    iids = [[] for j in range(4)]\n",
    "    \n",
    "    for i in range(4):\n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        xx[i] = tX[ind[i]]\n",
    "        yy[i] = y[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return yy, xx, iids, inds\n",
    "\n",
    "'''PREDICTIONS INTO COMPARABLE FORM'''\n",
    "def Decategorize(size, y_cat, ind):\n",
    "    y = np.zeros((size, 1), dtype=np.float)\n",
    "    for i in range(len(y)):\n",
    "        y[ind[i]] = y_cat[i]    \n",
    "    return y\n",
    "\n",
    "'''FEATURE CORRELATION MAP: ONLY FOR VISUALIZATION'''\n",
    "'''CORRELATED FEATURES: CORR > THRESHOLD : USE FOR SYNTHESIS'''\n",
    "def CorrMap(tX):\n",
    "    df = pd.DataFrame(tX)\n",
    "    corr = df.corr()\n",
    "    return corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "'''FEATURE ENGINEERING'''\n",
    "def FeatureSynthesis(tX_pp, tX_old):\n",
    "    '''CORRELATED FEATURES WILL BE USED FOR NEW FEATURE ADDITION'''\n",
    "    '''MIN PART'''\n",
    "    #tX = np.hstack((tX_pp, np.minimum((tX_old[:,15:16] - tX_old[:,18:19]),np.minimum((tX_old[:,15:16] - tX_old[:,20:21]),(tX_old[:,18:19] - tX_old[:,20:21])))))                                           \n",
    "    tX = np.hstack((tX_pp, np.minimum((tX_old[:,15:16] - tX_old[:,20:21]),(tX_old[:,18:19] - tX_old[:,20:21]))))\n",
    "    tX = np.hstack((tX, np.minimum((tX_old[:,15:16] - tX_old[:,18:19]),(tX_old[:,15:16] - tX_old[:,20:21]))))\n",
    "    tX = np.hstack((tX, (tX_old[:,18:19] - tX_old[:,20:21])))\n",
    "    '''LN PART'''\n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,13:14]*tX_old[:,14:15])))+(tX_old[:,13:14]*tX_old[:,14:15]))))                                                                                         \n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,16:17]*tX_old[:,17:18])))+(tX_old[:,16:17]*tX_old[:,16:17]))))                                                                                         \n",
    "    #tX = np.hstack((tX, np.log(np.abs(min((tX_old[:,23:24]*tX_old[:,24:25])))+(tX_old[:,23:24]*tX_old[:,24:25]))))                                                                                         \n",
    "    return tX\n",
    "\n",
    "'''SPLIT INTO TRAIN AND VALIDATION: HARDCODED'''\n",
    "def DataSplit(tX, y, ids, tX_cat, split_size = 0.1):\n",
    "    train_valid_split = int(tX.shape[0] *split_size)\n",
    "    print(\"Validation data size: \", train_valid_split)\n",
    "    tX = tX[train_valid_split:,:]\n",
    "    tX_cat = tX_cat[train_valid_split:]\n",
    "    y = y[train_valid_split:]\n",
    "    ids = ids[train_valid_split:]\n",
    "\n",
    "    tX_valid = tX[:train_valid_split,:]\n",
    "    tX_cat_val = tX_cat[:train_valid_split]\n",
    "    y_valid = y[:train_valid_split]\n",
    "    ids_valid = ids[:train_valid_split]\n",
    "\n",
    "    print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "    print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, ids_valid.shape)\n",
    "    return (tX, y, ids,tX_cat), (tX_valid, y_valid, ids_valid, tX_cat_val)\n",
    "\n",
    "'''RANDOM DATA SPLIT'''\n",
    "def RandomizedDataSplit(tX, y, ids, split_size = 0.1, my_seed=1):\n",
    "    '''SET SEED FOR RANDOMNESS'''\n",
    "    #np.random.seed(my_seed)\n",
    "    '''RANDOM INDEXES'''\n",
    "    size = y.shape[0]\n",
    "    ind = np.random.permutation(siz8e)\n",
    "    split = int(np.floor(split_size * size))\n",
    "    \n",
    "    ind_train = ind[split:]\n",
    "    ind_valid = ind[:split]\n",
    "    \n",
    "    '''SPLIT DATA ACCORDING TO RANDOM INDICES'''\n",
    "    tX_train = tX[ind_train]\n",
    "    tX_valid = tX[ind_valid]\n",
    "    ids_train = ids[ind_train]\n",
    "    ids_valid = ids[ind_valid]\n",
    "    y_train = y[ind_train]\n",
    "    y_valid = y[ind_valid]\n",
    "    \n",
    "    print(\"Shapes of tX, y & Ids for Training: \", tX.shape, y.shape, ids.shape)\n",
    "    print(\"Shapes of tX, y & Ids for Validation: \", tX_valid.shape, y_valid.shape, ids_valid.shape)\n",
    "    return (tX_train, y_train, ids_train),(tX_valid, y_valid, ids_valid)\n",
    "\n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel(y, tX_old, ids):\n",
    "    tX_pp = PreProcess(tX_old)\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize(y, tX_pp, ids)\n",
    "    \n",
    "    '''TRAIN SET'''\n",
    "    tX_tr_cat = [[] for j in range(4)]\n",
    "    y_tr_cat = [[] for j in range(4)]\n",
    "    id_tr_cat = [[] for j in range(4)]\n",
    "    \n",
    "    '''VALID SET'''\n",
    "    tX_val_cat = [[] for j in range(4)]\n",
    "    y_val_cat = [[] for j in range(4)]\n",
    "    id_val_cat = [[] for j in range(4)]\n",
    "    \n",
    "    for i in range(len(tX_cat)):\n",
    "        (tX_tr_cat[i], y_tr_cat[i],id_tr_cat[i]), (tX_val_cat[i], y_val_cat[i],id_val_cat[i]) = RandomizedDataSplit(tX_cat[i], y_cat[i], id_cat[i], 0.1, 42)\n",
    "    \n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_tr_cat = np.array((tX_tr_cat))\n",
    "    y_tr_cat = np.array((y_tr_cat))\n",
    "    id_tr_cat = np.array((id_tr_cat))\n",
    "    \n",
    "    tX_val_cat = np.array((tX_val_cat))\n",
    "    y_val_cat = np.array((y_val_cat))\n",
    "    id_val_cat = np.array((id_val_cat))\n",
    "    \n",
    "    return y_cat, tX_cat, id_cat\n",
    "\n",
    "'''BACKWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def backward_selection(y, tX, tX_valid, y_valid):\n",
    "    selected_features = []\n",
    "    cur_best_acc = 0\n",
    "    improved = True     \n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        worst_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features:\n",
    "                \n",
    "                diff = set(list(range(tX.shape[1]))) - set(selected_features + [i])            \n",
    "                #calculate accuracy\n",
    "                #print(tX[:,list(diff)].shape,y.shape)\n",
    "                (w3,loss3) = least_squares(y, tX[:,list(diff)])\n",
    "                ls_tr_pred = predict_labels(w3, tX_valid[:,list(diff)])\n",
    "                cur_acc = (ls_tr_pred == y_valid).mean()\n",
    "                \n",
    "                #accuracy is improved\n",
    "                if cur_best_acc < cur_acc:\n",
    "                    improved = True\n",
    "                    cur_best_acc = cur_acc\n",
    "                    worst_ft = i                    \n",
    "        if improved:\n",
    "            selected_features.append(worst_ft)      \n",
    "            \n",
    "    return list(set(list(range(tX.shape[1]))) - set(selected_features )), cur_best_acc\n",
    "\n",
    "'''FORWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def forward_selection(y, tX, tX_valid, y_valid):    \n",
    "\n",
    "    selected_features = []\n",
    "    cur_best_acc = 0    \n",
    "    improved = True\n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        best_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features: \n",
    "                \n",
    "                #calculate accuracy\n",
    "                (w3,loss3) = least_squares(y, tX[:,selected_features+[i]])\n",
    "                ls_tr_pred = predict_labels(w3, tX_valid[:,selected_features+[i]])\n",
    "                cur_acc = (ls_tr_pred == y_valid).mean()\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc < cur_acc:\n",
    "                    improved = True                   \n",
    "                    cur_best_acc = cur_acc\n",
    "                    best_ft = i                    \n",
    "        if improved:\n",
    "            selected_features.append(best_ft)\n",
    "    return selected_features, cur_best_acc\n",
    "\n",
    "def select_indices(y, k_fold, seed):\n",
    "    row_count = y.shape[0]\n",
    "    window_size = int((row_count / k_fold))\n",
    "    remainder = row_count % k_fold\n",
    "    np.random.seed(seed)\n",
    "    rand_indices = np.random.permutation(row_count)\n",
    "    indices = [[] for i in range(k_fold)]\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        \n",
    "            indices[k] = [rand_indices[k*window_size:(k+1)*window_size]]\n",
    "            \n",
    "    return np.array(indices)\n",
    "\n",
    "\n",
    "def cross_validate(y, tX, k):\n",
    "    seed = random.randint(0,10)\n",
    "    indices = select_indices(y,k,seed)\n",
    "    average_acc = 0\n",
    "    for i in range (k):        \n",
    "\n",
    "        xk_train = tX[~indices[i]]\n",
    "        xk_valid = tX[indices[i]]\n",
    "        yk_train = y[~indices[i]]\n",
    "        yk_valid = y[indices[i]]\n",
    "        #print(yk_train.shape,xk_train.shape)\n",
    "        #print(xk_valid.shape)\n",
    "        (w3,loss3) = least_squares(yk_train[0], xk_train[0])\n",
    "        ls_tr_pred = predict_labels(w3, xk_valid[0])\n",
    "        #print((ls_tr_pred == yk_valid[0]).mean())\n",
    "        average_acc += (ls_tr_pred == yk_valid[0]).mean()/k\n",
    "\n",
    "    print(average_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in log\n",
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-2b58286301e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildDataModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX_old\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Categorized y shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Categorized tX shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Categorized ids shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-0c8f8b24104e>\u001b[0m in \u001b[0;36mBuildDataModel\u001b[0;34m(y, tX_old, ids)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBuildDataModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mtX_pp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0my_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m'''TRAIN SET'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-0c8f8b24104e>\u001b[0m in \u001b[0;36mCategorize\u001b[0;34m(y, tX, ids)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0myy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "y_cat, tX_cat, ids_cat = BuildDataModel(y,tX_old,ids)\n",
    "print(\"Categorized y shape:\", y_cat.shape)\n",
    "print(\"Categorized tX shape:\", tX_cat.shape)\n",
    "print(\"Categorized ids shape:\", ids_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tx@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(w.T@w))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    z = 1 / (1 + np.exp(-1*(tx@w)))\n",
    "    return z\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm)))\n",
    "    \n",
    "    return loss\n",
    "def compute_log_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION FOR LR'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * (tx.T@(z - y))\n",
    "    return grad\n",
    "\n",
    "def compute_reg_log_loss(y, tx, w, lambda_):\n",
    "    '''LOGISTIC LOSS WITH REGULARIZATION'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    N = y.shape[0]\n",
    "    loss = (-1/N)*(np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(1 - sigm))) + ((lambda_/2)*np.sum(w.T@w)))\n",
    "    \n",
    "    return loss\n",
    "def compute_reg_log_gradient(y, tx, w, lambda_):\n",
    "    '''GRADIENT COMPUTATION FOR LR WITH REGULARIZATION'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * ((tx.T@(z - y)) + (lambda_*w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    w_ridge = np.linalg.inv((tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1]))@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_log_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_log_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad = compute_log_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    '''FOR GRADIENT DESCENT WITH REGULARIZATION'''\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_reg_log_loss(y, tx, w, lambda_)\n",
    "        grad = compute_reg_log_gradient(y, tx, w, lambda_)\n",
    "        w = w - (gamma * grad)\n",
    "        #print(\"Logistic Regression Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_reg_log_loss(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            grad = compute__reg_log_gradient(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n",
      "[[0.47752817 0.53237564 0.614171   0.40338695 0.4637636  0.64416938\n",
      "  0.62139253 0.26214141 0.57306712 0.61463914 0.40939511 0.36947413\n",
      "  0.59873518 0.14783013 0.77775938 0.80120927]\n",
      " [0.68800538 0.64373082 0.33953821 0.54799493 0.35130658 0.96164668\n",
      "  0.5968562  0.51642995 0.91842214 0.74127546 0.17422433 0.57602151\n",
      "  0.66033403 0.63888455 0.81576852 0.77760735]\n",
      " [0.43260563 0.7366731  0.38866435 0.51113149 0.14940554 0.08395296\n",
      "  0.43442385 0.00697077 0.56693497 0.99265572 0.19162477 0.27167145\n",
      "  0.82826809 0.18852534 0.93029302 0.45987876]\n",
      " [0.96037117 0.12330554 0.36276116 0.68132134 0.02006043 0.26148079\n",
      "  0.17547043 0.90954437 0.97759811 0.41406438 0.94430178 0.16762658\n",
      "  0.31519373 0.12138497 0.9689438  0.08747229]]\n"
     ]
    }
   ],
   "source": [
    "init_w = np.array((InitWeights(),InitWeights(),InitWeights(),InitWeights()))\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-926b73215137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m'''BATCH GD'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmod1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgd_tr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "'''BATCH GD'''\n",
    "count = 0\n",
    "w1 = np.zeros((init_w.shape[0],init_w.shape[1]))\n",
    "mod1 = [0, 1, 2 ,3]\n",
    "gd_tr_pred = [np.zeros((y_val_final[0].shape)), np.zeros((y_val_final[1].shape)), np.zeros((y_val_final[2].shape)), np.zeros((y_val_final[3].shape))]\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        for mod in mod1:\n",
    "            (w1[mod],loss1) = least_squares_GD(y_final[mod], tX_final[mod], init_w[mod], n_iter, n_gamma)\n",
    "            gd_tr_pred[mod] = predict_labels(w1[mod], tX_val_final[mod])\n",
    "        #gd_tr_pred = np.flatten(gd_tr_pred)\n",
    "        #gd_tr_pred = np.sort(gd_tr_pred,'mergesort')\n",
    "        for acc in range(len(mod1)):\n",
    "            \n",
    "        res = (gd_tr_pred == y_val_final).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = np.flatten(w1)\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "print(\"Parameters for best accuracy in BGD: \", best_res[count], \" are gamma:= \",best_gamma[count], \" & iteration number:=\", best_iter[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.array((InitWeights(tX_final[0]),InitWeights(tX_final[1]),InitWeights(tX_final[2]),InitWeights(tX_final[3])))\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SGD'''\n",
    "count = 1\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        (w2,loss2) = least_squares_SGD(y, tX, init_w, n_iter, n_gamma)\n",
    "        sgd_tr_pred = predict_labels(w2, tX_valid)\n",
    "        res = (sgd_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w2\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "print(\"Parameters for best accuracy in SGD: \", best_res[count], \" are gamma:= \",best_gamma[count], \" & iteration number:=\", best_iter[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LS WITH NORMAL EQ'''\n",
    "count = 2\n",
    "(w3,loss3) = least_squares(y, tX)\n",
    "ls_tr_pred = predict_labels(w3, tX_valid)\n",
    "print((ls_tr_pred == y_valid).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RIDGE REGRESSION'''\n",
    "'''CHOOSE BEST LAMBDA'''\n",
    "count = 3\n",
    "lambda_ = np.logspace(-1, -10, 50)\n",
    "min_loss = 1000000\n",
    "ind = 0\n",
    "for i in range(lambda_.shape[0]):\n",
    "    (w4,loss4) = ridge_regression(y, tX, lambda_[i])\n",
    "    if min_loss > loss4:\n",
    "        best_lambda[count] = lambda_[i]\n",
    "        min_loss = loss4\n",
    "(w4,loss4) = ridge_regression(y, tX, best_lambda[count])       \n",
    "rd_tr_pred = predict_labels(w4, tX_valid)\n",
    "print((rd_tr_pred == y_valid).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.array((InitWeights(),InitWeights(),InitWeights(),InitWeights()))\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOGISTIC REGRESSION WITH (STOCHASTIC) GRADIENT DESCENT'''\n",
    "count = 4\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        (w5,loss5) = logistic_regression(y, tX, init_w, n_iter, n_gamma)\n",
    "        log_tr_pred = predict_labels(w5, tX_valid)\n",
    "        res = (log_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w5\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "print(\"Parameters for best accuracy in LR with no regularization: \", best_res[count], \" are gamma:= \",\\\n",
    "      best_gamma[count], \" & iteration number:=\", best_iter[count])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.array((InitWeights(),InitWeights(),InitWeights(),InitWeights()))\n",
    "print(init_w.shape)\n",
    "print(init_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOGISTIC REGRESSION WITH (STOCHASTIC) GRADIENT DESCENT + REGULARIZATION'''\n",
    "'''FOR OPTIMAL PARAMETERS: TAKES SOME TIME TO TRAIN COMPLETELY'''\n",
    "'''TO ENABLE LAMBDA ITERATION: UNCOMMENT LAMBDA_2 LINES + CHANGE best_lambda[3] TO lambda_2[n_lambda]'''\n",
    "'''OTHERWISE, LAMBDA CHOSEN FOR RIDGE REGRESSION WILL BE USED'''\n",
    "count = 5\n",
    "ind2 = 0\n",
    "min_loss2 = 1000000\n",
    "#lambda_2 = np.logspace(-1, -6, 30)\n",
    "for n_iter in max_iter:\n",
    "    for n_gamma in gamma:\n",
    "        #for n_lambda in range(lambda_2.shape[0]):\n",
    "        (w6,loss6) = reg_logistic_regression(y, tX, best_lambda[3], init_w, n_iter, n_gamma)\n",
    "        log_tr_pred = predict_labels(w6, tX_valid)\n",
    "        res = (log_tr_pred == y_valid).mean()\n",
    "        if res > best_res[count]:\n",
    "            best_grad[count] = w6\n",
    "            best_res[count] = res\n",
    "            best_iter[count] = n_iter\n",
    "            best_gamma[count] = n_gamma\n",
    "            #best_lambda[count] = lambda_2[n_lambda]\n",
    "print(\"Parameters for best accuracy in LR with regularization: \", best_res[count], \" are gamma:= \",\\\n",
    "      best_gamma[count], \", iteration number:=\", best_iter[count], \" & lambda:\", best_lambda[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/lr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(best_grad[0], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
