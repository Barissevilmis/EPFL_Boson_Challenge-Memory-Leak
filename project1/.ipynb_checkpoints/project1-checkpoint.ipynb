{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "'''ONLY FOR VISUALIZATION'''\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Modify DATA_PATH if needed\n",
    "DATA_TRAIN_PATH = '../../data_project1/train.csv'\n",
    "y, tX_old, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples:  [[ 138.47    51.655   97.827 ...    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ... -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [ 105.457   60.526   75.839 ... -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]  & shape: \n",
      "Targets:  [ 1. -1. -1. ...  1. -1. -1.]\n",
      "Ids:  [100000 100001 100002 ... 349997 349998 349999]\n",
      "Shapes of tX, y & Ids:  (250000, 30) (250000,) (250000,)\n"
     ]
    }
   ],
   "source": [
    "'''DATASET INTRINSICS AND SHAPE (TARGETS AND IDS INCLUDED)'''\n",
    "def DataSetInfo(y, tX_old, ids):\n",
    "    print(\"Training examples: \", tX_old, \" & shape: \")\n",
    "    print(\"Targets: \", y)\n",
    "    print(\"Ids: \",ids)\n",
    "    print(\"Shapes of tX, y & Ids: \", tX_old.shape, y.shape, ids.shape)\n",
    "DataSetInfo(y, tX_old, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIALIZE WEIGHTS'''\n",
    "def InitWeights(feat):\n",
    "    #ww = np.random.rand(feat)\n",
    "    ww = np.zeros((feat))\n",
    "    init_w = np.array(ww, dtype=np.float64)\n",
    "    return init_w\n",
    "\n",
    "'''HYPER PARAMETERS'''\n",
    "def HyperParameters():\n",
    "    max_iter = 800\n",
    "    epochs = 10\n",
    "    gamma = 1e-2\n",
    "    lambda_ = 1e-1\n",
    "    decay = 1e-2\n",
    "    k_fold = 10\n",
    "    return max_iter, epochs, gamma, lambda_, decay, k_fold\n",
    "\n",
    "'''DECREASE LEARNING RATE AT EVERY EPOCH'''\n",
    "def GammaScheduler(gamma, decay, epoch):\n",
    "    gamma = gamma * (1/(1 + decay * epoch))\n",
    "    return gamma\n",
    "\n",
    "'''TAKE LOG TRANSFORMATION OF FEATURES'''\n",
    "def LogTransformData(tX, features):  \n",
    "    data = tX[:, features]\n",
    "    indices = np.where(data > -999)\n",
    "    data[indices] = np.log(1 + data[indices])\n",
    "    tX = ManipulateFeatures(tX, data, features)    \n",
    "    return tX\n",
    "\n",
    "'''DELETE GIVEN FEATURE VECTOR FROM FEATURE AND CONCETENATE WITH NEW DATA '''\n",
    "def ManipulateFeatures(tX, data,features):\n",
    "    tX = np.delete(tX, features, 1)\n",
    "    return np.hstack((tX, data))\n",
    "\n",
    "'''IMPUTE DATA WITH MEANS'''\n",
    "def ImputeData(tX):\n",
    "    tX = np.where(tX == -999, np.nan, tX)\n",
    "    #Remove all columns with NAN\n",
    "    tX = tX[:, ~np.all(np.isnan(tX), axis=0)]\n",
    "    #Remove highly correlated features\n",
    "    tX = tX[:, ~np.all(tX[1:] == tX[:-1], axis=0)]\n",
    "    #Find Mean excluding NAN values\n",
    "    tX_mean = np.nanmean(tX, axis=0)\n",
    "    # NAN = MEAN\n",
    "    tX[np.where(np.isnan(tX))] = np.take(tX_mean, np.where(np.isnan(tX))[1])\n",
    "    print(tX.shape)\n",
    "    return tX\n",
    "\n",
    "'''STANDARDIZE'''\n",
    "def Standardize(tX):\n",
    "    mean_x = np.mean(tX, axis=0)\n",
    "    tX = tX - mean_x\n",
    "    std_x = np.std(tX, axis=0)\n",
    "    tX[:, std_x > 0] = tX[:, std_x > 0] / std_x[std_x > 0]\n",
    "    return tX, mean_x, std_x\n",
    "\n",
    "'''PREPROCESS'''\n",
    "def PreProcess(tX, rr_ = True):\n",
    "    '''FEATURES PICKED BY HAND FOR LOG TRANSFORM'''\n",
    "    log_feature_vec = np.array(([0, 2, 5, 9, 13, 16, 19, 21, 23, 26, 29]))\n",
    "    tX = LogTransformData(tX, log_feature_vec)\n",
    "    print(tX.shape)\n",
    "    tX = ImputeData(tX)\n",
    "    if rr_:\n",
    "        tX = Standardize(tX)[0]\n",
    "        tX = AddFeatures(tX)\n",
    "    else:\n",
    "        tX = AddFeatures(tX)\n",
    "        tX = Standardize(tX)[0]\n",
    "    return tX\n",
    "\n",
    "'''DATASET SEPERATED IN TERMS OF CATEGORIES IN COLUMN 22'''\n",
    "def Categorize_Train(y, tX, ids):\n",
    "    '''CATEGORY 2 AND 3 CONCETENATED'''\n",
    "    ind = [[] for j in range(3)]\n",
    "    xx = [[] for j in range(3)]\n",
    "    yy = [[] for j in range(3)]\n",
    "    iids = [[] for j in range(3)]\n",
    "    \n",
    "    for i in range(3): \n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        if i == 2:\n",
    "            ind[i] = np.hstack((ind[i], np.nonzero(tX[:, 22] == (i+1))[0].T))       \n",
    "        xx[i] = tX[ind[i]]\n",
    "        yy[i] = y[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return np.array((yy)), np.array((xx)), np.array((iids)), np.array((ind))\n",
    "\n",
    "'''CATEGORIZE TEST'''\n",
    "def Categorize_Test(tX, ids):\n",
    "    '''CATEGORY 2 AND 3 CONCETENATED'''\n",
    "    ind = [[] for j in range(3)]\n",
    "    xx = [[] for j in range(3)]\n",
    "    iids = [[] for j in range(3)]\n",
    "    \n",
    "    for i in range(3): \n",
    "        ind[i] = np.nonzero(tX[:, 22] == i)[0]\n",
    "        if i == 2:\n",
    "            ind[i] = np.hstack((ind[i], np.nonzero(tX[:, 22] == (i+1))[0].T))   \n",
    "        xx[i] = tX[ind[i]]\n",
    "        iids[i] = ids[ind[i]]\n",
    "        \n",
    "    return np.array((xx)), np.array((iids)), np.array((ind))\n",
    "\n",
    "'''PREDICTIONS INTO COMPARABLE FORM'''\n",
    "def Decategorize(y_cat, ind):\n",
    "    size = y_cat[0].shape[0] + y_cat[1].shape[0] + y_cat[2].shape[0]\n",
    "    y = np.zeros((size,), dtype=np.float)\n",
    "    for i in range(len(y_cat)):\n",
    "        y[ind[i]] = y_cat[i]\n",
    "    return y\n",
    "\n",
    "'''CHECK VALIDATION SCORE'''\n",
    "def WeightedAverage(pred, target):\n",
    "    total_count = pred[0].shape[0] + pred[1].shape[0] + pred[2].shape[0]\n",
    "    true_count = 0\n",
    "    for i in range(3):\n",
    "        true_count +=  np.sum(pred[i] == target[i])\n",
    "    acc = true_count / total_count\n",
    "    return acc\n",
    "'''FEATURE CORRELATION MAP: ONLY FOR VISUALIZATION'''\n",
    "'''CORRELATED FEATURES: CORR > THRESHOLD : USE FOR SYNTHESIS'''\n",
    "def CorrMap(tX):\n",
    "    df = pd.DataFrame(tX)\n",
    "    corr = df.corr()\n",
    "    return corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "'''RANDOM DATA SPLIT'''\n",
    "def RandomizedDataSplit(tX, y, ids, inds, split_size = 0.1, my_seed=1):\n",
    "    '''SET SEED FOR REMOVING RANDOMNESS'''\n",
    "    #np.random.seed(my_seed)\n",
    "    '''RANDOM INDEXES'''\n",
    "    size = y.shape[0]\n",
    "    ind = np.random.permutation(size)\n",
    "    split = int(np.floor(split_size * size))\n",
    "    \n",
    "    ind_train = ind[split:]\n",
    "    ind_valid = ind[:split]\n",
    "    \n",
    "    \n",
    "    '''SPLIT DATA ACCORDING TO RANDOM INDICES'''\n",
    "    tX_train = tX[ind_train]\n",
    "    tX_valid = tX[ind_valid]\n",
    "    y_train = y[ind_train]\n",
    "    y_valid = y[ind_valid]\n",
    "    ids_train = ids[ind_train]\n",
    "    ids_valid = ids[ind_valid]\n",
    "    inds_train = inds[ind_train]\n",
    "    inds_valid = inds[ind_valid]\n",
    "    \n",
    "    print(\"Shapes of tX, y, Ids & Indices for Training: \", tX_train.shape, y_train.shape, ids_train.shape, inds_train.shape)\n",
    "    print(\"Shapes of tX, y, Ids & Indices for Validation: \", tX_valid.shape, y_valid.shape, ids_valid.shape, inds_valid.shape)\n",
    "    return (tX_train, y_train, ids_train, inds_train),(tX_valid, y_valid, ids_valid, inds_valid)\n",
    "\n",
    "'''BACKWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def BackwardSelection(y, tX, tX_valid, y_valid, model = \"RR\"):\n",
    "    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0\n",
    "    improved = True     \n",
    "    while improved:\n",
    "        improved = False\n",
    "        worst_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features:\n",
    "                \n",
    "                diff = set(list(range(tX.shape[1]))) - set(selected_features + [i])            \n",
    "                #calculate accuracy\n",
    "                #print(tX[:,list(diff)].shape,y.shape)\n",
    "                \n",
    "                cur_acc = CrossValidation(y, tX[:,list(diff)],10)\n",
    "                #print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    #print(\"best so far: \",cur_best_acc)\n",
    "                    improved = True\n",
    "                    cur_best_acc = cur_acc\n",
    "                    worst_ft = i                    \n",
    "        if improved:\n",
    "            selected_features.append(worst_ft)  \n",
    "        #print(\"burada\",improved)\n",
    "            \n",
    "    return list(set(list(range(tX.shape[1]))) - set(selected_features )), cur_best_acc\n",
    "\n",
    "'''FORWARD OF BACKWARD SELECTION'''\n",
    "def Selection(y, tX, tX_valid, y_valid, typ = \"BS\", model = \"RR\"):\n",
    "    if typ == \"BS\":\n",
    "        selected_features, cur_best_acc = BackwardSelection(y, tX, tX_valid, y_valid,model)\n",
    "        return selected_features, cur_best_acc\n",
    "    elif typ == \"FS\":\n",
    "        selected_features, cur_best_acc = ForwardSelection(y, tX, tX_valid, y_valid,model) \n",
    "        return selected_features, cur_best_acc\n",
    "    \n",
    "'''FORWARD SELECTION METHOD FOR BEST FEATURE SELECTION: GREEDY APPROACH'''\n",
    "def ForwardSelection(y, tX, tX_valid, y_valid, model = \"RR\"):    \n",
    "    selected_features = []\n",
    "    cur_best_acc = 0  \n",
    "    improved = True\n",
    "    while improved:\n",
    "        \n",
    "        improved = False\n",
    "        best_ft = -1 \n",
    "        for i in range(tX.shape[1]):\n",
    "            if i not in selected_features: \n",
    "                #calculate accuracy             \n",
    "                cur_acc = CrossValidation(y, tX[:,selected_features+[i]],5)\n",
    "                #print(cur_acc)\n",
    "                #accuracy is improved\n",
    "                if cur_best_acc <= cur_acc:\n",
    "                    improved = True                   \n",
    "                    cur_best_acc = cur_acc\n",
    "                    best_ft = i                 \n",
    "                    \n",
    "        if improved:\n",
    "            selected_features.append(best_ft)\n",
    "            #print(selected_features)\n",
    "         \n",
    "    return selected_features, cur_best_acc\n",
    "\n",
    "'''ADD NEW FUTURES'''\n",
    "def AddFeatures(tX):\n",
    "    prime_numbers = [2,3]\n",
    "    #ADD COS / SIN , SQRT \n",
    "    #CHECK FEATURE SYNTHESIS\n",
    "    pm = 0\n",
    "    loop_count = tX.shape[1]\n",
    "    for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.cos(tX[:,i]).reshape(-1,1)))\n",
    "    for i in range(loop_count):\n",
    "            tX = np.hstack((tX, np.sin(tX[:,i]).reshape(-1,1)))\n",
    "                \n",
    "    for pm in range(len(prime_numbers)):\n",
    "        for i in range(3*loop_count):\n",
    "            tX = np.hstack((tX, np.power(tX[:,i], prime_numbers[pm]).reshape(-1,1)))\n",
    "    \n",
    "    return tX\n",
    "\n",
    "'''CROSS VALIDATION HELPER FUNCTION'''\n",
    "def SelectIndices(y, k_fold, seed):\n",
    "    row_count = y.shape[0]\n",
    "    window_size = int((row_count / k_fold))\n",
    "    remainder = row_count % k_fold\n",
    "    '''SEED IN TERMS OF SHUFFLING ONLY ONCE'''\n",
    "    np.random.seed(seed)\n",
    "    rand_indices = np.random.permutation(row_count)\n",
    "    indices = [[] for i in range(k_fold)]\n",
    "    \n",
    "    for k in range(k_fold):\n",
    "        \n",
    "            indices[k] = np.array((rand_indices[k*window_size:(k+1)*window_size]))\n",
    "            \n",
    "    return np.array(indices)\n",
    "'''CROSS VALIDATION'''\n",
    "def CrossValidation(y, tX, k, cat_, lambda_):\n",
    "    seed = np.random.randint(10)\n",
    "    indices = SelectIndices(y,k,seed)\n",
    "    average_acc = 0\n",
    "    w_vec = list()\n",
    "    \n",
    "    for i in range (k): \n",
    "        tr_indices = list()\n",
    "        count = 1\n",
    "        for tr_ in range(len(indices)):\n",
    "            if i != tr_:\n",
    "                if count == 1:\n",
    "                    tr_indices = np.array((indices[tr_]))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    tr_indices = np.hstack((tr_indices, indices[tr_]))\n",
    "                \n",
    "        #print(tr_indices)\n",
    "        valid_indices = indices[i]   \n",
    "        xk_train = tX[tr_indices]\n",
    "        xk_valid = tX[valid_indices]\n",
    "        yk_train = y[tr_indices]\n",
    "        yk_valid = y[valid_indices]\n",
    "        #print(yk_train.shape,xk_train.shape)\n",
    "        #print(xk_valid.shape)\n",
    "        init_w_gd = np.array((InitWeights(xk_train.shape[1])))\n",
    "        (w,loss) = ridge_regression(yk_train, xk_train, lambda_)\n",
    "        ls_tr_pred = predict_labels(w, xk_valid)\n",
    "        average_acc += (ls_tr_pred == yk_valid).mean()/k\n",
    "        w_vec.append(w)\n",
    "        \n",
    "    print(\"Cross Validation Accuracy for Category \",cat_,\": \",average_acc)\n",
    "    w_final = w_vec[0]\n",
    "    for weight in range(1,len(w_vec)):\n",
    "        w_final += w_vec[weight]\n",
    "    w_final = w_final / len(w_vec)\n",
    "    \n",
    "    return w_final, average_acc\n",
    "    \n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Train(y, tX_old, ids):\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize_Train(y, tX_old, ids)  \n",
    "    for cat_ in range(tX_cat.shape[0]):\n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_], False)  \n",
    "         \n",
    "    '''TRAIN SET'''\n",
    "    tX_tr_cat = [[] for j in range(3)]\n",
    "    y_tr_cat = [[] for j in range(3)]\n",
    "    id_tr_cat = [[] for j in range(3)]\n",
    "    ind_tr_cat = [[] for j in range(3)]\n",
    "\n",
    "    '''VALID SET'''\n",
    "    tX_val_cat = [[] for j in range(3)]\n",
    "    y_val_cat = [[] for j in range(3)]\n",
    "    id_val_cat = [[] for j in range(3)]\n",
    "    ind_val_cat = [[] for j in range(3)]\n",
    "\n",
    "    for i in range(len(tX_cat)):\n",
    "        (tX_tr_cat[i], y_tr_cat[i],id_tr_cat[i],ind_tr_cat[i]), (tX_val_cat[i], y_val_cat[i],id_val_cat[i], ind_val_cat[i]) = RandomizedDataSplit(tX_cat[i], y_cat[i], id_cat[i], ind_cat[i])\n",
    "\n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_tr_cat = np.array((tX_tr_cat))\n",
    "    y_tr_cat = np.array((y_tr_cat))\n",
    "    id_tr_cat = np.array((id_tr_cat))\n",
    "    ind_tr_cat = np.array((ind_tr_cat))\n",
    "\n",
    "    tX_val_cat = np.array((tX_val_cat))\n",
    "    y_val_cat = np.array((y_val_cat))\n",
    "    id_val_cat = np.array((id_val_cat))\n",
    "    ind_val_cat = np.array((ind_val_cat))\n",
    "\n",
    "    return (y_tr_cat, tX_tr_cat, id_tr_cat, ind_tr_cat), (y_val_cat, tX_val_cat, id_val_cat, ind_val_cat)\n",
    "\n",
    "'''CROSS VALIDATION DATA MODEL'''\n",
    "def BuildDataModel_CV(y, tX_old, ids):\n",
    "    y_cat, tX_cat, id_cat, ind_cat = Categorize_Train(y, tX_old, ids)\n",
    "    \n",
    "    for cat_ in range(tX_cat.shape[0]): \n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_], True)\n",
    "        \n",
    "    return y_cat, tX_cat, id_cat, ind_cat\n",
    "\n",
    "'''BUILD FULL DATA MODEL WITH CATEGORIZATION'''\n",
    "def BuildDataModel_Test(tX_old, ids):\n",
    "    tX_cat, id_cat, ind_cat = Categorize_Test(tX_old, ids)\n",
    "    \n",
    "    for cat_ in range(tX_cat.shape[0]):\n",
    "        tX_cat[cat_] = PreProcess(tX_cat[cat_])\n",
    "        \n",
    "    '''CONVERT TRAIN AND DATASET INTO NUMPY ARRAYS'''\n",
    "    tX_cat = np.array((tX_cat))\n",
    "    id_cat = np.array((id_cat))\n",
    "    ind_cat = np.array((ind_cat))\n",
    "    \n",
    "    return tX_cat, id_cat, ind_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAD AND LOSS FUNCTIONS'''\n",
    "def compute_loss(y, tx, w, typ):\n",
    "    '''typ = <LOSS_TYPE(WITH CAPITAL LETTERS)>'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    if typ == \"MSE\":\n",
    "        loss = (1/(2*N))*np.sum(np.square(y - (tx@w)))        \n",
    "    elif typ == \"MAE\":\n",
    "        loss = (1/(2*N))*np.sum(np.abs(y - (tx@w)))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION'''\n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N) * (tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    '''STOCHASTIC GRADIENT DESCENT GRADIENT COMPUTATION''' \n",
    "    N = y.shape[0]\n",
    "    e = y - tx@w\n",
    "    grad = (-1/N)*(tx.T@e)\n",
    "    return grad\n",
    "\n",
    "def compute_ls_loss(y, tx, w):\n",
    "    '''LEAST SQUARES WITH NORMAL EQUATIONS LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*(tx.T@(y - tx@w))\n",
    "    \n",
    "def compute_rdg_loss(y, tx, w, lambda_):\n",
    "    '''RIDGE REGRESSION LOSS COMPUTATION'''\n",
    "    loss = 0\n",
    "    N = y.shape[0]\n",
    "    loss = (1/(2*N))*np.sum(np.square(y - (tx@w))) + (lambda_*np.sum(w.T@w))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(tx, w):\n",
    "    '''SIGMOID CALCULATION'''\n",
    "    z = 1 / (1 + np.exp(-1*(tx@w)))\n",
    "    return z\n",
    "\n",
    "def compute_log_loss(y, tx, w):\n",
    "    '''LOGISTIC LOSS'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    sigm2 = 1 - sigm\n",
    "    N = y.shape[0]\n",
    "    '''GIVEN THAT WE HAVE A VALUE THAT IS NEGATIVE OR REALLY SMALL(PYTHON CONVERTS IT TO ZERO DURING COMPUTATION)'''\n",
    "    '''CONVERT THEM TO 1e-100'''\n",
    "    sigm[sigm < 1e-50] = 1e-50\n",
    "    sigm2[sigm2 < 1e-50] = 1e-50\n",
    "    \n",
    "    loss = (-1/N)*np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(sigm2)))\n",
    "    \n",
    "    return loss\n",
    "def compute_log_gradient(y, tx, w):\n",
    "    '''GRADIENT COMPUTATION FOR LR'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * (tx.T@(z - y))\n",
    "    return grad\n",
    "\n",
    "def compute_reg_log_loss(y, tx, w, lambda_):\n",
    "    '''LOGISTIC LOSS WITH REGULARIZATION'''\n",
    "    loss = 0;\n",
    "    sigm = sigmoid(tx,w)\n",
    "    sigm2 = 1 - sigm\n",
    "    N = y.shape[0]\n",
    "    sigm[sigm < 1e-50] = 1e-50\n",
    "    sigm2[sigm2 < 1e-50] = 1e-50\n",
    "    loss = (-1/N)*(np.sum(y.T@np.log(sigm) + ((1-y).T@np.log(sigm2))) + ((lambda_/2)*np.sum(w.T@w)))\n",
    "    \n",
    "    return loss\n",
    "def compute_reg_log_gradient(y, tx, w, lambda_):\n",
    "    '''GRADIENT COMPUTATION FOR LR WITH REGULARIZATION'''\n",
    "    N = y.shape[0]\n",
    "    z = sigmoid(tx,w)\n",
    "    grad = (1/N) * ((tx.T@(z - y)) + (lambda_*w))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma, cat_):\n",
    "    '''BATCH GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w, \"MSE\")\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        if (n_iter % 100) == 0:\n",
    "            print(\"Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma, cat_):\n",
    "    '''STOCHASTIC GRADIENT DESCENT'''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w, \"MSE\")\n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            if (n_iter % 100) == 0:\n",
    "                print(\"Stochastic Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "                \n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''COMPUTE W_STAR: WEIGHT FOR NORMAL EQUATIONS BY LINEAR EQUATION SOLVER'''\n",
    "    w_star = np.linalg.solve(tx.T@tx, tx.T@y)\n",
    "    loss = compute_ls_loss(y, tx, w_star)\n",
    "    return (w_star,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''RIDGE REGRESSION WITH LAMBDA PARAMETER AS REGULARIZATION PARAMETER'''\n",
    "    N = y.shape[0]\n",
    "    a = tx.shape[0]\n",
    "    m = (tx.T@tx)+(lambda_/(2*N))*np.identity(tx.shape[1])\n",
    "    i = np.eye(m.shape[0],m.shape[0])\n",
    "    w_ridge = np.linalg.lstsq(m,i)[0]@tx.T@y\n",
    "    loss = compute_rdg_loss(y, tx, w_ridge, lambda_)\n",
    "    return (w_ridge, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma, cat_):\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_log_loss(y, tx, w)\n",
    "        grad = compute_log_gradient(y, tx, w)\n",
    "        w = w - (gamma * grad)\n",
    "        if (n_iter % 100) == 0:\n",
    "            print(\"Logistic Regression Gradient Descent({bi}/{ti}) for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_log_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad = compute_log_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma, cat_):\n",
    "    '''FOR GRADIENT DESCENT WITH REGULARIZATION'''\n",
    "    '''FOR GRADIENT DESCENT'''\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_reg_log_loss(y, tx, w, lambda_)\n",
    "        grad = compute_reg_log_gradient(y, tx, w, lambda_)\n",
    "        w = w - (gamma * grad)\n",
    "        if (n_iter % 100) == 0:\n",
    "            print(\"Logistic Regression Gradient Descent({bi}/{ti})for Category-{pi}: loss={l}\".format(bi=n_iter, ti=max_iters - 1,pi = cat_, l=loss))\n",
    "\n",
    "\n",
    "\n",
    "    return (w, loss)\n",
    "    '''FOR STOCHASTIC GRADIENT DESCENT'''\n",
    "    '''\n",
    "    w = initial_w \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            loss = compute_reg_log_loss(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            grad = compute__reg_log_gradient(minibatch_y, minibatch_tx, w, lambda_)\n",
    "            w = w - gamma * grad\n",
    "            #print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return (w, loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../../data_project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test_old, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(99913, 18)\n",
      "(77544, 30)\n",
      "(77544, 22)\n",
      "(72543, 30)\n",
      "(72543, 30)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (89922, 162) (89922,) (89922,) (89922,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (9991, 162) (9991,) (9991,) (9991,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (69790, 198) (69790,) (69790,) (69790,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (7754, 198) (7754,) (7754,) (7754,)\n",
      "Shapes of tX, y, Ids & Indices for Training:  (65289, 270) (65289,) (65289,) (65289,)\n",
      "Shapes of tX, y, Ids & Indices for Validation:  (7254, 270) (7254,) (7254,) (7254,)\n"
     ]
    }
   ],
   "source": [
    "(y_cat, tX_cat, ids_cat, ind_cat), (y_val_cat, tX_val_cat, ids_val_cat, ind_val_cat) = BuildDataModel_Train(y,tX_old,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(99913, 18)\n",
      "(77544, 30)\n",
      "(77544, 22)\n",
      "(72543, 30)\n",
      "(72543, 30)\n"
     ]
    }
   ],
   "source": [
    "y_cv_cat, tX_cv_cat, ids_cv_cat, ind_cv_cat = BuildDataModel_CV(y,tX_old,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227458, 30)\n",
      "(227458, 18)\n",
      "(175338, 30)\n",
      "(175338, 22)\n",
      "(165442, 30)\n",
      "(165442, 30)\n"
     ]
    }
   ],
   "source": [
    "tX_test_cat, id_test_cat, ind_test_cat = BuildDataModel_Test(tX_test_old,ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/799)for Category-0: loss=0.5\n",
      "Gradient Descent(100/799)for Category-0: loss=0.37235323101034923\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3691122369315538\n",
      "Gradient Descent(300/799)for Category-0: loss=0.36712117543664274\n",
      "Gradient Descent(400/799)for Category-0: loss=0.36564180974006355\n",
      "Gradient Descent(500/799)for Category-0: loss=0.36448654670771335\n",
      "Gradient Descent(600/799)for Category-0: loss=0.3635644049626767\n",
      "Gradient Descent(700/799)for Category-0: loss=0.3628164983880047\n",
      "Gradient Descent(0/799)for Category-1: loss=0.5\n",
      "Gradient Descent(100/799)for Category-1: loss=0.37017359278586176\n",
      "Gradient Descent(200/799)for Category-1: loss=0.36305329054400437\n",
      "Gradient Descent(300/799)for Category-1: loss=0.35869594953391987\n",
      "Gradient Descent(400/799)for Category-1: loss=0.3557131039607014\n",
      "Gradient Descent(500/799)for Category-1: loss=0.35352976532773767\n",
      "Gradient Descent(600/799)for Category-1: loss=0.35184970168832086\n",
      "Gradient Descent(700/799)for Category-1: loss=0.35050787168619835\n",
      "Gradient Descent(0/799)for Category-2: loss=0.5\n",
      "Gradient Descent(100/799)for Category-2: loss=0.31087985283318215\n",
      "Gradient Descent(200/799)for Category-2: loss=0.3023418132152814\n",
      "Gradient Descent(300/799)for Category-2: loss=0.2970685094591793\n",
      "Gradient Descent(400/799)for Category-2: loss=0.2934679012273125\n",
      "Gradient Descent(500/799)for Category-2: loss=0.2908630290373668\n",
      "Gradient Descent(600/799)for Category-2: loss=0.2889007003903193\n",
      "Gradient Descent(700/799)for Category-2: loss=0.2873763709676986\n",
      "0.01\n",
      "Gradient Descent(0/799)for Category-0: loss=0.36220114485439747\n",
      "Gradient Descent(100/799)for Category-0: loss=0.3616879529831045\n",
      "Gradient Descent(200/799)for Category-0: loss=0.3612544300561283\n",
      "Gradient Descent(300/799)for Category-0: loss=0.3608837381345581\n",
      "Gradient Descent(400/799)for Category-0: loss=0.3605631396839385\n"
     ]
    }
   ],
   "source": [
    "'''PREDICTIONS FOR MODELS'''\n",
    "def Main(y_cat, tX_cat, y_val_cat, tX_val_cat):\n",
    "    init_w_gd = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[1].shape[1]),InitWeights(tX_cat[2].shape[1])))\n",
    "    gd_tr_pred = np.copy((y_val_cat))\n",
    "    w_gd = np.copy((init_w_gd))\n",
    "    max_iters, epochs, gamma, lambda_,decay,k_fold = HyperParameters()\n",
    "    cat_lst = [0, 1, 2]\n",
    "    \n",
    "    for epoch_ in range(epochs):\n",
    "        \n",
    "        for cat_ in cat_lst:   \n",
    "            (w_gd[cat_],loss1) = least_squares_GD(y_cat[cat_], tX_cat[cat_],w_gd[cat_], max_iters, gamma, cat_)\n",
    "        \n",
    "        print(gamma)\n",
    "        gamma = GammaScheduler(gamma, decay, epoch_)\n",
    "        \n",
    "    '''PREDICTIONS'''\n",
    "    for cat_ in cat_lst:\n",
    "        gd_tr_pred[cat_] = predict_labels(w_gd[cat_], tX_val_cat[cat_])\n",
    "        print(gd_tr_pred[cat_])\n",
    "            \n",
    "    acc = WeightedAverage(gd_tr_pred, y_val_cat)\n",
    "    print(\"Accuracy of Model:\", acc)\n",
    "    return w_gd\n",
    "w_normal = Main(y_cat, tX_cat, y_val_cat, tX_val_cat)\n",
    "print(w_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/OneForAll/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy for Category  0 :  0.8448748748748749\n",
      "Final weight vector shape:  (162,)\n",
      "Cross Validation Accuracy for Category  1 :  0.807544979686593\n",
      "Final weight vector shape:  (198,)\n",
      "Cross Validation Accuracy for Category  2 :  0.8340639647091261\n",
      "Final weight vector shape:  (270,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "'''CROSS VALIDATION WEIGHT'''\n",
    "'''WEIGHTS ARE TAKEN AS MEAN OF VARIOUS MEANS BY TRAINING ON DIFFERENT TRAIN-VALID PARTITIONS'''\n",
    "'''ACCURACY INCREASE IS NOT THE PURPOSE BUT WEIGHT AND ACCURACY VALIDATION ARE FIXED'''\n",
    "def CV_Main(y_cat, tX_cat):\n",
    "    cat_lst = [0, 1, 2]\n",
    "    max_iters, epochs, gamma, lambda_, decay, k_fold = HyperParameters()\n",
    "    w_res = np.array((InitWeights(tX_cat[0].shape[1]),InitWeights(tX_cat[1].shape[1]),InitWeights(tX_cat[2].shape[1])))\n",
    "    cv_tr_pred = [[] for i in range(len(cat_lst))]\n",
    "    for cat_ in cat_lst:\n",
    "        w_final, avg_acc = CrossValidation(y_cat[cat_], tX_cat[cat_], k_fold, cat_, lambda_)\n",
    "        w_res[cat_] = w_final\n",
    "        print(\"Final weight vector shape: \",w_final.shape)\n",
    "    return w_res\n",
    "\n",
    "\n",
    "w_cv = CV_Main(y_cv_cat, tX_cv_cat)\n",
    "print(w_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../../data_project1/results.csv' # TODO: fill in desired name of output file for submission\n",
    "cat_lst = [0, 1, 2]\n",
    "y_pred = [[] for i in range(3)]\n",
    "for cat_ in cat_lst:\n",
    "    y_pred[cat_] = predict_labels(w_res[cat_], tX_test_cat[cat_])\n",
    "y_pred = np.array((y_pred))\n",
    "pred_vec = Decategorize(y_pred, ind_test_cat)\n",
    "pred_ids = Decategorize(id_test_cat, ind_test_cat)\n",
    "create_csv_submission(pred_ids, pred_vec, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
